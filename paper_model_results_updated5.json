[
    {
        "AlbertForMaskedLM": {
            "pytorch_torch_compile_inductor": {
                "inference_s": 0.021100466251373292,
                "training_s": 0.061902382373809815
            },
            "pytorch_torch_compile_eager": {
                "inference_s": 0.024616193771362305,
                "training_s": 0.08525827407836914
            },
            "pytorch_torch_compile_cudagraphs": {
                "inference_s": 0.04821393251419068,
                "training_s": 0.12888015508651735
            },
            "pytorch_torch_compile_onnxrt": {
                "inference_s": 0.028959681987762453,
                "training_s": 0.12833453178405763
            },
            "pytorch_torch_compile_openxla": {
                "inference_s": 0.028580038547515868,
                "training_s": 0.08937520503997803
            },
            "pytorch_torch_compile_tvm": {
                "inference_s": 0.02840841770172119,
                "training_s": 0.08892325639724731
            },
            "jax_flax_xla": {
                "inference_s": 0.0007437586784362793,
                "training_s": 0.002564246654510498
            },
            "tensorflow_eager": {
                "inference_s": 0.1890813136100769,
                "training_s": 0.4248156976699829
            },
            "tensorflow_xla": {
                "inference_s": 0.0015209388732910156,
                "training_s": 0.005039141178131103
            }
        }
    },
    {
        "AlbertForQuestionAnswering": {
            "pytorch_torch_compile_inductor": {
                "inference_s": 0.015715312957763673,
                "training_s": 0.05026669263839722
            },
            "pytorch_torch_compile_eager": {
                "inference_s": 0.022517130374908448,
                "training_s": 0.08483960628509521
            },
            "pytorch_torch_compile_cudagraphs": {
                "inference_s": 0.048532476425170896,
                "training_s": 0.1225699782371521
            },
            "pytorch_torch_compile_onnxrt": {
                "inference_s": 0.024739091396331788,
                "training_s": 0.10671566724777222
            },
            "pytorch_torch_compile_openxla": {
                "inference_s": 0.02663666009902954,
                "training_s": 0.08497506618499756
            },
            "pytorch_torch_compile_tvm": {
                "inference_s": 0.029220511913299562,
                "training_s": 0.08518055438995362
            },
            "jax_flax_xla": {
                "inference_s": 0.0007474374771118164,
                "training_s": 0.0026793694496154786
            },
            "tensorflow_eager": {
                "inference_s": 0.19077762126922607,
                "training_s": 0.41417508363723754
            },
            "tensorflow_xla": {
                "inference_s": 0.0014851093292236328,
                "training_s": 0.005166933536529541
            }
        }
    },
    {
        "AllenaiLongformerBase": {
            "pytorch_torch_compile_inductor": {
                "inference_s": 0.30670461893081663,
                "training_s": 1.1909909057617187
            },
            "pytorch_torch_compile_eager": {
                "inference_s": 0.3607925844192505,
                "training_s": 1.28913907289505
            },
            "pytorch_torch_compile_cudagraphs": {
                "inference_s": 0.773241994380951,
                "training_s": 1.9166420817375183
            },
            "pytorch_torch_compile_onnxrt": {
                "inference_s": 3.551206374168396,
                "training_s": 10.2787002825737
            },
            "pytorch_torch_compile_openxla": {
                "error": "torch_xla/csrc/runtime/pjrt_registry.cc:214 : Check failed: client \n*** Begin stack trace ***\n\ttsl::CurrentStackTrace()\n\ttorch_xla::runtime::InitializePjRt(std::string const&)\n\ttorch_xla::runtime::PjRtComputationClient::PjRtComputationClient()\n\t\n\ttorch_xla::runtime::GetComputationClient()\n\ttorch_xla::bridge::GetDefaultDevice()\n\ttorch_xla::bridge::GetCurrentDevice()\n\ttorch_xla::bridge::GetCurrentAtenDevice()\n\t\n\t\n\t\n\t_PyObject_MakeTpCall\n\t_PyEval_EvalFrameDefault\n\t_PyFunction_Vectorcall\n\t_PyEval_EvalFrameDefault\n\t_PyFunction_Vectorcall\n\t_PyEval_EvalFrameDefault\n\t_PyFunction_Vectorcall\n\t_PyEval_EvalFrameDefault\n\t_PyFunction_Vectorcall\n\t_PyEval_EvalFrameDefault\n\t_PyFunction_Vectorcall\n\t_PyEval_EvalFrameDefault\n\t_PyFunction_Vectorcall\n\t_PyEval_EvalFrameDefault\n\t_PyFunction_Vectorcall\n\t_PyEval_EvalFrameDefault\n\t_PyFunction_Vectorcall\n\t_PyEval_EvalFrameDefault\n\t_PyFunction_Vectorcall\n\t_PyEval_EvalFrameDefault\n\t_PyFunction_Vectorcall\n\t_PyEval_EvalFrameDefault\n\t\n\t_PyFunction_Vectorcall\n\t_PyEval_EvalFrameDefault\n\t\n\t\n\t\n\tPyObject_Call\n\t_PyEval_EvalFrameDefault\n\t\n\t\n\tPyObject_Call\n\t_PyEval_EvalFrameDefault\n\t\n\t_PyFunction_Vectorcall\n\t_PyObject_FastCallDictTstate\n\t_PyObject_Call_Prepend\n\t\n\t_PyObject_MakeTpCall\n\t_PyEval_EvalFrameDefault\n\t\n\t\n\t\n\tPyObject_Call\n\t_PyEval_EvalFrameDefault\n\t\n\t\n\tPyObject_Call\n\t_PyEval_EvalFrameDefault\n\t\n\t\n\tPyObject_Call\n\t_PyEval_EvalFrameDefault\n\t_PyFunction_Vectorcall\n\tPyObject_Call\n\t_PyEval_EvalFrameDefault\n\t\n\tPyObject_Call\n\t_PyEval_EvalFrameDefault\n\t_PyFunction_Vectorcall\n\t_PyObject_FastCallDictTstate\n\t_PyObject_Call_Prepend\n\t\n\tPyObject_Call\n\t_PyEval_EvalFrameDefault\n\t_PyFunction_Vectorcall\n\t_PyEval_EvalFrameDefault\n\t_PyFunction_Vectorcall\n\t_PyEval_EvalFrameDefault\n\t\n\tPyEval_EvalCode\n\t\n\t\n\t\n\t_PyRun_SimpleFileObject\n\t_PyRun_AnyFileObject\n\tPy_RunMain\n\tPy_BytesMain\n\t__libc_start_main\n\t\n*** End stack trace ***\nUnknown PJRT_DEVICE 'CUDA'"
            },
            "pytorch_torch_compile_tvm": {
                "inference_s": 0.3801512861251831,
                "training_s": 1.2937897491455077
            },
            "jax_flax_xla": {
                "error": "Unrecognized configuration class <class 'transformers.models.longformer.configuration_longformer.LongformerConfig'> for this kind of AutoModel: FlaxAutoModel.\nModel type should be one of AlbertConfig, BartConfig, BeitConfig, BertConfig, BigBirdConfig, BlenderbotConfig, BlenderbotSmallConfig, BloomConfig, CLIPConfig, Dinov2Config, DistilBertConfig, ElectraConfig, GemmaConfig, GPT2Config, GPT2Config, GPTNeoConfig, GPTJConfig, LlamaConfig, LongT5Config, MarianConfig, MBartConfig, MistralConfig, MT5Config, OPTConfig, PegasusConfig, RegNetConfig, ResNetConfig, RobertaConfig, RobertaPreLayerNormConfig, RoFormerConfig, T5Config, VisionTextDualEncoderConfig, ViTConfig, Wav2Vec2Config, WhisperConfig, XGLMConfig, XLMRobertaConfig."
            },
            "tensorflow_eager": {
                "inference_s": 0.8666441702842712,
                "training_s": 1.599133026599884
            },
            "tensorflow_xla": {
                "error": "0 is duplicated in the input.\n\nStack trace for op definition: \nFile \"home/mmoffatt/jax_vs_pytorch/benchmark.py\", line 402, in <module>\nFile \"home/mmoffatt/jax_vs_pytorch/benchmark.py\", line 381, in run_benchmarks\nFile \"home/mmoffatt/jax_vs_pytorch/benchmark.py\", line 221, in benchmark_training_tf_xla\nFile \"home/mmoffatt/jax_vs_pytorch/benchmark.py\", line 215, in train_step\n\n\t [[{{node gradient_tape/tf_longformer_for_masked_lm_1/longformer/encoder/layer_._11/attention/self/cond_3/gradients/tf_longformer_for_masked_lm_1/longformer/encoder/layer_._11/attention/self/cond_3/query_global/Tensordot/transpose_grad/InvertPermutation}}]]\n\ttf2xla conversion failed while converting tf_longformer_for_masked_lm_1_longformer_encoder_layer_._11_attention_self_cond_3_true_4374079_grad_4374764[]. Run with TF_DUMP_GRAPH_PREFIX=/path/to/dump/dir and --vmodule=xla_compiler=2 to obtain a dump of the compiled functions.\n\nStack trace for op definition: \nFile \"home/mmoffatt/jax_vs_pytorch/benchmark.py\", line 402, in <module>\nFile \"home/mmoffatt/jax_vs_pytorch/benchmark.py\", line 381, in run_benchmarks\nFile \"home/mmoffatt/jax_vs_pytorch/benchmark.py\", line 221, in benchmark_training_tf_xla\nFile \"home/mmoffatt/jax_vs_pytorch/benchmark.py\", line 215, in train_step\n\n\t [[gradient_tape/tf_longformer_for_masked_lm_1/longformer/encoder/layer_._11/attention/self/cond_3/StatelessIf]]\n\ttf2xla conversion failed while converting __inference_train_step_4389267[_XlaMustCompile=true,config_proto=2201667018877855759,executor_type=11160318154034397263]. Run with TF_DUMP_GRAPH_PREFIX=/path/to/dump/dir and --vmodule=xla_compiler=2 to obtain a dump of the compiled functions. [Op:__inference_train_step_4389267]"
            }
        }
    },
    {
        "BartForCausalLM": {
            "pytorch_torch_compile_inductor": {
                "inference_s": 0.049907243251800536,
                "training_s": 0.22308647155761718
            },
            "pytorch_torch_compile_eager": {
                "inference_s": 0.061008107662200925,
                "training_s": 0.22945211410522462
            },
            "pytorch_torch_compile_cudagraphs": {
                "inference_s": 0.0915701699256897,
                "training_s": 0.27514387130737306
            },
            "pytorch_torch_compile_onnxrt": {
                "inference_s": 0.20790636539459229,
                "training_s": 0.4427159261703491
            },
            "pytorch_torch_compile_openxla": {
                "inference_s": 0.062346968650817874,
                "training_s": 0.2291019105911255
            },
            "pytorch_torch_compile_tvm": {
                "inference_s": 0.0617411994934082,
                "training_s": 0.23290417909622194
            },
            "jax_flax_xla": {
                "inference_s": 0.001906111240386963,
                "training_s": 0.00865433931350708
            },
            "tensorflow_eager": {
                "inference_s": 0.6504508042335511,
                "training_s": 1.604433732032776
            },
            "tensorflow_xla": {
                "inference_s": 0.006454174518585205,
                "training_s": 0.017441701889038087
            }
        }
    },
    {
        "BartForConditionalGeneration": {
            "pytorch_torch_compile_inductor": {
                "inference_s": 0.09804497718811035,
                "training_s": 0.47864279508590696
            },
            "pytorch_torch_compile_eager": {
                "inference_s": 0.12513136386871337,
                "training_s": 0.5387560677528381
            },
            "pytorch_torch_compile_cudagraphs": {
                "inference_s": 0.1537837266921997,
                "training_s": 0.5769260239601135
            },
            "pytorch_torch_compile_onnxrt": {
                "error": "backend='onnxrt' raised:\nAssertionError: Please convert all Tensors to FakeTensors first or instantiate FakeTensorMode with 'allow_non_fake_inputs'. Found in aten.alias.default(tensor(2, size=()))\n\nWhile executing %setitem_1 : [num_users=0] = call_function[target=operator.setitem](args = (%shifted_input_ids, (slice(None, None, None), 0), 2), kwargs = {})\nOriginal traceback:\n  File \"/opt/conda/envs/xla-env/lib/python3.10/site-packages/transformers/models/bart/modeling_bart.py\", line 1655, in forward\n    outputs = self.model(\n  File \"/opt/conda/envs/xla-env/lib/python3.10/site-packages/transformers/models/bart/modeling_bart.py\", line 1504, in forward\n    decoder_input_ids = shift_tokens_right(\n  File \"/opt/conda/envs/xla-env/lib/python3.10/site-packages/transformers/models/bart/modeling_bart.py\", line 87, in shift_tokens_right\n    shifted_input_ids[:, 0] = decoder_start_token_id\n\n\nSet TORCH_LOGS=\"+dynamo\" and TORCHDYNAMO_VERBOSE=1 for more information\n\n\nYou can suppress this exception and fall back to eager by setting:\n    import torch._dynamo\n    torch._dynamo.config.suppress_errors = True\n"
            },
            "pytorch_torch_compile_openxla": {
                "inference_s": 0.12295098781585694,
                "training_s": 0.5385088610649109
            },
            "pytorch_torch_compile_tvm": {
                "inference_s": 0.12279985189437866,
                "training_s": 0.5383733010292053
            },
            "jax_flax_xla": {
                "inference_s": 0.0022248578071594237,
                "training_s": 0.007985684871673584
            },
            "error": "AttributeError(\"'BartForConditionalGeneration' object has no attribute '_shift_right'\")"
        }
    },
    {
        "BertForMaskedLM": {
            "pytorch_torch_compile_inductor": {
                "inference_s": 0.029893267154693603,
                "training_s": 0.10914779663085937
            },
            "pytorch_torch_compile_eager": {
                "inference_s": 0.035263431072235105,
                "training_s": 0.11940330743789673
            },
            "pytorch_torch_compile_cudagraphs": {
                "inference_s": 0.06131981611251831,
                "training_s": 0.15159297704696656
            },
            "pytorch_torch_compile_onnxrt": {
                "inference_s": 0.1243315577507019,
                "training_s": 0.25059462547302247
            },
            "pytorch_torch_compile_openxla": {
                "inference_s": 0.04026915073394775,
                "training_s": 0.1264740514755249
            },
            "pytorch_torch_compile_tvm": {
                "inference_s": 0.04089537620544434,
                "training_s": 0.1333151388168335
            },
            "jax_flax_xla": {
                "inference_s": 0.0008049297332763672,
                "training_s": 0.004240846633911133
            },
            "tensorflow_eager": {
                "inference_s": 0.18956151247024536,
                "training_s": 0.5587964487075806
            },
            "tensorflow_xla": {
                "inference_s": 0.0028113436698913572,
                "training_s": 0.006826703548431397
            }
        }
    },
    {
        "BertForQuestionAnswering": {
            "pytorch_torch_compile_inductor": {
                "inference_s": 0.022814204692840578,
                "training_s": 0.0927875804901123
            },
            "pytorch_torch_compile_eager": {
                "inference_s": 0.029385123252868652,
                "training_s": 0.10736689567565919
            },
            "pytorch_torch_compile_cudagraphs": {
                "inference_s": 0.055239810943603515,
                "training_s": 0.14156921625137328
            },
            "pytorch_torch_compile_onnxrt": {
                "inference_s": 0.09625702142715455,
                "training_s": 0.18707181692123412
            },
            "pytorch_torch_compile_openxla": {
                "inference_s": 0.03362046718597412,
                "training_s": 0.11213145494461059
            },
            "pytorch_torch_compile_tvm": {
                "inference_s": 0.03372425556182861,
                "training_s": 0.11167156457901001
            },
            "jax_flax_xla": {
                "inference_s": 0.0007921814918518067,
                "training_s": 0.004440436363220215
            },
            "tensorflow_eager": {
                "inference_s": 0.18728304862976075,
                "training_s": 0.5539432740211487
            },
            "tensorflow_xla": {
                "inference_s": 0.0027457618713378905,
                "training_s": 0.0070023941993713375
            }
        }
    },
    {
        "BigBird": {
            "pytorch_torch_compile_inductor": {
                "inference_s": 0.033681397438049314,
                "training_s": 0.19432084798812865
            },
            "pytorch_torch_compile_eager": {
                "inference_s": 0.040601518154144284,
                "training_s": 0.1925595498085022
            },
            "pytorch_torch_compile_cudagraphs": {
                "inference_s": 0.0690289831161499,
                "training_s": 0.22317550659179688
            },
            "pytorch_torch_compile_onnxrt": {
                "inference_s": 0.35567219495773317,
                "training_s": 0.763629503250122
            },
            "pytorch_torch_compile_openxla": {
                "inference_s": 0.04646574974060059,
                "training_s": 0.15757113695144653
            },
            "pytorch_torch_compile_tvm": {
                "inference_s": 0.04685439109802246,
                "training_s": 0.160214102268219
            },
            "jax_flax_xla": {
                "error": "Sequence length must be multiple of block size, but sequence length is 12, while block size is 64."
            },
            "tensorflow_eager": {
                "error": "Unrecognized configuration class <class 'transformers.models.big_bird.configuration_big_bird.BigBirdConfig'> for this kind of AutoModel: TFAutoModel.\nModel type should be one of AlbertConfig, BartConfig, BertConfig, BlenderbotConfig, BlenderbotSmallConfig, BlipConfig, CamembertConfig, CLIPConfig, ConvBertConfig, ConvNextConfig, ConvNextV2Config, CTRLConfig, CvtConfig, Data2VecVisionConfig, DebertaConfig, DebertaV2Config, DeiTConfig, DistilBertConfig, DPRConfig, EfficientFormerConfig, ElectraConfig, EsmConfig, FlaubertConfig, FunnelConfig, GPT2Config, GPT2Config, GPTJConfig, GroupViTConfig, HubertConfig, IdeficsConfig, LayoutLMConfig, LayoutLMv3Config, LEDConfig, LongformerConfig, LxmertConfig, MarianConfig, MBartConfig, MistralConfig, MobileBertConfig, MobileViTConfig, MPNetConfig, MT5Config, OpenAIGPTConfig, OPTConfig, PegasusConfig, RegNetConfig, RemBertConfig, ResNetConfig, RobertaConfig, RobertaPreLayerNormConfig, RoFormerConfig, SamConfig, SegformerConfig, Speech2TextConfig, SwiftFormerConfig, SwinConfig, T5Config, TapasConfig, TransfoXLConfig, VisionTextDualEncoderConfig, ViTConfig, ViTMAEConfig, Wav2Vec2Config, WhisperConfig, XGLMConfig, XLMConfig, XLMRobertaConfig, XLNetConfig."
            },
            "tensorflow_xla": {
                "error": "Unrecognized configuration class <class 'transformers.models.big_bird.configuration_big_bird.BigBirdConfig'> for this kind of AutoModel: TFAutoModel.\nModel type should be one of AlbertConfig, BartConfig, BertConfig, BlenderbotConfig, BlenderbotSmallConfig, BlipConfig, CamembertConfig, CLIPConfig, ConvBertConfig, ConvNextConfig, ConvNextV2Config, CTRLConfig, CvtConfig, Data2VecVisionConfig, DebertaConfig, DebertaV2Config, DeiTConfig, DistilBertConfig, DPRConfig, EfficientFormerConfig, ElectraConfig, EsmConfig, FlaubertConfig, FunnelConfig, GPT2Config, GPT2Config, GPTJConfig, GroupViTConfig, HubertConfig, IdeficsConfig, LayoutLMConfig, LayoutLMv3Config, LEDConfig, LongformerConfig, LxmertConfig, MarianConfig, MBartConfig, MistralConfig, MobileBertConfig, MobileViTConfig, MPNetConfig, MT5Config, OpenAIGPTConfig, OPTConfig, PegasusConfig, RegNetConfig, RemBertConfig, ResNetConfig, RobertaConfig, RobertaPreLayerNormConfig, RoFormerConfig, SamConfig, SegformerConfig, Speech2TextConfig, SwiftFormerConfig, SwinConfig, T5Config, TapasConfig, TransfoXLConfig, VisionTextDualEncoderConfig, ViTConfig, ViTMAEConfig, Wav2Vec2Config, WhisperConfig, XGLMConfig, XLMConfig, XLMRobertaConfig, XLNetConfig."
            }
        }
    },
    {
        "BlenderbotForCausalLM": {
            "pytorch_torch_compile_inductor": {
                "inference_s": 0.07039751768112183,
                "training_s": 0.2655063271522522
            },
            "pytorch_torch_compile_eager": {
                "inference_s": 0.08371442556381226,
                "training_s": 0.2345528507232666
            },
            "pytorch_torch_compile_cudagraphs": {
                "inference_s": 0.11619025468826294,
                "training_s": 0.28292778730392454
            },
            "pytorch_torch_compile_onnxrt": {
                "inference_s": 0.26532081127166746,
                "training_s": 0.5178327369689941
            },
            "pytorch_torch_compile_openxla": {
                "inference_s": 0.0838738465309143,
                "training_s": 0.24598398208618164
            },
            "pytorch_torch_compile_tvm": {
                "inference_s": 0.08566758632659913,
                "training_s": 0.24059807538986205
            },
            "jax_flax_xla": {
                "inference_s": 0.0016313719749450684,
                "training_s": 0.006185884475708008
            },
            "tensorflow_eager": {
                "error": "Exception encountered when calling layer 'decoder' (type TFBlenderbotDecoder).\n\nYou have to specify either decoder_input_ids or decoder_inputs_embeds\n\nCall arguments received by layer 'decoder' (type TFBlenderbotDecoder):\n  \u2022 input_ids=None\n  \u2022 inputs_embeds=None\n  \u2022 attention_mask=None\n  \u2022 position_ids=None\n  \u2022 encoder_hidden_states=tf.Tensor(shape=(1, 32, 1280), dtype=float32)\n  \u2022 encoder_attention_mask=tf.Tensor(shape=(1, 32), dtype=int32)\n  \u2022 head_mask=None\n  \u2022 cross_attn_head_mask=None\n  \u2022 past_key_values=None\n  \u2022 use_cache=True\n  \u2022 output_attentions=False\n  \u2022 output_hidden_states=False\n  \u2022 return_dict=True\n  \u2022 training=False"
            },
            "tensorflow_xla": {
                "error": "in user code:\n\n    File \"/home/mmoffatt/jax_vs_pytorch/benchmark.py\", line 193, in fwd  *\n        return model(**inp)[0]\n    File \"/opt/conda/envs/xla-env/lib/python3.10/site-packages/tf_keras/src/utils/traceback_utils.py\", line 70, in error_handler  **\n        raise e.with_traceback(filtered_tb) from None\n    File \"/var/tmp/__autograph_generated_fileso2b_d_j.py\", line 37, in tf__run_call_with_unpacked_inputs\n        retval_ = ag__.converted_call(ag__.ld(func), (ag__.ld(self),), dict(**ag__.ld(unpacked_inputs)), fscope)\n    File \"/var/tmp/__autograph_generated_filed1m9aeqq.py\", line 11, in tf__call\n        outputs = ag__.converted_call(ag__.ld(self).model, (), dict(input_ids=ag__.ld(input_ids), attention_mask=ag__.ld(attention_mask), decoder_input_ids=ag__.ld(decoder_input_ids), decoder_attention_mask=ag__.ld(decoder_attention_mask), decoder_position_ids=ag__.ld(decoder_position_ids), head_mask=ag__.ld(head_mask), decoder_head_mask=ag__.ld(decoder_head_mask), cross_attn_head_mask=ag__.ld(cross_attn_head_mask), encoder_outputs=ag__.ld(encoder_outputs), past_key_values=ag__.ld(past_key_values), inputs_embeds=ag__.ld(inputs_embeds), decoder_inputs_embeds=ag__.ld(decoder_inputs_embeds), use_cache=ag__.ld(use_cache), output_attentions=ag__.ld(output_attentions), output_hidden_states=ag__.ld(output_hidden_states), return_dict=ag__.ld(return_dict), training=ag__.ld(training)), fscope)\n    File \"/var/tmp/__autograph_generated_fileso2b_d_j.py\", line 37, in tf__run_call_with_unpacked_inputs\n        retval_ = ag__.converted_call(ag__.ld(func), (ag__.ld(self),), dict(**ag__.ld(unpacked_inputs)), fscope)\n    File \"/var/tmp/__autograph_generated_fileqf222_ip.py\", line 58, in tf__call\n        decoder_outputs = ag__.converted_call(ag__.ld(self).decoder, (ag__.ld(decoder_input_ids),), dict(attention_mask=ag__.ld(decoder_attention_mask), position_ids=ag__.ld(decoder_position_ids), encoder_hidden_states=ag__.ld(encoder_outputs)[0], encoder_attention_mask=ag__.ld(attention_mask), head_mask=ag__.ld(decoder_head_mask), cross_attn_head_mask=ag__.ld(cross_attn_head_mask), past_key_values=ag__.ld(past_key_values), inputs_embeds=ag__.ld(decoder_inputs_embeds), use_cache=ag__.ld(use_cache), output_attentions=ag__.ld(output_attentions), output_hidden_states=ag__.ld(output_hidden_states), return_dict=ag__.ld(return_dict), training=ag__.ld(training)), fscope)\n    File \"/var/tmp/__autograph_generated_fileso2b_d_j.py\", line 37, in tf__run_call_with_unpacked_inputs\n        retval_ = ag__.converted_call(ag__.ld(func), (ag__.ld(self),), dict(**ag__.ld(unpacked_inputs)), fscope)\n    File \"/var/tmp/__autograph_generated_file3l7wmxge.py\", line 128, in tf__call\n        ag__.if_stmt(ag__.and_(lambda : ag__.ld(input_ids) is not None, lambda : ag__.ld(inputs_embeds) is not None), if_body_2, else_body_2, get_state_2, set_state_2, ('input_shape',), 1)\n    File \"/var/tmp/__autograph_generated_file3l7wmxge.py\", line 126, in else_body_2\n        ag__.if_stmt(ag__.ld(input_ids) is not None, if_body_1, else_body_1, get_state_1, set_state_1, ('input_shape',), 1)\n    File \"/var/tmp/__autograph_generated_file3l7wmxge.py\", line 124, in else_body_1\n        ag__.if_stmt(ag__.ld(inputs_embeds) is not None, if_body, else_body, get_state, set_state, ('input_shape',), 1)\n    File \"/var/tmp/__autograph_generated_file3l7wmxge.py\", line 122, in else_body\n        raise ag__.converted_call(ag__.ld(ValueError), ('You have to specify either decoder_input_ids or decoder_inputs_embeds',), None, fscope)\n\n    ValueError: Exception encountered when calling layer 'tf_blenderbot_model_1' (type TFBlenderbotModel).\n    \n    in user code:\n    \n        File \"/opt/conda/envs/xla-env/lib/python3.10/site-packages/transformers/modeling_tf_utils.py\", line 1079, in run_call_with_unpacked_inputs  *\n            return func(self, **unpacked_inputs)\n        File \"/opt/conda/envs/xla-env/lib/python3.10/site-packages/transformers/models/blenderbot/modeling_tf_blenderbot.py\", line 1276, in call  *\n            outputs = self.model(\n        File \"/opt/conda/envs/xla-env/lib/python3.10/site-packages/tf_keras/src/utils/traceback_utils.py\", line 70, in error_handler  **\n            raise e.with_traceback(filtered_tb) from None\n        File \"/var/tmp/__autograph_generated_fileso2b_d_j.py\", line 37, in tf__run_call_with_unpacked_inputs\n            retval_ = ag__.converted_call(ag__.ld(func), (ag__.ld(self),), dict(**ag__.ld(unpacked_inputs)), fscope)\n        File \"/var/tmp/__autograph_generated_fileqf222_ip.py\", line 58, in tf__call\n            decoder_outputs = ag__.converted_call(ag__.ld(self).decoder, (ag__.ld(decoder_input_ids),), dict(attention_mask=ag__.ld(decoder_attention_mask), position_ids=ag__.ld(decoder_position_ids), encoder_hidden_states=ag__.ld(encoder_outputs)[0], encoder_attention_mask=ag__.ld(attention_mask), head_mask=ag__.ld(decoder_head_mask), cross_attn_head_mask=ag__.ld(cross_attn_head_mask), past_key_values=ag__.ld(past_key_values), inputs_embeds=ag__.ld(decoder_inputs_embeds), use_cache=ag__.ld(use_cache), output_attentions=ag__.ld(output_attentions), output_hidden_states=ag__.ld(output_hidden_states), return_dict=ag__.ld(return_dict), training=ag__.ld(training)), fscope)\n        File \"/var/tmp/__autograph_generated_fileso2b_d_j.py\", line 37, in tf__run_call_with_unpacked_inputs\n            retval_ = ag__.converted_call(ag__.ld(func), (ag__.ld(self),), dict(**ag__.ld(unpacked_inputs)), fscope)\n        File \"/var/tmp/__autograph_generated_file3l7wmxge.py\", line 128, in tf__call\n            ag__.if_stmt(ag__.and_(lambda : ag__.ld(input_ids) is not None, lambda : ag__.ld(inputs_embeds) is not None), if_body_2, else_body_2, get_state_2, set_state_2, ('input_shape',), 1)\n        File \"/var/tmp/__autograph_generated_file3l7wmxge.py\", line 126, in else_body_2\n            ag__.if_stmt(ag__.ld(input_ids) is not None, if_body_1, else_body_1, get_state_1, set_state_1, ('input_shape',), 1)\n        File \"/var/tmp/__autograph_generated_file3l7wmxge.py\", line 124, in else_body_1\n            ag__.if_stmt(ag__.ld(inputs_embeds) is not None, if_body, else_body, get_state, set_state, ('input_shape',), 1)\n        File \"/var/tmp/__autograph_generated_file3l7wmxge.py\", line 122, in else_body\n            raise ag__.converted_call(ag__.ld(ValueError), ('You have to specify either decoder_input_ids or decoder_inputs_embeds',), None, fscope)\n    \n        ValueError: Exception encountered when calling layer 'model' (type TFBlenderbotMainLayer).\n        \n        in user code:\n        \n            File \"/opt/conda/envs/xla-env/lib/python3.10/site-packages/transformers/modeling_tf_utils.py\", line 1079, in run_call_with_unpacked_inputs  *\n                return func(self, **unpacked_inputs)\n            File \"/opt/conda/envs/xla-env/lib/python3.10/site-packages/transformers/models/blenderbot/modeling_tf_blenderbot.py\", line 1168, in call  *\n                decoder_outputs = self.decoder(\n            File \"/opt/conda/envs/xla-env/lib/python3.10/site-packages/tf_keras/src/utils/traceback_utils.py\", line 70, in error_handler  **\n                raise e.with_traceback(filtered_tb) from None\n            File \"/var/tmp/__autograph_generated_fileso2b_d_j.py\", line 37, in tf__run_call_with_unpacked_inputs\n                retval_ = ag__.converted_call(ag__.ld(func), (ag__.ld(self),), dict(**ag__.ld(unpacked_inputs)), fscope)\n            File \"/var/tmp/__autograph_generated_file3l7wmxge.py\", line 128, in tf__call\n                ag__.if_stmt(ag__.and_(lambda : ag__.ld(input_ids) is not None, lambda : ag__.ld(inputs_embeds) is not None), if_body_2, else_body_2, get_state_2, set_state_2, ('input_shape',), 1)\n            File \"/var/tmp/__autograph_generated_file3l7wmxge.py\", line 126, in else_body_2\n                ag__.if_stmt(ag__.ld(input_ids) is not None, if_body_1, else_body_1, get_state_1, set_state_1, ('input_shape',), 1)\n            File \"/var/tmp/__autograph_generated_file3l7wmxge.py\", line 124, in else_body_1\n                ag__.if_stmt(ag__.ld(inputs_embeds) is not None, if_body, else_body, get_state, set_state, ('input_shape',), 1)\n            File \"/var/tmp/__autograph_generated_file3l7wmxge.py\", line 122, in else_body\n                raise ag__.converted_call(ag__.ld(ValueError), ('You have to specify either decoder_input_ids or decoder_inputs_embeds',), None, fscope)\n        \n            ValueError: Exception encountered when calling layer 'decoder' (type TFBlenderbotDecoder).\n            \n            in user code:\n            \n                File \"/opt/conda/envs/xla-env/lib/python3.10/site-packages/transformers/modeling_tf_utils.py\", line 1079, in run_call_with_unpacked_inputs  *\n                    return func(self, **unpacked_inputs)\n                File \"/opt/conda/envs/xla-env/lib/python3.10/site-packages/transformers/models/blenderbot/modeling_tf_blenderbot.py\", line 979, in call  *\n                    raise ValueError(\"You have to specify either decoder_input_ids or decoder_inputs_embeds\")\n            \n                ValueError: You have to specify either decoder_input_ids or decoder_inputs_embeds\n            \n            \n            Call arguments received by layer 'decoder' (type TFBlenderbotDecoder):\n              \u2022 input_ids=None\n              \u2022 inputs_embeds=None\n              \u2022 attention_mask=None\n              \u2022 position_ids=None\n              \u2022 encoder_hidden_states=tf.Tensor(shape=(1, 32, 1280), dtype=float32)\n              \u2022 encoder_attention_mask=tf.Tensor(shape=(1, 32), dtype=int32)\n              \u2022 head_mask=None\n              \u2022 cross_attn_head_mask=None\n              \u2022 past_key_values=None\n              \u2022 use_cache=True\n              \u2022 output_attentions=False\n              \u2022 output_hidden_states=False\n              \u2022 return_dict=True\n              \u2022 training=False\n        \n        \n        Call arguments received by layer 'model' (type TFBlenderbotMainLayer):\n          \u2022 input_ids=tf.Tensor(shape=(1, 32), dtype=int32)\n          \u2022 attention_mask=tf.Tensor(shape=(1, 32), dtype=int32)\n          \u2022 decoder_input_ids=None\n          \u2022 decoder_attention_mask=None\n          \u2022 decoder_position_ids=None\n          \u2022 head_mask=None\n          \u2022 decoder_head_mask=None\n          \u2022 cross_attn_head_mask=None\n          \u2022 encoder_outputs=None\n          \u2022 past_key_values=None\n          \u2022 inputs_embeds=None\n          \u2022 decoder_inputs_embeds=None\n          \u2022 use_cache=True\n          \u2022 output_attentions=False\n          \u2022 output_hidden_states=False\n          \u2022 return_dict=True\n          \u2022 training=False\n          \u2022 kwargs=<class 'inspect._empty'>\n    \n    \n    Call arguments received by layer 'tf_blenderbot_model_1' (type TFBlenderbotModel):\n      \u2022 input_ids=tf.Tensor(shape=(1, 32), dtype=int32)\n      \u2022 attention_mask=tf.Tensor(shape=(1, 32), dtype=int32)\n      \u2022 decoder_input_ids=None\n      \u2022 decoder_attention_mask=None\n      \u2022 decoder_position_ids=None\n      \u2022 head_mask=None\n      \u2022 decoder_head_mask=None\n      \u2022 cross_attn_head_mask=None\n      \u2022 encoder_outputs=None\n      \u2022 past_key_values=None\n      \u2022 inputs_embeds=None\n      \u2022 decoder_inputs_embeds=None\n      \u2022 use_cache=None\n      \u2022 output_attentions=None\n      \u2022 output_hidden_states=None\n      \u2022 return_dict=None\n      \u2022 training=False\n      \u2022 kwargs=<class 'inspect._empty'>\n"
            }
        }
    },
    {
        "BlenderbotSmallForCausalLM": {
            "pytorch_torch_compile_inductor": {
                "inference_s": 0.01624600648880005,
                "training_s": 0.07349587202072144
            },
            "pytorch_torch_compile_eager": {
                "inference_s": 0.02271538019180298,
                "training_s": 0.08350432634353638
            },
            "pytorch_torch_compile_cudagraphs": {
                "inference_s": 0.040918357372283935,
                "training_s": 0.1065909743309021
            },
            "pytorch_torch_compile_onnxrt": {
                "inference_s": 0.07826354026794434,
                "training_s": 0.17006351947784423
            },
            "pytorch_torch_compile_openxla": {
                "inference_s": 0.022514171600341797,
                "training_s": 0.08849172115325928
            },
            "pytorch_torch_compile_tvm": {
                "inference_s": 0.02254645586013794,
                "training_s": 0.08801054000854493
            },
            "jax_flax_xla": {
                "inference_s": 0.0011029362678527832,
                "training_s": 0.006510429382324219
            },
            "tensorflow_eager": {
                "error": "Exception encountered when calling layer 'decoder' (type TFBlenderbotSmallDecoder).\n\nYou have to specify either decoder_input_ids or decoder_inputs_embeds\n\nCall arguments received by layer 'decoder' (type TFBlenderbotSmallDecoder):\n  \u2022 input_ids=None\n  \u2022 inputs_embeds=None\n  \u2022 attention_mask=None\n  \u2022 position_ids=None\n  \u2022 encoder_hidden_states=tf.Tensor(shape=(1, 32, 512), dtype=float32)\n  \u2022 encoder_attention_mask=tf.Tensor(shape=(1, 32), dtype=int32)\n  \u2022 head_mask=None\n  \u2022 cross_attn_head_mask=None\n  \u2022 past_key_values=None\n  \u2022 use_cache=True\n  \u2022 output_attentions=False\n  \u2022 output_hidden_states=False\n  \u2022 return_dict=True\n  \u2022 training=False"
            },
            "tensorflow_xla": {
                "error": "in user code:\n\n    File \"/home/mmoffatt/jax_vs_pytorch/benchmark.py\", line 193, in fwd  *\n        return model(**inp)[0]\n    File \"/opt/conda/envs/xla-env/lib/python3.10/site-packages/tf_keras/src/utils/traceback_utils.py\", line 70, in error_handler  **\n        raise e.with_traceback(filtered_tb) from None\n    File \"/var/tmp/__autograph_generated_fileso2b_d_j.py\", line 37, in tf__run_call_with_unpacked_inputs\n        retval_ = ag__.converted_call(ag__.ld(func), (ag__.ld(self),), dict(**ag__.ld(unpacked_inputs)), fscope)\n    File \"/var/tmp/__autograph_generated_filef7hrefya.py\", line 11, in tf__call\n        outputs = ag__.converted_call(ag__.ld(self).model, (), dict(input_ids=ag__.ld(input_ids), attention_mask=ag__.ld(attention_mask), decoder_input_ids=ag__.ld(decoder_input_ids), decoder_attention_mask=ag__.ld(decoder_attention_mask), decoder_position_ids=ag__.ld(decoder_position_ids), head_mask=ag__.ld(head_mask), decoder_head_mask=ag__.ld(decoder_head_mask), cross_attn_head_mask=ag__.ld(cross_attn_head_mask), encoder_outputs=ag__.ld(encoder_outputs), past_key_values=ag__.ld(past_key_values), inputs_embeds=ag__.ld(inputs_embeds), decoder_inputs_embeds=ag__.ld(decoder_inputs_embeds), use_cache=ag__.ld(use_cache), output_attentions=ag__.ld(output_attentions), output_hidden_states=ag__.ld(output_hidden_states), return_dict=ag__.ld(return_dict), training=ag__.ld(training)), fscope)\n    File \"/var/tmp/__autograph_generated_fileso2b_d_j.py\", line 37, in tf__run_call_with_unpacked_inputs\n        retval_ = ag__.converted_call(ag__.ld(func), (ag__.ld(self),), dict(**ag__.ld(unpacked_inputs)), fscope)\n    File \"/var/tmp/__autograph_generated_file9cel92d9.py\", line 58, in tf__call\n        decoder_outputs = ag__.converted_call(ag__.ld(self).decoder, (ag__.ld(decoder_input_ids),), dict(attention_mask=ag__.ld(decoder_attention_mask), position_ids=ag__.ld(decoder_position_ids), encoder_hidden_states=ag__.ld(encoder_outputs)[0], encoder_attention_mask=ag__.ld(attention_mask), head_mask=ag__.ld(decoder_head_mask), cross_attn_head_mask=ag__.ld(cross_attn_head_mask), past_key_values=ag__.ld(past_key_values), inputs_embeds=ag__.ld(decoder_inputs_embeds), use_cache=ag__.ld(use_cache), output_attentions=ag__.ld(output_attentions), output_hidden_states=ag__.ld(output_hidden_states), return_dict=ag__.ld(return_dict), training=ag__.ld(training)), fscope)\n    File \"/var/tmp/__autograph_generated_fileso2b_d_j.py\", line 37, in tf__run_call_with_unpacked_inputs\n        retval_ = ag__.converted_call(ag__.ld(func), (ag__.ld(self),), dict(**ag__.ld(unpacked_inputs)), fscope)\n    File \"/var/tmp/__autograph_generated_file007acu4j.py\", line 128, in tf__call\n        ag__.if_stmt(ag__.and_(lambda : ag__.ld(input_ids) is not None, lambda : ag__.ld(inputs_embeds) is not None), if_body_2, else_body_2, get_state_2, set_state_2, ('input_shape',), 1)\n    File \"/var/tmp/__autograph_generated_file007acu4j.py\", line 126, in else_body_2\n        ag__.if_stmt(ag__.ld(input_ids) is not None, if_body_1, else_body_1, get_state_1, set_state_1, ('input_shape',), 1)\n    File \"/var/tmp/__autograph_generated_file007acu4j.py\", line 124, in else_body_1\n        ag__.if_stmt(ag__.ld(inputs_embeds) is not None, if_body, else_body, get_state, set_state, ('input_shape',), 1)\n    File \"/var/tmp/__autograph_generated_file007acu4j.py\", line 122, in else_body\n        raise ag__.converted_call(ag__.ld(ValueError), ('You have to specify either decoder_input_ids or decoder_inputs_embeds',), None, fscope)\n\n    ValueError: Exception encountered when calling layer 'tf_blenderbot_small_model_1' (type TFBlenderbotSmallModel).\n    \n    in user code:\n    \n        File \"/opt/conda/envs/xla-env/lib/python3.10/site-packages/transformers/modeling_tf_utils.py\", line 1079, in run_call_with_unpacked_inputs  *\n            return func(self, **unpacked_inputs)\n        File \"/opt/conda/envs/xla-env/lib/python3.10/site-packages/transformers/models/blenderbot_small/modeling_tf_blenderbot_small.py\", line 1261, in call  *\n            outputs = self.model(\n        File \"/opt/conda/envs/xla-env/lib/python3.10/site-packages/tf_keras/src/utils/traceback_utils.py\", line 70, in error_handler  **\n            raise e.with_traceback(filtered_tb) from None\n        File \"/var/tmp/__autograph_generated_fileso2b_d_j.py\", line 37, in tf__run_call_with_unpacked_inputs\n            retval_ = ag__.converted_call(ag__.ld(func), (ag__.ld(self),), dict(**ag__.ld(unpacked_inputs)), fscope)\n        File \"/var/tmp/__autograph_generated_file9cel92d9.py\", line 58, in tf__call\n            decoder_outputs = ag__.converted_call(ag__.ld(self).decoder, (ag__.ld(decoder_input_ids),), dict(attention_mask=ag__.ld(decoder_attention_mask), position_ids=ag__.ld(decoder_position_ids), encoder_hidden_states=ag__.ld(encoder_outputs)[0], encoder_attention_mask=ag__.ld(attention_mask), head_mask=ag__.ld(decoder_head_mask), cross_attn_head_mask=ag__.ld(cross_attn_head_mask), past_key_values=ag__.ld(past_key_values), inputs_embeds=ag__.ld(decoder_inputs_embeds), use_cache=ag__.ld(use_cache), output_attentions=ag__.ld(output_attentions), output_hidden_states=ag__.ld(output_hidden_states), return_dict=ag__.ld(return_dict), training=ag__.ld(training)), fscope)\n        File \"/var/tmp/__autograph_generated_fileso2b_d_j.py\", line 37, in tf__run_call_with_unpacked_inputs\n            retval_ = ag__.converted_call(ag__.ld(func), (ag__.ld(self),), dict(**ag__.ld(unpacked_inputs)), fscope)\n        File \"/var/tmp/__autograph_generated_file007acu4j.py\", line 128, in tf__call\n            ag__.if_stmt(ag__.and_(lambda : ag__.ld(input_ids) is not None, lambda : ag__.ld(inputs_embeds) is not None), if_body_2, else_body_2, get_state_2, set_state_2, ('input_shape',), 1)\n        File \"/var/tmp/__autograph_generated_file007acu4j.py\", line 126, in else_body_2\n            ag__.if_stmt(ag__.ld(input_ids) is not None, if_body_1, else_body_1, get_state_1, set_state_1, ('input_shape',), 1)\n        File \"/var/tmp/__autograph_generated_file007acu4j.py\", line 124, in else_body_1\n            ag__.if_stmt(ag__.ld(inputs_embeds) is not None, if_body, else_body, get_state, set_state, ('input_shape',), 1)\n        File \"/var/tmp/__autograph_generated_file007acu4j.py\", line 122, in else_body\n            raise ag__.converted_call(ag__.ld(ValueError), ('You have to specify either decoder_input_ids or decoder_inputs_embeds',), None, fscope)\n    \n        ValueError: Exception encountered when calling layer 'model' (type TFBlenderbotSmallMainLayer).\n        \n        in user code:\n        \n            File \"/opt/conda/envs/xla-env/lib/python3.10/site-packages/transformers/modeling_tf_utils.py\", line 1079, in run_call_with_unpacked_inputs  *\n                return func(self, **unpacked_inputs)\n            File \"/opt/conda/envs/xla-env/lib/python3.10/site-packages/transformers/models/blenderbot_small/modeling_tf_blenderbot_small.py\", line 1169, in call  *\n                decoder_outputs = self.decoder(\n            File \"/opt/conda/envs/xla-env/lib/python3.10/site-packages/tf_keras/src/utils/traceback_utils.py\", line 70, in error_handler  **\n                raise e.with_traceback(filtered_tb) from None\n            File \"/var/tmp/__autograph_generated_fileso2b_d_j.py\", line 37, in tf__run_call_with_unpacked_inputs\n                retval_ = ag__.converted_call(ag__.ld(func), (ag__.ld(self),), dict(**ag__.ld(unpacked_inputs)), fscope)\n            File \"/var/tmp/__autograph_generated_file007acu4j.py\", line 128, in tf__call\n                ag__.if_stmt(ag__.and_(lambda : ag__.ld(input_ids) is not None, lambda : ag__.ld(inputs_embeds) is not None), if_body_2, else_body_2, get_state_2, set_state_2, ('input_shape',), 1)\n            File \"/var/tmp/__autograph_generated_file007acu4j.py\", line 126, in else_body_2\n                ag__.if_stmt(ag__.ld(input_ids) is not None, if_body_1, else_body_1, get_state_1, set_state_1, ('input_shape',), 1)\n            File \"/var/tmp/__autograph_generated_file007acu4j.py\", line 124, in else_body_1\n                ag__.if_stmt(ag__.ld(inputs_embeds) is not None, if_body, else_body, get_state, set_state, ('input_shape',), 1)\n            File \"/var/tmp/__autograph_generated_file007acu4j.py\", line 122, in else_body\n                raise ag__.converted_call(ag__.ld(ValueError), ('You have to specify either decoder_input_ids or decoder_inputs_embeds',), None, fscope)\n        \n            ValueError: Exception encountered when calling layer 'decoder' (type TFBlenderbotSmallDecoder).\n            \n            in user code:\n            \n                File \"/opt/conda/envs/xla-env/lib/python3.10/site-packages/transformers/modeling_tf_utils.py\", line 1079, in run_call_with_unpacked_inputs  *\n                    return func(self, **unpacked_inputs)\n                File \"/opt/conda/envs/xla-env/lib/python3.10/site-packages/transformers/models/blenderbot_small/modeling_tf_blenderbot_small.py\", line 983, in call  *\n                    raise ValueError(\"You have to specify either decoder_input_ids or decoder_inputs_embeds\")\n            \n                ValueError: You have to specify either decoder_input_ids or decoder_inputs_embeds\n            \n            \n            Call arguments received by layer 'decoder' (type TFBlenderbotSmallDecoder):\n              \u2022 input_ids=None\n              \u2022 inputs_embeds=None\n              \u2022 attention_mask=None\n              \u2022 position_ids=None\n              \u2022 encoder_hidden_states=tf.Tensor(shape=(1, 32, 512), dtype=float32)\n              \u2022 encoder_attention_mask=tf.Tensor(shape=(1, 32), dtype=int32)\n              \u2022 head_mask=None\n              \u2022 cross_attn_head_mask=None\n              \u2022 past_key_values=None\n              \u2022 use_cache=True\n              \u2022 output_attentions=False\n              \u2022 output_hidden_states=False\n              \u2022 return_dict=True\n              \u2022 training=False\n        \n        \n        Call arguments received by layer 'model' (type TFBlenderbotSmallMainLayer):\n          \u2022 input_ids=tf.Tensor(shape=(1, 32), dtype=int32)\n          \u2022 attention_mask=tf.Tensor(shape=(1, 32), dtype=int32)\n          \u2022 decoder_input_ids=None\n          \u2022 decoder_attention_mask=None\n          \u2022 decoder_position_ids=None\n          \u2022 head_mask=None\n          \u2022 decoder_head_mask=None\n          \u2022 cross_attn_head_mask=None\n          \u2022 encoder_outputs=None\n          \u2022 past_key_values=None\n          \u2022 inputs_embeds=None\n          \u2022 decoder_inputs_embeds=None\n          \u2022 use_cache=True\n          \u2022 output_attentions=False\n          \u2022 output_hidden_states=False\n          \u2022 return_dict=True\n          \u2022 training=False\n          \u2022 kwargs=<class 'inspect._empty'>\n    \n    \n    Call arguments received by layer 'tf_blenderbot_small_model_1' (type TFBlenderbotSmallModel):\n      \u2022 input_ids=tf.Tensor(shape=(1, 32), dtype=int32)\n      \u2022 attention_mask=tf.Tensor(shape=(1, 32), dtype=int32)\n      \u2022 decoder_input_ids=None\n      \u2022 decoder_attention_mask=None\n      \u2022 decoder_position_ids=None\n      \u2022 head_mask=None\n      \u2022 decoder_head_mask=None\n      \u2022 cross_attn_head_mask=None\n      \u2022 encoder_outputs=None\n      \u2022 past_key_values=None\n      \u2022 inputs_embeds=None\n      \u2022 decoder_inputs_embeds=None\n      \u2022 use_cache=None\n      \u2022 output_attentions=None\n      \u2022 output_hidden_states=None\n      \u2022 return_dict=None\n      \u2022 training=False\n      \u2022 kwargs=<class 'inspect._empty'>\n"
            }
        }
    },
    {
        "BlenderbotSmallForConditionalGeneration": {
            "pytorch_torch_compile_inductor": {
                "inference_s": 0.02744128704071045,
                "training_s": 0.18075281381607056
            },
            "pytorch_torch_compile_eager": {
                "inference_s": 0.042872228622436524,
                "training_s": 0.18041650533676148
            },
            "pytorch_torch_compile_cudagraphs": {
                "inference_s": 0.08838886499404908,
                "training_s": 0.23944429874420167
            },
            "pytorch_torch_compile_onnxrt": {
                "inference_s": 0.12671951532363893,
                "training_s": 0.28417110919952393
            },
            "pytorch_torch_compile_openxla": {
                "inference_s": 0.042681057453155515,
                "training_s": 0.18600024461746215
            },
            "pytorch_torch_compile_tvm": {
                "inference_s": 0.04320469617843628,
                "training_s": 0.1852393341064453
            },
            "jax_flax_xla": {
                "inference_s": 0.0011750197410583497,
                "training_s": 0.006331441402435303
            },
            "error": "AttributeError(\"'BlenderbotSmallForConditionalGeneration' object has no attribute '_shift_right'\")"
        }
    },
    {
        "CamemBert": {
            "pytorch_torch_compile_inductor": {
                "inference_s": 0.027831311225891112,
                "training_s": 0.10261031150817872
            },
            "pytorch_torch_compile_eager": {
                "inference_s": 0.03201932907104492,
                "training_s": 0.10712735414505005
            },
            "pytorch_torch_compile_cudagraphs": {
                "inference_s": 0.06258674383163453,
                "training_s": 0.1375955295562744
            },
            "pytorch_torch_compile_onnxrt": {
                "inference_s": 0.09997310400009156,
                "training_s": 0.1980069351196289
            },
            "pytorch_torch_compile_openxla": {
                "inference_s": 0.03792635917663574,
                "training_s": 0.142994441986084
            },
            "pytorch_torch_compile_tvm": {
                "inference_s": 0.03809902906417847,
                "training_s": 0.1461581015586853
            },
            "jax_flax_xla": {
                "error": "Unrecognized configuration class <class 'transformers.models.camembert.configuration_camembert.CamembertConfig'> for this kind of AutoModel: FlaxAutoModel.\nModel type should be one of AlbertConfig, BartConfig, BeitConfig, BertConfig, BigBirdConfig, BlenderbotConfig, BlenderbotSmallConfig, BloomConfig, CLIPConfig, Dinov2Config, DistilBertConfig, ElectraConfig, GemmaConfig, GPT2Config, GPT2Config, GPTNeoConfig, GPTJConfig, LlamaConfig, LongT5Config, MarianConfig, MBartConfig, MistralConfig, MT5Config, OPTConfig, PegasusConfig, RegNetConfig, ResNetConfig, RobertaConfig, RobertaPreLayerNormConfig, RoFormerConfig, T5Config, VisionTextDualEncoderConfig, ViTConfig, Wav2Vec2Config, WhisperConfig, XGLMConfig, XLMRobertaConfig."
            },
            "tensorflow_eager": {
                "inference_s": 0.18659595012664795,
                "training_s": 0.555629951953888
            },
            "tensorflow_xla": {
                "inference_s": 0.002718205451965332,
                "training_s": 0.007352726459503174
            }
        }
    },
    {
        "DebertaForMaskedLM": {
            "pytorch_torch_compile_inductor": {
                "inference_s": 0.07133142948150635,
                "training_s": 0.2740667414665222
            },
            "pytorch_torch_compile_eager": {
                "inference_s": 0.07779613256454468,
                "training_s": 0.2463206434249878
            },
            "pytorch_torch_compile_cudagraphs": {
                "inference_s": 0.14131004095077515,
                "training_s": 0.3326647186279297
            },
            "pytorch_torch_compile_onnxrt": {
                "error": "backend='onnxrt' raised:\nAssertionError: Please convert all Tensors to FakeTensors first or instantiate FakeTensorMode with 'allow_non_fake_inputs'. Found in aten.alias.default(tensor(64., size=()))\n\nWhile executing %tensor : [num_users=1] = call_function[target=torch.tensor](args = (64,), kwargs = {dtype: torch.float32})\nOriginal traceback:\n  File \"/opt/conda/envs/xla-env/lib/python3.10/site-packages/transformers/models/deberta/modeling_deberta.py\", line 274, in forward\n    scale = scaled_size_sqrt(query_layer, scale_factor)\n  File \"/opt/conda/envs/xla-env/lib/python3.10/site-packages/torch/_dynamo/polyfills/__init__.py\", line 160, in getattr_and_trace\n    return fn(*args[2:], **kwargs)\n  File \"/opt/conda/envs/xla-env/lib/python3.10/site-packages/transformers/models/deberta/modeling_deberta.py\", line 137, in scaled_size_sqrt\n    return torch.sqrt(torch.tensor(query_layer.size(-1), dtype=torch.float) * scale_factor)\n\n\nSet TORCH_LOGS=\"+dynamo\" and TORCHDYNAMO_VERBOSE=1 for more information\n\n\nYou can suppress this exception and fall back to eager by setting:\n    import torch._dynamo\n    torch._dynamo.config.suppress_errors = True\n"
            },
            "pytorch_torch_compile_openxla": {
                "error": "torch_xla/csrc/runtime/pjrt_registry.cc:214 : Check failed: client \n*** Begin stack trace ***\n\ttsl::CurrentStackTrace()\n\ttorch_xla::runtime::InitializePjRt(std::string const&)\n\ttorch_xla::runtime::PjRtComputationClient::PjRtComputationClient()\n\t\n\ttorch_xla::runtime::GetComputationClient()\n\ttorch_xla::bridge::GetDefaultDevice()\n\ttorch_xla::bridge::GetCurrentDevice()\n\ttorch_xla::bridge::GetCurrentAtenDevice()\n\t\n\t\n\t\n\t_PyObject_MakeTpCall\n\t_PyEval_EvalFrameDefault\n\t\n\t_PyFunction_Vectorcall\n\t_PyEval_EvalFrameDefault\n\t\n\t_PyFunction_Vectorcall\n\t_PyEval_EvalFrameDefault\n\t\n\t_PyFunction_Vectorcall\n\t_PyEval_EvalFrameDefault\n\t\n\t_PyFunction_Vectorcall\n\t_PyEval_EvalFrameDefault\n\t\n\t_PyFunction_Vectorcall\n\t_PyEval_EvalFrameDefault\n\t\n\t_PyFunction_Vectorcall\n\t_PyEval_EvalFrameDefault\n\t\n\t_PyFunction_Vectorcall\n\t_PyEval_EvalFrameDefault\n\t\n\t_PyFunction_Vectorcall\n\t_PyEval_EvalFrameDefault\n\t\n\t_PyFunction_Vectorcall\n\tTHPFunction_apply(_object*, _object*)\n\t\n\tPyObject_Call\n\t_PyEval_EvalFrameDefault\n\t\n\t\n\t_PyEval_EvalFrameDefault\n\t\n\t_PyFunction_Vectorcall\n\t_PyEval_EvalFrameDefault\n\t\n\t_PyFunction_Vectorcall\n\t_PyEval_EvalFrameDefault\n\t\n\t_PyFunction_Vectorcall\n\t_PyEval_EvalFrameDefault\n\t\n\t_PyFunction_Vectorcall\n\t_PyEval_EvalFrameDefault\n\t\n\t_PyFunction_Vectorcall\n\t_PyEval_EvalFrameDefault\n\t\n\t\n\t\n\tPyObject_Call\n\t_PyEval_EvalFrameDefault\n\t\n\t\n\tPyObject_Call\n\t_PyEval_EvalFrameDefault\n\t\n\t_PyFunction_Vectorcall\n\t_PyObject_FastCallDictTstate\n\t_PyObject_Call_Prepend\n\t\n\t_PyObject_MakeTpCall\n\t_PyEval_EvalFrameDefault\n\t\n\t\n\t\n\tPyObject_Call\n\t_PyEval_EvalFrameDefault\n\t\n\t\n\tPyObject_Call\n\t_PyEval_EvalFrameDefault\n\t\n\t\n\tPyObject_Call\n\t_PyEval_EvalFrameDefault\n\t\n\t_PyFunction_Vectorcall\n\tPyObject_Call\n\t_PyEval_EvalFrameDefault\n\t\n\t\n\tPyObject_Call\n\t_PyEval_EvalFrameDefault\n\t\n\t_PyFunction_Vectorcall\n\t_PyObject_FastCallDictTstate\n\t_PyObject_Call_Prepend\n\t\n\tPyObject_Call\n\t_PyEval_EvalFrameDefault\n\t\n\t_PyFunction_Vectorcall\n\t_PyEval_EvalFrameDefault\n\t_PyFunction_Vectorcall\n\t_PyEval_EvalFrameDefault\n\t\n\tPyEval_EvalCode\n\t\n\t\n\t\n\t_PyRun_SimpleFileObject\n\t_PyRun_AnyFileObject\n\tPy_RunMain\n\tPy_BytesMain\n\t__libc_start_main\n\t\n*** End stack trace ***\nUnknown PJRT_DEVICE 'CUDA'"
            },
            "pytorch_torch_compile_tvm": {
                "inference_s": 0.07409691572189331,
                "training_s": 0.24516931295394898
            },
            "jax_flax_xla": {
                "error": "Unrecognized configuration class <class 'transformers.models.deberta.configuration_deberta.DebertaConfig'> for this kind of AutoModel: FlaxAutoModel.\nModel type should be one of AlbertConfig, BartConfig, BeitConfig, BertConfig, BigBirdConfig, BlenderbotConfig, BlenderbotSmallConfig, BloomConfig, CLIPConfig, Dinov2Config, DistilBertConfig, ElectraConfig, GemmaConfig, GPT2Config, GPT2Config, GPTNeoConfig, GPTJConfig, LlamaConfig, LongT5Config, MarianConfig, MBartConfig, MistralConfig, MT5Config, OPTConfig, PegasusConfig, RegNetConfig, ResNetConfig, RobertaConfig, RobertaPreLayerNormConfig, RoFormerConfig, T5Config, VisionTextDualEncoderConfig, ViTConfig, Wav2Vec2Config, WhisperConfig, XGLMConfig, XLMRobertaConfig."
            },
            "tensorflow_eager": {
                "inference_s": 0.3437200474739075,
                "training_s": 1.0319571638107299
            },
            "tensorflow_xla": {
                "error": "0 is duplicated in the input.\n\nStack trace for op definition: \nFile \"home/mmoffatt/jax_vs_pytorch/benchmark.py\", line 402, in <module>\nFile \"home/mmoffatt/jax_vs_pytorch/benchmark.py\", line 381, in run_benchmarks\nFile \"home/mmoffatt/jax_vs_pytorch/benchmark.py\", line 221, in benchmark_training_tf_xla\nFile \"home/mmoffatt/jax_vs_pytorch/benchmark.py\", line 215, in train_step\n\n\t [[{{node gradient_tape/tf_deberta_for_masked_lm_1/deberta/encoder/layer_._11/attention/self/cond_1/gradients/tf_deberta_for_masked_lm_1/deberta/encoder/layer_._11/attention/self/cond_1/transpose_grad/InvertPermutation}}]]\n\ttf2xla conversion failed while converting tf_deberta_for_masked_lm_1_deberta_encoder_layer_._11_attention_self_cond_1_true_9911234_grad_9912180[]. Run with TF_DUMP_GRAPH_PREFIX=/path/to/dump/dir and --vmodule=xla_compiler=2 to obtain a dump of the compiled functions.\n\nStack trace for op definition: \nFile \"home/mmoffatt/jax_vs_pytorch/benchmark.py\", line 402, in <module>\nFile \"home/mmoffatt/jax_vs_pytorch/benchmark.py\", line 381, in run_benchmarks\nFile \"home/mmoffatt/jax_vs_pytorch/benchmark.py\", line 221, in benchmark_training_tf_xla\nFile \"home/mmoffatt/jax_vs_pytorch/benchmark.py\", line 215, in train_step\n\n\t [[gradient_tape/tf_deberta_for_masked_lm_1/deberta/encoder/layer_._11/attention/self/cond_1/StatelessIf]]\n\ttf2xla conversion failed while converting __inference_train_step_9922589[_XlaMustCompile=true,config_proto=2201667018877855759,executor_type=11160318154034397263]. Run with TF_DUMP_GRAPH_PREFIX=/path/to/dump/dir and --vmodule=xla_compiler=2 to obtain a dump of the compiled functions. [Op:__inference_train_step_9922589]"
            }
        }
    },
    {
        "DebertaForQuestionAnswering": {
            "pytorch_torch_compile_inductor": {
                "inference_s": 0.06595691204071046,
                "training_s": 0.2570645213127136
            },
            "pytorch_torch_compile_eager": {
                "inference_s": 0.07275067567825318,
                "training_s": 0.19019917249679566
            },
            "pytorch_torch_compile_cudagraphs": {
                "inference_s": 0.13159679412841796,
                "training_s": 0.28097004413604737
            },
            "pytorch_torch_compile_onnxrt": {
                "error": "backend='onnxrt' raised:\nAssertionError: Please convert all Tensors to FakeTensors first or instantiate FakeTensorMode with 'allow_non_fake_inputs'. Found in aten.alias.default(tensor(64., size=()))\n\nWhile executing %tensor : [num_users=1] = call_function[target=torch.tensor](args = (64,), kwargs = {dtype: torch.float32})\nOriginal traceback:\n  File \"/opt/conda/envs/xla-env/lib/python3.10/site-packages/transformers/models/deberta/modeling_deberta.py\", line 274, in forward\n    scale = scaled_size_sqrt(query_layer, scale_factor)\n  File \"/opt/conda/envs/xla-env/lib/python3.10/site-packages/torch/_dynamo/polyfills/__init__.py\", line 160, in getattr_and_trace\n    return fn(*args[2:], **kwargs)\n  File \"/opt/conda/envs/xla-env/lib/python3.10/site-packages/transformers/models/deberta/modeling_deberta.py\", line 137, in scaled_size_sqrt\n    return torch.sqrt(torch.tensor(query_layer.size(-1), dtype=torch.float) * scale_factor)\n\n\nSet TORCH_LOGS=\"+dynamo\" and TORCHDYNAMO_VERBOSE=1 for more information\n\n\nYou can suppress this exception and fall back to eager by setting:\n    import torch._dynamo\n    torch._dynamo.config.suppress_errors = True\n"
            },
            "pytorch_torch_compile_openxla": {
                "error": "torch_xla/csrc/runtime/pjrt_registry.cc:214 : Check failed: client \n*** Begin stack trace ***\n\ttsl::CurrentStackTrace()\n\ttorch_xla::runtime::InitializePjRt(std::string const&)\n\ttorch_xla::runtime::PjRtComputationClient::PjRtComputationClient()\n\t\n\ttorch_xla::runtime::GetComputationClient()\n\ttorch_xla::bridge::GetDefaultDevice()\n\ttorch_xla::bridge::GetCurrentDevice()\n\ttorch_xla::bridge::GetCurrentAtenDevice()\n\t\n\t\n\t\n\t_PyObject_MakeTpCall\n\t_PyEval_EvalFrameDefault\n\t\n\t_PyFunction_Vectorcall\n\t_PyEval_EvalFrameDefault\n\t\n\t_PyFunction_Vectorcall\n\t_PyEval_EvalFrameDefault\n\t\n\t_PyFunction_Vectorcall\n\t_PyEval_EvalFrameDefault\n\t\n\t_PyFunction_Vectorcall\n\t_PyEval_EvalFrameDefault\n\t\n\t_PyFunction_Vectorcall\n\t_PyEval_EvalFrameDefault\n\t\n\t_PyFunction_Vectorcall\n\t_PyEval_EvalFrameDefault\n\t\n\t_PyFunction_Vectorcall\n\t_PyEval_EvalFrameDefault\n\t\n\t_PyFunction_Vectorcall\n\t_PyEval_EvalFrameDefault\n\t\n\t_PyFunction_Vectorcall\n\tTHPFunction_apply(_object*, _object*)\n\t\n\tPyObject_Call\n\t_PyEval_EvalFrameDefault\n\t\n\t\n\t_PyEval_EvalFrameDefault\n\t\n\t_PyFunction_Vectorcall\n\t_PyEval_EvalFrameDefault\n\t\n\t_PyFunction_Vectorcall\n\t_PyEval_EvalFrameDefault\n\t\n\t_PyFunction_Vectorcall\n\t_PyEval_EvalFrameDefault\n\t\n\t_PyFunction_Vectorcall\n\t_PyEval_EvalFrameDefault\n\t\n\t_PyFunction_Vectorcall\n\t_PyEval_EvalFrameDefault\n\t\n\t\n\t\n\tPyObject_Call\n\t_PyEval_EvalFrameDefault\n\t\n\t\n\tPyObject_Call\n\t_PyEval_EvalFrameDefault\n\t\n\t_PyFunction_Vectorcall\n\t_PyObject_FastCallDictTstate\n\t_PyObject_Call_Prepend\n\t\n\t_PyObject_MakeTpCall\n\t_PyEval_EvalFrameDefault\n\t\n\t\n\t\n\tPyObject_Call\n\t_PyEval_EvalFrameDefault\n\t\n\t\n\tPyObject_Call\n\t_PyEval_EvalFrameDefault\n\t\n\t\n\tPyObject_Call\n\t_PyEval_EvalFrameDefault\n\t\n\t_PyFunction_Vectorcall\n\tPyObject_Call\n\t_PyEval_EvalFrameDefault\n\t\n\t\n\tPyObject_Call\n\t_PyEval_EvalFrameDefault\n\t\n\t_PyFunction_Vectorcall\n\t_PyObject_FastCallDictTstate\n\t_PyObject_Call_Prepend\n\t\n\tPyObject_Call\n\t_PyEval_EvalFrameDefault\n\t\n\t_PyFunction_Vectorcall\n\t_PyEval_EvalFrameDefault\n\t_PyFunction_Vectorcall\n\t_PyEval_EvalFrameDefault\n\t\n\tPyEval_EvalCode\n\t\n\t\n\t\n\t_PyRun_SimpleFileObject\n\t_PyRun_AnyFileObject\n\tPy_RunMain\n\tPy_BytesMain\n\t__libc_start_main\n\t\n*** End stack trace ***\nUnknown PJRT_DEVICE 'CUDA'"
            },
            "pytorch_torch_compile_tvm": {
                "inference_s": 0.06400709867477417,
                "training_s": 0.17686929941177368
            },
            "jax_flax_xla": {
                "error": "Unrecognized configuration class <class 'transformers.models.deberta.configuration_deberta.DebertaConfig'> for this kind of AutoModel: FlaxAutoModel.\nModel type should be one of AlbertConfig, BartConfig, BeitConfig, BertConfig, BigBirdConfig, BlenderbotConfig, BlenderbotSmallConfig, BloomConfig, CLIPConfig, Dinov2Config, DistilBertConfig, ElectraConfig, GemmaConfig, GPT2Config, GPT2Config, GPTNeoConfig, GPTJConfig, LlamaConfig, LongT5Config, MarianConfig, MBartConfig, MistralConfig, MT5Config, OPTConfig, PegasusConfig, RegNetConfig, ResNetConfig, RobertaConfig, RobertaPreLayerNormConfig, RoFormerConfig, T5Config, VisionTextDualEncoderConfig, ViTConfig, Wav2Vec2Config, WhisperConfig, XGLMConfig, XLMRobertaConfig."
            },
            "tensorflow_eager": {
                "inference_s": 0.3433998727798462,
                "training_s": 1.0032264614105224
            },
            "tensorflow_xla": {
                "error": "0 is duplicated in the input.\n\nStack trace for op definition: \nFile \"home/mmoffatt/jax_vs_pytorch/benchmark.py\", line 402, in <module>\nFile \"home/mmoffatt/jax_vs_pytorch/benchmark.py\", line 381, in run_benchmarks\nFile \"home/mmoffatt/jax_vs_pytorch/benchmark.py\", line 221, in benchmark_training_tf_xla\nFile \"home/mmoffatt/jax_vs_pytorch/benchmark.py\", line 215, in train_step\n\n\t [[{{node gradient_tape/tf_deberta_for_question_answering_1/deberta/encoder/layer_._11/attention/self/cond_1/gradients/tf_deberta_for_question_answering_1/deberta/encoder/layer_._11/attention/self/cond_1/transpose_grad/InvertPermutation}}]]\n\ttf2xla conversion failed while converting tf_deberta_for_question_answering_1_deberta_encoder_layer_._11_attention_self_cond_1_true_11475560_grad_11476366[]. Run with TF_DUMP_GRAPH_PREFIX=/path/to/dump/dir and --vmodule=xla_compiler=2 to obtain a dump of the compiled functions.\n\nStack trace for op definition: \nFile \"home/mmoffatt/jax_vs_pytorch/benchmark.py\", line 402, in <module>\nFile \"home/mmoffatt/jax_vs_pytorch/benchmark.py\", line 381, in run_benchmarks\nFile \"home/mmoffatt/jax_vs_pytorch/benchmark.py\", line 221, in benchmark_training_tf_xla\nFile \"home/mmoffatt/jax_vs_pytorch/benchmark.py\", line 215, in train_step\n\n\t [[gradient_tape/tf_deberta_for_question_answering_1/deberta/encoder/layer_._11/attention/self/cond_1/StatelessIf]]\n\ttf2xla conversion failed while converting __inference_train_step_11486161[_XlaMustCompile=true,config_proto=2201667018877855759,executor_type=11160318154034397263]. Run with TF_DUMP_GRAPH_PREFIX=/path/to/dump/dir and --vmodule=xla_compiler=2 to obtain a dump of the compiled functions. [Op:__inference_train_step_11486161]"
            }
        }
    },
    {
        "DebertaV2ForMaskedLM": {
            "pytorch_torch_compile_inductor": {
                "error": "backend='inductor' raised:\nAssertionError: buf28\n\nSet TORCH_LOGS=\"+dynamo\" and TORCHDYNAMO_VERBOSE=1 for more information\n\n\nYou can suppress this exception and fall back to eager by setting:\n    import torch._dynamo\n    torch._dynamo.config.suppress_errors = True\n"
            },
            "pytorch_torch_compile_eager": {
                "inference_s": 0.5444075989723206,
                "training_s": 1.7965512871742249
            },
            "pytorch_torch_compile_cudagraphs": {
                "inference_s": 0.628669068813324,
                "training_s": 1.826754972934723
            },
            "pytorch_torch_compile_onnxrt": {
                "error": "backend='onnxrt' raised:\nAssertionError: Please convert all Tensors to FakeTensors first or instantiate FakeTensorMode with 'allow_non_fake_inputs'. Found in aten.alias.default(tensor(127, size=()))\n\nWhile executing %tensor : [num_users=1] = call_function[target=torch.tensor](args = (127,), kwargs = {})\nOriginal traceback:\n  File \"/opt/conda/envs/xla-env/lib/python3.10/site-packages/transformers/models/deberta_v2/modeling_deberta_v2.py\", line 1059, in forward\n    outputs = self.deberta(\n  File \"/opt/conda/envs/xla-env/lib/python3.10/site-packages/transformers/models/deberta_v2/modeling_deberta_v2.py\", line 870, in forward\n    encoder_outputs = self.encoder(\n  File \"/opt/conda/envs/xla-env/lib/python3.10/site-packages/transformers/models/deberta_v2/modeling_deberta_v2.py\", line 655, in forward\n    relative_pos = self.get_rel_pos(hidden_states, query_states, relative_pos)\n  File \"/opt/conda/envs/xla-env/lib/python3.10/site-packages/transformers/models/deberta_v2/modeling_deberta_v2.py\", line 632, in get_rel_pos\n    relative_pos = build_relative_position(\n  File \"/opt/conda/envs/xla-env/lib/python3.10/site-packages/transformers/models/deberta_v2/modeling_deberta_v2.py\", line 103, in build_relative_position\n    rel_pos_ids = make_log_bucket_position(rel_pos_ids, bucket_size, max_position)\n  File \"/opt/conda/envs/xla-env/lib/python3.10/site-packages/torch/_dynamo/polyfills/__init__.py\", line 160, in getattr_and_trace\n    return fn(*args[2:], **kwargs)\n  File \"/opt/conda/envs/xla-env/lib/python3.10/site-packages/transformers/models/deberta_v2/modeling_deberta_v2.py\", line 68, in make_log_bucket_position\n    torch.tensor(mid - 1).type_as(relative_pos),\n\n\nSet TORCH_LOGS=\"+dynamo\" and TORCHDYNAMO_VERBOSE=1 for more information\n\n\nYou can suppress this exception and fall back to eager by setting:\n    import torch._dynamo\n    torch._dynamo.config.suppress_errors = True\n"
            },
            "pytorch_torch_compile_openxla": {
                "error": "torch_xla/csrc/runtime/pjrt_registry.cc:214 : Check failed: client \n*** Begin stack trace ***\n\ttsl::CurrentStackTrace()\n\ttorch_xla::runtime::InitializePjRt(std::string const&)\n\ttorch_xla::runtime::PjRtComputationClient::PjRtComputationClient()\n\t\n\ttorch_xla::runtime::GetComputationClient()\n\ttorch_xla::bridge::GetDefaultDevice()\n\ttorch_xla::bridge::GetCurrentDevice()\n\ttorch_xla::bridge::GetCurrentAtenDevice()\n\t\n\t\n\t\n\t_PyObject_MakeTpCall\n\t_PyEval_EvalFrameDefault\n\t\n\t_PyFunction_Vectorcall\n\t_PyEval_EvalFrameDefault\n\t\n\t_PyFunction_Vectorcall\n\t_PyEval_EvalFrameDefault\n\t\n\t_PyFunction_Vectorcall\n\t_PyEval_EvalFrameDefault\n\t\n\t_PyFunction_Vectorcall\n\t_PyEval_EvalFrameDefault\n\t\n\t_PyFunction_Vectorcall\n\t_PyEval_EvalFrameDefault\n\t\n\t_PyFunction_Vectorcall\n\t_PyEval_EvalFrameDefault\n\t\n\t_PyFunction_Vectorcall\n\t_PyEval_EvalFrameDefault\n\t\n\t_PyFunction_Vectorcall\n\t_PyEval_EvalFrameDefault\n\t\n\t_PyFunction_Vectorcall\n\tTHPFunction_apply(_object*, _object*)\n\t\n\tPyObject_Call\n\t_PyEval_EvalFrameDefault\n\t\n\t\n\t_PyEval_EvalFrameDefault\n\t\n\t_PyFunction_Vectorcall\n\t_PyEval_EvalFrameDefault\n\t\n\t_PyFunction_Vectorcall\n\t_PyEval_EvalFrameDefault\n\t\n\t_PyFunction_Vectorcall\n\t_PyEval_EvalFrameDefault\n\t\n\t_PyFunction_Vectorcall\n\t_PyEval_EvalFrameDefault\n\t\n\t_PyFunction_Vectorcall\n\t_PyEval_EvalFrameDefault\n\t\n\t\n\t\n\tPyObject_Call\n\t_PyEval_EvalFrameDefault\n\t\n\t\n\tPyObject_Call\n\t_PyEval_EvalFrameDefault\n\t\n\t\n\tPyObject_Call\n\t_PyEval_EvalFrameDefault\n\t\n\t_PyFunction_Vectorcall\n\tPyObject_Call\n\t_PyEval_EvalFrameDefault\n\t\n\t\n\tPyObject_Call\n\t_PyEval_EvalFrameDefault\n\t\n\t_PyFunction_Vectorcall\n\t_PyObject_FastCallDictTstate\n\t_PyObject_Call_Prepend\n\t\n\tPyObject_Call\n\t_PyEval_EvalFrameDefault\n\t\n\t_PyFunction_Vectorcall\n\t_PyEval_EvalFrameDefault\n\t_PyFunction_Vectorcall\n\t_PyEval_EvalFrameDefault\n\t\n\tPyEval_EvalCode\n\t\n\t\n\t\n\t_PyRun_SimpleFileObject\n\t_PyRun_AnyFileObject\n\tPy_RunMain\n\tPy_BytesMain\n\t__libc_start_main\n\t\n*** End stack trace ***\nUnknown PJRT_DEVICE 'CUDA'"
            },
            "pytorch_torch_compile_tvm": {
                "error": "backend='tvm' raised:\nModuleNotFoundError: No module named 'tvm'\n\nSet TORCH_LOGS=\"+dynamo\" and TORCHDYNAMO_VERBOSE=1 for more information\n\n\nYou can suppress this exception and fall back to eager by setting:\n    import torch._dynamo\n    torch._dynamo.config.suppress_errors = True\n"
            },
            "jax_flax_xla": {
                "error": "Unrecognized configuration class <class 'transformers.models.deberta_v2.configuration_deberta_v2.DebertaV2Config'> for this kind of AutoModel: FlaxAutoModel.\nModel type should be one of AlbertConfig, BartConfig, BeitConfig, BertConfig, BigBirdConfig, BlenderbotConfig, BlenderbotSmallConfig, BloomConfig, CLIPConfig, Dinov2Config, DistilBertConfig, ElectraConfig, GemmaConfig, GPT2Config, GPT2Config, GPTNeoConfig, GPTJConfig, LlamaConfig, LongT5Config, MarianConfig, MBartConfig, MistralConfig, MT5Config, OPTConfig, PegasusConfig, RegNetConfig, ResNetConfig, RobertaConfig, RobertaPreLayerNormConfig, RoFormerConfig, T5Config, VisionTextDualEncoderConfig, ViTConfig, Wav2Vec2Config, WhisperConfig, XGLMConfig, XLMRobertaConfig."
            },
            "tensorflow_eager": {
                "inference_s": 0.666126172542572,
                "training_s": 2.325440127849579
            },
            "tensorflow_xla": {
                "inference_s": 0.00757145643234253,
                "training_s": 0.034332396984100344
            }
        }
    },
    {
        "DebertaV2ForQuestionAnswering": {
            "pytorch_torch_compile_inductor": {
                "error": "backend='inductor' raised:\nAssertionError: buf28\n\nSet TORCH_LOGS=\"+dynamo\" and TORCHDYNAMO_VERBOSE=1 for more information\n\n\nYou can suppress this exception and fall back to eager by setting:\n    import torch._dynamo\n    torch._dynamo.config.suppress_errors = True\n"
            },
            "pytorch_torch_compile_eager": {
                "inference_s": 0.4519592022895813,
                "training_s": 1.3288793635368348
            },
            "pytorch_torch_compile_cudagraphs": {
                "inference_s": 0.5509950470924377,
                "training_s": 1.4734239983558655
            },
            "pytorch_torch_compile_onnxrt": {
                "error": "backend='onnxrt' raised:\nAssertionError: Please convert all Tensors to FakeTensors first or instantiate FakeTensorMode with 'allow_non_fake_inputs'. Found in aten.alias.default(tensor(127, size=()))\n\nWhile executing %tensor : [num_users=1] = call_function[target=torch.tensor](args = (127,), kwargs = {})\nOriginal traceback:\n  File \"/opt/conda/envs/xla-env/lib/python3.10/site-packages/transformers/models/deberta_v2/modeling_deberta_v2.py\", line 1361, in forward\n    outputs = self.deberta(\n  File \"/opt/conda/envs/xla-env/lib/python3.10/site-packages/transformers/models/deberta_v2/modeling_deberta_v2.py\", line 870, in forward\n    encoder_outputs = self.encoder(\n  File \"/opt/conda/envs/xla-env/lib/python3.10/site-packages/transformers/models/deberta_v2/modeling_deberta_v2.py\", line 655, in forward\n    relative_pos = self.get_rel_pos(hidden_states, query_states, relative_pos)\n  File \"/opt/conda/envs/xla-env/lib/python3.10/site-packages/transformers/models/deberta_v2/modeling_deberta_v2.py\", line 632, in get_rel_pos\n    relative_pos = build_relative_position(\n  File \"/opt/conda/envs/xla-env/lib/python3.10/site-packages/transformers/models/deberta_v2/modeling_deberta_v2.py\", line 103, in build_relative_position\n    rel_pos_ids = make_log_bucket_position(rel_pos_ids, bucket_size, max_position)\n  File \"/opt/conda/envs/xla-env/lib/python3.10/site-packages/torch/_dynamo/polyfills/__init__.py\", line 160, in getattr_and_trace\n    return fn(*args[2:], **kwargs)\n  File \"/opt/conda/envs/xla-env/lib/python3.10/site-packages/transformers/models/deberta_v2/modeling_deberta_v2.py\", line 68, in make_log_bucket_position\n    torch.tensor(mid - 1).type_as(relative_pos),\n\n\nSet TORCH_LOGS=\"+dynamo\" and TORCHDYNAMO_VERBOSE=1 for more information\n\n\nYou can suppress this exception and fall back to eager by setting:\n    import torch._dynamo\n    torch._dynamo.config.suppress_errors = True\n"
            },
            "pytorch_torch_compile_openxla": {
                "error": "torch_xla/csrc/runtime/pjrt_registry.cc:214 : Check failed: client \n*** Begin stack trace ***\n\ttsl::CurrentStackTrace()\n\ttorch_xla::runtime::InitializePjRt(std::string const&)\n\ttorch_xla::runtime::PjRtComputationClient::PjRtComputationClient()\n\t\n\ttorch_xla::runtime::GetComputationClient()\n\ttorch_xla::bridge::GetDefaultDevice()\n\ttorch_xla::bridge::GetCurrentDevice()\n\ttorch_xla::bridge::GetCurrentAtenDevice()\n\t\n\t\n\t\n\t_PyObject_MakeTpCall\n\t_PyEval_EvalFrameDefault\n\t\n\t_PyFunction_Vectorcall\n\t_PyEval_EvalFrameDefault\n\t\n\t_PyFunction_Vectorcall\n\t_PyEval_EvalFrameDefault\n\t\n\t_PyFunction_Vectorcall\n\t_PyEval_EvalFrameDefault\n\t\n\t_PyFunction_Vectorcall\n\t_PyEval_EvalFrameDefault\n\t\n\t_PyFunction_Vectorcall\n\t_PyEval_EvalFrameDefault\n\t\n\t_PyFunction_Vectorcall\n\t_PyEval_EvalFrameDefault\n\t\n\t_PyFunction_Vectorcall\n\t_PyEval_EvalFrameDefault\n\t\n\t_PyFunction_Vectorcall\n\t_PyEval_EvalFrameDefault\n\t\n\t_PyFunction_Vectorcall\n\tTHPFunction_apply(_object*, _object*)\n\t\n\tPyObject_Call\n\t_PyEval_EvalFrameDefault\n\t\n\t\n\t_PyEval_EvalFrameDefault\n\t\n\t_PyFunction_Vectorcall\n\t_PyEval_EvalFrameDefault\n\t\n\t_PyFunction_Vectorcall\n\t_PyEval_EvalFrameDefault\n\t\n\t_PyFunction_Vectorcall\n\t_PyEval_EvalFrameDefault\n\t\n\t_PyFunction_Vectorcall\n\t_PyEval_EvalFrameDefault\n\t\n\t_PyFunction_Vectorcall\n\t_PyEval_EvalFrameDefault\n\t\n\t\n\t\n\tPyObject_Call\n\t_PyEval_EvalFrameDefault\n\t\n\t\n\tPyObject_Call\n\t_PyEval_EvalFrameDefault\n\t\n\t\n\tPyObject_Call\n\t_PyEval_EvalFrameDefault\n\t\n\t_PyFunction_Vectorcall\n\tPyObject_Call\n\t_PyEval_EvalFrameDefault\n\t\n\t\n\tPyObject_Call\n\t_PyEval_EvalFrameDefault\n\t\n\t_PyFunction_Vectorcall\n\t_PyObject_FastCallDictTstate\n\t_PyObject_Call_Prepend\n\t\n\tPyObject_Call\n\t_PyEval_EvalFrameDefault\n\t\n\t_PyFunction_Vectorcall\n\t_PyEval_EvalFrameDefault\n\t_PyFunction_Vectorcall\n\t_PyEval_EvalFrameDefault\n\t\n\tPyEval_EvalCode\n\t\n\t\n\t\n\t_PyRun_SimpleFileObject\n\t_PyRun_AnyFileObject\n\tPy_RunMain\n\tPy_BytesMain\n\t__libc_start_main\n\t\n*** End stack trace ***\nUnknown PJRT_DEVICE 'CUDA'"
            },
            "pytorch_torch_compile_tvm": {
                "error": "backend='tvm' raised:\nModuleNotFoundError: No module named 'tvm'\n\nSet TORCH_LOGS=\"+dynamo\" and TORCHDYNAMO_VERBOSE=1 for more information\n\n\nYou can suppress this exception and fall back to eager by setting:\n    import torch._dynamo\n    torch._dynamo.config.suppress_errors = True\n"
            },
            "jax_flax_xla": {
                "error": "Unrecognized configuration class <class 'transformers.models.deberta_v2.configuration_deberta_v2.DebertaV2Config'> for this kind of AutoModel: FlaxAutoModel.\nModel type should be one of AlbertConfig, BartConfig, BeitConfig, BertConfig, BigBirdConfig, BlenderbotConfig, BlenderbotSmallConfig, BloomConfig, CLIPConfig, Dinov2Config, DistilBertConfig, ElectraConfig, GemmaConfig, GPT2Config, GPT2Config, GPTNeoConfig, GPTJConfig, LlamaConfig, LongT5Config, MarianConfig, MBartConfig, MistralConfig, MT5Config, OPTConfig, PegasusConfig, RegNetConfig, ResNetConfig, RobertaConfig, RobertaPreLayerNormConfig, RoFormerConfig, T5Config, VisionTextDualEncoderConfig, ViTConfig, Wav2Vec2Config, WhisperConfig, XGLMConfig, XLMRobertaConfig."
            },
            "tensorflow_eager": {
                "inference_s": 0.6690380120277405,
                "training_s": 2.182475492954254
            },
            "tensorflow_xla": {
                "inference_s": 0.007169899940490723,
                "training_s": 0.03305800914764404
            }
        }
    },
    {
        "DistilBertForMaskedLM": {
            "pytorch_torch_compile_inductor": {
                "inference_s": 0.01863626956939697,
                "training_s": 0.08792136430740356
            },
            "pytorch_torch_compile_eager": {
                "inference_s": 0.021961617469787597,
                "training_s": 0.10529442787170411
            },
            "pytorch_torch_compile_cudagraphs": {
                "inference_s": 0.03626620054244995,
                "training_s": 0.12187812328338624
            },
            "pytorch_torch_compile_onnxrt": {
                "error": "backend='onnxrt' raised:\nAssertionError: Please convert all Tensors to FakeTensors first or instantiate FakeTensorMode with 'allow_non_fake_inputs'. Found in aten.alias.default(tensor(-3.4028e+38, size=()))\n\nWhile executing %tensor : [num_users=1] = call_function[target=torch.tensor](args = (-3.4028234663852886e+38,), kwargs = {})\nOriginal traceback:\n  File \"/opt/conda/envs/xla-env/lib/python3.10/site-packages/transformers/models/distilbert/modeling_distilbert.py\", line 880, in forward\n    dlbrt_output = self.distilbert(\n  File \"/opt/conda/envs/xla-env/lib/python3.10/site-packages/transformers/models/distilbert/modeling_distilbert.py\", line 797, in forward\n    return self.transformer(\n  File \"/opt/conda/envs/xla-env/lib/python3.10/site-packages/transformers/models/distilbert/modeling_distilbert.py\", line 550, in forward\n    layer_outputs = layer_module(\n  File \"/opt/conda/envs/xla-env/lib/python3.10/site-packages/transformers/models/distilbert/modeling_distilbert.py\", line 476, in forward\n    sa_output = self.attention(\n  File \"/opt/conda/envs/xla-env/lib/python3.10/site-packages/transformers/models/distilbert/modeling_distilbert.py\", line 221, in forward\n    mask, torch.tensor(torch.finfo(scores.dtype).min)\n\n\nSet TORCH_LOGS=\"+dynamo\" and TORCHDYNAMO_VERBOSE=1 for more information\n\n\nYou can suppress this exception and fall back to eager by setting:\n    import torch._dynamo\n    torch._dynamo.config.suppress_errors = True\n"
            },
            "pytorch_torch_compile_openxla": {
                "error": "torch_xla/csrc/runtime/pjrt_registry.cc:214 : Check failed: client \n*** Begin stack trace ***\n\ttsl::CurrentStackTrace()\n\ttorch_xla::runtime::InitializePjRt(std::string const&)\n\ttorch_xla::runtime::PjRtComputationClient::PjRtComputationClient()\n\t\n\ttorch_xla::runtime::GetComputationClient()\n\ttorch_xla::bridge::GetDefaultDevice()\n\ttorch_xla::bridge::GetCurrentDevice()\n\ttorch_xla::bridge::GetCurrentAtenDevice()\n\t\n\t\n\t\n\t_PyObject_MakeTpCall\n\t_PyEval_EvalFrameDefault\n\t\n\t_PyFunction_Vectorcall\n\t_PyEval_EvalFrameDefault\n\t\n\t_PyFunction_Vectorcall\n\t_PyEval_EvalFrameDefault\n\t\n\t_PyFunction_Vectorcall\n\t_PyEval_EvalFrameDefault\n\t\n\t_PyFunction_Vectorcall\n\t_PyEval_EvalFrameDefault\n\t\n\t_PyFunction_Vectorcall\n\t_PyEval_EvalFrameDefault\n\t\n\t_PyFunction_Vectorcall\n\t_PyEval_EvalFrameDefault\n\t\n\t_PyFunction_Vectorcall\n\t_PyEval_EvalFrameDefault\n\t\n\t_PyFunction_Vectorcall\n\t_PyEval_EvalFrameDefault\n\t\n\t_PyFunction_Vectorcall\n\tTHPFunction_apply(_object*, _object*)\n\t\n\tPyObject_Call\n\t_PyEval_EvalFrameDefault\n\t\n\t\n\t_PyEval_EvalFrameDefault\n\t\n\t_PyFunction_Vectorcall\n\t_PyEval_EvalFrameDefault\n\t\n\t_PyFunction_Vectorcall\n\t_PyEval_EvalFrameDefault\n\t\n\t_PyFunction_Vectorcall\n\t_PyEval_EvalFrameDefault\n\t\n\t_PyFunction_Vectorcall\n\t_PyEval_EvalFrameDefault\n\t\n\t_PyFunction_Vectorcall\n\t_PyEval_EvalFrameDefault\n\t\n\t\n\t\n\tPyObject_Call\n\t_PyEval_EvalFrameDefault\n\t\n\t\n\tPyObject_Call\n\t_PyEval_EvalFrameDefault\n\t\n\t\n\tPyObject_Call\n\t_PyEval_EvalFrameDefault\n\t\n\t_PyFunction_Vectorcall\n\tPyObject_Call\n\t_PyEval_EvalFrameDefault\n\t\n\t\n\tPyObject_Call\n\t_PyEval_EvalFrameDefault\n\t\n\t_PyFunction_Vectorcall\n\t_PyObject_FastCallDictTstate\n\t_PyObject_Call_Prepend\n\t\n\tPyObject_Call\n\t_PyEval_EvalFrameDefault\n\t\n\t_PyFunction_Vectorcall\n\t_PyEval_EvalFrameDefault\n\t_PyFunction_Vectorcall\n\t_PyEval_EvalFrameDefault\n\t\n\tPyEval_EvalCode\n\t\n\t\n\t\n\t_PyRun_SimpleFileObject\n\t_PyRun_AnyFileObject\n\tPy_RunMain\n\tPy_BytesMain\n\t__libc_start_main\n\t\n*** End stack trace ***\nUnknown PJRT_DEVICE 'CUDA'"
            },
            "pytorch_torch_compile_tvm": {
                "inference_s": 0.024389266967773438,
                "training_s": 0.10864519596099853
            },
            "jax_flax_xla": {
                "inference_s": 0.0003989410400390625,
                "training_s": 0.002221786975860596
            },
            "tensorflow_eager": {
                "inference_s": 0.10072088003158569,
                "training_s": 0.28844839096069336
            },
            "tensorflow_xla": {
                "inference_s": 0.0016673707962036133,
                "training_s": 0.0038292574882507324
            }
        }
    },
    {
        "DistilBertForQuestionAnswering": {
            "pytorch_torch_compile_inductor": {
                "inference_s": 0.01175797700881958,
                "training_s": 0.06554309606552124
            },
            "pytorch_torch_compile_eager": {
                "inference_s": 0.015114870071411133,
                "training_s": 0.0776622462272644
            },
            "pytorch_torch_compile_cudagraphs": {
                "inference_s": 0.028927276134490965,
                "training_s": 0.09155826091766357
            },
            "pytorch_torch_compile_onnxrt": {
                "error": "backend='onnxrt' raised:\nAssertionError: Please convert all Tensors to FakeTensors first or instantiate FakeTensorMode with 'allow_non_fake_inputs'. Found in aten.alias.default(tensor(-3.4028e+38, size=()))\n\nWhile executing %tensor : [num_users=1] = call_function[target=torch.tensor](args = (-3.4028234663852886e+38,), kwargs = {})\nOriginal traceback:\n  File \"/opt/conda/envs/xla-env/lib/python3.10/site-packages/transformers/models/distilbert/modeling_distilbert.py\", line 1099, in forward\n    distilbert_output = self.distilbert(\n  File \"/opt/conda/envs/xla-env/lib/python3.10/site-packages/transformers/models/distilbert/modeling_distilbert.py\", line 797, in forward\n    return self.transformer(\n  File \"/opt/conda/envs/xla-env/lib/python3.10/site-packages/transformers/models/distilbert/modeling_distilbert.py\", line 550, in forward\n    layer_outputs = layer_module(\n  File \"/opt/conda/envs/xla-env/lib/python3.10/site-packages/transformers/models/distilbert/modeling_distilbert.py\", line 476, in forward\n    sa_output = self.attention(\n  File \"/opt/conda/envs/xla-env/lib/python3.10/site-packages/transformers/models/distilbert/modeling_distilbert.py\", line 221, in forward\n    mask, torch.tensor(torch.finfo(scores.dtype).min)\n\n\nSet TORCH_LOGS=\"+dynamo\" and TORCHDYNAMO_VERBOSE=1 for more information\n\n\nYou can suppress this exception and fall back to eager by setting:\n    import torch._dynamo\n    torch._dynamo.config.suppress_errors = True\n"
            },
            "pytorch_torch_compile_openxla": {
                "error": "torch_xla/csrc/runtime/pjrt_registry.cc:214 : Check failed: client \n*** Begin stack trace ***\n\ttsl::CurrentStackTrace()\n\ttorch_xla::runtime::InitializePjRt(std::string const&)\n\ttorch_xla::runtime::PjRtComputationClient::PjRtComputationClient()\n\t\n\ttorch_xla::runtime::GetComputationClient()\n\ttorch_xla::bridge::GetDefaultDevice()\n\ttorch_xla::bridge::GetCurrentDevice()\n\ttorch_xla::bridge::GetCurrentAtenDevice()\n\t\n\t\n\t\n\t_PyObject_MakeTpCall\n\t_PyEval_EvalFrameDefault\n\t\n\t_PyFunction_Vectorcall\n\t_PyEval_EvalFrameDefault\n\t\n\t_PyFunction_Vectorcall\n\t_PyEval_EvalFrameDefault\n\t\n\t_PyFunction_Vectorcall\n\t_PyEval_EvalFrameDefault\n\t\n\t_PyFunction_Vectorcall\n\t_PyEval_EvalFrameDefault\n\t\n\t_PyFunction_Vectorcall\n\t_PyEval_EvalFrameDefault\n\t\n\t_PyFunction_Vectorcall\n\t_PyEval_EvalFrameDefault\n\t\n\t_PyFunction_Vectorcall\n\t_PyEval_EvalFrameDefault\n\t\n\t_PyFunction_Vectorcall\n\t_PyEval_EvalFrameDefault\n\t\n\t_PyFunction_Vectorcall\n\tTHPFunction_apply(_object*, _object*)\n\t\n\tPyObject_Call\n\t_PyEval_EvalFrameDefault\n\t\n\t\n\t_PyEval_EvalFrameDefault\n\t\n\t_PyFunction_Vectorcall\n\t_PyEval_EvalFrameDefault\n\t\n\t_PyFunction_Vectorcall\n\t_PyEval_EvalFrameDefault\n\t\n\t_PyFunction_Vectorcall\n\t_PyEval_EvalFrameDefault\n\t\n\t_PyFunction_Vectorcall\n\t_PyEval_EvalFrameDefault\n\t\n\t_PyFunction_Vectorcall\n\t_PyEval_EvalFrameDefault\n\t\n\t\n\t\n\tPyObject_Call\n\t_PyEval_EvalFrameDefault\n\t\n\t\n\tPyObject_Call\n\t_PyEval_EvalFrameDefault\n\t\n\t\n\tPyObject_Call\n\t_PyEval_EvalFrameDefault\n\t\n\t_PyFunction_Vectorcall\n\tPyObject_Call\n\t_PyEval_EvalFrameDefault\n\t\n\t\n\tPyObject_Call\n\t_PyEval_EvalFrameDefault\n\t\n\t_PyFunction_Vectorcall\n\t_PyObject_FastCallDictTstate\n\t_PyObject_Call_Prepend\n\t\n\tPyObject_Call\n\t_PyEval_EvalFrameDefault\n\t\n\t_PyFunction_Vectorcall\n\t_PyEval_EvalFrameDefault\n\t_PyFunction_Vectorcall\n\t_PyEval_EvalFrameDefault\n\t\n\tPyEval_EvalCode\n\t\n\t\n\t\n\t_PyRun_SimpleFileObject\n\t_PyRun_AnyFileObject\n\tPy_RunMain\n\tPy_BytesMain\n\t__libc_start_main\n\t\n*** End stack trace ***\nUnknown PJRT_DEVICE 'CUDA'"
            },
            "pytorch_torch_compile_tvm": {
                "inference_s": 0.0172753643989563,
                "training_s": 0.07663649082183838
            },
            "jax_flax_xla": {
                "inference_s": 0.00041503190994262695,
                "training_s": 0.002299318313598633
            },
            "tensorflow_eager": {
                "inference_s": 0.09717570543289185,
                "training_s": 0.2758841276168823
            },
            "tensorflow_xla": {
                "inference_s": 0.0015733480453491212,
                "training_s": 0.003939182758331299
            }
        }
    },
    {
        "DistillGPT2": {
            "pytorch_torch_compile_inductor": {
                "inference_s": 0.010163877010345459,
                "training_s": 0.05750533580780029
            },
            "pytorch_torch_compile_eager": {
                "inference_s": 0.014473066329956055,
                "training_s": 0.0697019076347351
            },
            "pytorch_torch_compile_cudagraphs": {
                "inference_s": 0.02847034692764282,
                "training_s": 0.08984949350357056
            },
            "pytorch_torch_compile_onnxrt": {
                "inference_s": 0.1127955150604248,
                "training_s": 0.18542590379714965
            },
            "pytorch_torch_compile_openxla": {
                "error": "torch_xla/csrc/runtime/pjrt_registry.cc:214 : Check failed: client \n*** Begin stack trace ***\n\ttsl::CurrentStackTrace()\n\ttorch_xla::runtime::InitializePjRt(std::string const&)\n\ttorch_xla::runtime::PjRtComputationClient::PjRtComputationClient()\n\t\n\ttorch_xla::runtime::GetComputationClient()\n\ttorch_xla::bridge::GetDefaultDevice()\n\ttorch_xla::bridge::GetCurrentDevice()\n\ttorch_xla::bridge::GetCurrentAtenDevice()\n\t\n\t\n\t\n\t_PyObject_MakeTpCall\n\t_PyEval_EvalFrameDefault\n\t\n\t_PyFunction_Vectorcall\n\t_PyEval_EvalFrameDefault\n\t\n\t_PyFunction_Vectorcall\n\t_PyEval_EvalFrameDefault\n\t\n\t_PyFunction_Vectorcall\n\t_PyEval_EvalFrameDefault\n\t\n\t_PyFunction_Vectorcall\n\t_PyEval_EvalFrameDefault\n\t\n\t_PyFunction_Vectorcall\n\t_PyEval_EvalFrameDefault\n\t\n\t_PyFunction_Vectorcall\n\t_PyEval_EvalFrameDefault\n\t\n\t_PyFunction_Vectorcall\n\t_PyEval_EvalFrameDefault\n\t\n\t_PyFunction_Vectorcall\n\t_PyEval_EvalFrameDefault\n\t\n\t_PyFunction_Vectorcall\n\tTHPFunction_apply(_object*, _object*)\n\t\n\tPyObject_Call\n\t_PyEval_EvalFrameDefault\n\t\n\t\n\t_PyEval_EvalFrameDefault\n\t\n\t_PyFunction_Vectorcall\n\t_PyEval_EvalFrameDefault\n\t\n\t_PyFunction_Vectorcall\n\t_PyEval_EvalFrameDefault\n\t\n\t_PyFunction_Vectorcall\n\t_PyEval_EvalFrameDefault\n\t\n\t_PyFunction_Vectorcall\n\t_PyEval_EvalFrameDefault\n\t\n\t_PyFunction_Vectorcall\n\t_PyEval_EvalFrameDefault\n\t\n\t\n\t\n\tPyObject_Call\n\t_PyEval_EvalFrameDefault\n\t\n\t\n\tPyObject_Call\n\t_PyEval_EvalFrameDefault\n\t\n\t\n\tPyObject_Call\n\t_PyEval_EvalFrameDefault\n\t\n\t_PyFunction_Vectorcall\n\tPyObject_Call\n\t_PyEval_EvalFrameDefault\n\t\n\t\n\tPyObject_Call\n\t_PyEval_EvalFrameDefault\n\t\n\t_PyFunction_Vectorcall\n\t_PyObject_FastCallDictTstate\n\t_PyObject_Call_Prepend\n\t\n\tPyObject_Call\n\t_PyEval_EvalFrameDefault\n\t\n\t_PyFunction_Vectorcall\n\t_PyEval_EvalFrameDefault\n\t_PyFunction_Vectorcall\n\t_PyEval_EvalFrameDefault\n\t\n\tPyEval_EvalCode\n\t\n\t\n\t\n\t_PyRun_SimpleFileObject\n\t_PyRun_AnyFileObject\n\tPy_RunMain\n\tPy_BytesMain\n\t__libc_start_main\n\t\n*** End stack trace ***\nUnknown PJRT_DEVICE 'CUDA'"
            },
            "pytorch_torch_compile_tvm": {
                "inference_s": 0.016164717674255372,
                "training_s": 0.07210115194320679
            },
            "jax_flax_xla": {
                "inference_s": 0.0003450608253479004,
                "training_s": 0.0018687820434570312
            },
            "tensorflow_eager": {
                "error": "Asking to pad but the tokenizer does not have a padding token. Please select a token to use as `pad_token` `(tokenizer.pad_token = tokenizer.eos_token e.g.)` or add a new pad token via `tokenizer.add_special_tokens({'pad_token': '[PAD]'})`."
            },
            "tensorflow_xla": {
                "error": "Asking to pad but the tokenizer does not have a padding token. Please select a token to use as `pad_token` `(tokenizer.pad_token = tokenizer.eos_token e.g.)` or add a new pad token via `tokenizer.add_special_tokens({'pad_token': '[PAD]'})`."
            }
        }
    },
    {
        "ElectraForCausalLM": {
            "pytorch_torch_compile_inductor": {
                "inference_s": 0.008177273273468018,
                "training_s": 0.030325970649719237
            },
            "pytorch_torch_compile_eager": {
                "inference_s": 0.01373317003250122,
                "training_s": 0.04443093061447143
            },
            "pytorch_torch_compile_cudagraphs": {
                "inference_s": 0.039825651645660404,
                "training_s": 0.07310187816619873
            },
            "pytorch_torch_compile_onnxrt": {
                "inference_s": 0.047201375961303714,
                "training_s": 0.10682649850845337
            },
            "pytorch_torch_compile_openxla": {
                "error": "torch_xla/csrc/runtime/pjrt_registry.cc:214 : Check failed: client \n*** Begin stack trace ***\n\ttsl::CurrentStackTrace()\n\ttorch_xla::runtime::InitializePjRt(std::string const&)\n\ttorch_xla::runtime::PjRtComputationClient::PjRtComputationClient()\n\t\n\ttorch_xla::runtime::GetComputationClient()\n\ttorch_xla::bridge::GetDefaultDevice()\n\ttorch_xla::bridge::GetCurrentDevice()\n\ttorch_xla::bridge::GetCurrentAtenDevice()\n\t\n\t\n\t\n\t_PyObject_MakeTpCall\n\t_PyEval_EvalFrameDefault\n\t\n\t_PyFunction_Vectorcall\n\t_PyEval_EvalFrameDefault\n\t\n\t_PyFunction_Vectorcall\n\t_PyEval_EvalFrameDefault\n\t\n\t_PyFunction_Vectorcall\n\t_PyEval_EvalFrameDefault\n\t\n\t_PyFunction_Vectorcall\n\t_PyEval_EvalFrameDefault\n\t\n\t_PyFunction_Vectorcall\n\t_PyEval_EvalFrameDefault\n\t\n\t_PyFunction_Vectorcall\n\t_PyEval_EvalFrameDefault\n\t\n\t_PyFunction_Vectorcall\n\t_PyEval_EvalFrameDefault\n\t\n\t_PyFunction_Vectorcall\n\t_PyEval_EvalFrameDefault\n\t\n\t_PyFunction_Vectorcall\n\tTHPFunction_apply(_object*, _object*)\n\t\n\tPyObject_Call\n\t_PyEval_EvalFrameDefault\n\t\n\t\n\t_PyEval_EvalFrameDefault\n\t\n\t_PyFunction_Vectorcall\n\t_PyEval_EvalFrameDefault\n\t\n\t_PyFunction_Vectorcall\n\t_PyEval_EvalFrameDefault\n\t\n\t_PyFunction_Vectorcall\n\t_PyEval_EvalFrameDefault\n\t\n\t_PyFunction_Vectorcall\n\t_PyEval_EvalFrameDefault\n\t\n\t_PyFunction_Vectorcall\n\t_PyEval_EvalFrameDefault\n\t\n\t\n\t\n\tPyObject_Call\n\t_PyEval_EvalFrameDefault\n\t\n\t\n\tPyObject_Call\n\t_PyEval_EvalFrameDefault\n\t\n\t\n\tPyObject_Call\n\t_PyEval_EvalFrameDefault\n\t\n\t_PyFunction_Vectorcall\n\tPyObject_Call\n\t_PyEval_EvalFrameDefault\n\t\n\t\n\tPyObject_Call\n\t_PyEval_EvalFrameDefault\n\t\n\t_PyFunction_Vectorcall\n\t_PyObject_FastCallDictTstate\n\t_PyObject_Call_Prepend\n\t\n\tPyObject_Call\n\t_PyEval_EvalFrameDefault\n\t\n\t_PyFunction_Vectorcall\n\t_PyEval_EvalFrameDefault\n\t_PyFunction_Vectorcall\n\t_PyEval_EvalFrameDefault\n\t\n\tPyEval_EvalCode\n\t\n\t\n\t\n\t_PyRun_SimpleFileObject\n\t_PyRun_AnyFileObject\n\tPy_RunMain\n\tPy_BytesMain\n\t__libc_start_main\n\t\n*** End stack trace ***\nUnknown PJRT_DEVICE 'CUDA'"
            },
            "pytorch_torch_compile_tvm": {
                "inference_s": 0.01750129222869873,
                "training_s": 0.04794451951980591
            },
            "jax_flax_xla": {
                "inference_s": 0.0005807018280029297,
                "training_s": 0.0035596680641174316
            },
            "tensorflow_eager": {
                "error": "Unrecognized configuration class <class 'transformers.models.electra.configuration_electra.ElectraConfig'> for this kind of AutoModel: TFAutoModelForCausalLM.\nModel type should be one of BertConfig, CamembertConfig, CTRLConfig, GPT2Config, GPT2Config, GPTJConfig, MistralConfig, OpenAIGPTConfig, OPTConfig, RemBertConfig, RobertaConfig, RobertaPreLayerNormConfig, RoFormerConfig, TransfoXLConfig, XGLMConfig, XLMConfig, XLMRobertaConfig, XLNetConfig."
            },
            "tensorflow_xla": {
                "error": "Unrecognized configuration class <class 'transformers.models.electra.configuration_electra.ElectraConfig'> for this kind of AutoModel: TFAutoModelForCausalLM.\nModel type should be one of BertConfig, CamembertConfig, CTRLConfig, GPT2Config, GPT2Config, GPTJConfig, MistralConfig, OpenAIGPTConfig, OPTConfig, RemBertConfig, RobertaConfig, RobertaPreLayerNormConfig, RoFormerConfig, TransfoXLConfig, XGLMConfig, XLMConfig, XLMRobertaConfig, XLNetConfig."
            }
        }
    },
    {
        "ElectraForQuestionAnswering": {
            "pytorch_torch_compile_inductor": {
                "inference_s": 0.006622691154479981,
                "training_s": 0.026164820194244386
            },
            "pytorch_torch_compile_eager": {
                "inference_s": 0.011851017475128173,
                "training_s": 0.03894781112670898
            },
            "pytorch_torch_compile_cudagraphs": {
                "inference_s": 0.03775694131851196,
                "training_s": 0.0662157940864563
            },
            "pytorch_torch_compile_onnxrt": {
                "inference_s": 0.041648895740509034,
                "training_s": 0.10596037387847901
            },
            "pytorch_torch_compile_openxla": {
                "error": "torch_xla/csrc/runtime/pjrt_registry.cc:214 : Check failed: client \n*** Begin stack trace ***\n\ttsl::CurrentStackTrace()\n\ttorch_xla::runtime::InitializePjRt(std::string const&)\n\ttorch_xla::runtime::PjRtComputationClient::PjRtComputationClient()\n\t\n\ttorch_xla::runtime::GetComputationClient()\n\ttorch_xla::bridge::GetDefaultDevice()\n\ttorch_xla::bridge::GetCurrentDevice()\n\ttorch_xla::bridge::GetCurrentAtenDevice()\n\t\n\t\n\t\n\t_PyObject_MakeTpCall\n\t_PyEval_EvalFrameDefault\n\t\n\t_PyFunction_Vectorcall\n\t_PyEval_EvalFrameDefault\n\t\n\t_PyFunction_Vectorcall\n\t_PyEval_EvalFrameDefault\n\t\n\t_PyFunction_Vectorcall\n\t_PyEval_EvalFrameDefault\n\t\n\t_PyFunction_Vectorcall\n\t_PyEval_EvalFrameDefault\n\t\n\t_PyFunction_Vectorcall\n\t_PyEval_EvalFrameDefault\n\t\n\t_PyFunction_Vectorcall\n\t_PyEval_EvalFrameDefault\n\t\n\t_PyFunction_Vectorcall\n\t_PyEval_EvalFrameDefault\n\t\n\t_PyFunction_Vectorcall\n\t_PyEval_EvalFrameDefault\n\t\n\t_PyFunction_Vectorcall\n\tTHPFunction_apply(_object*, _object*)\n\t\n\tPyObject_Call\n\t_PyEval_EvalFrameDefault\n\t\n\t\n\t_PyEval_EvalFrameDefault\n\t\n\t_PyFunction_Vectorcall\n\t_PyEval_EvalFrameDefault\n\t\n\t_PyFunction_Vectorcall\n\t_PyEval_EvalFrameDefault\n\t\n\t_PyFunction_Vectorcall\n\t_PyEval_EvalFrameDefault\n\t\n\t_PyFunction_Vectorcall\n\t_PyEval_EvalFrameDefault\n\t\n\t_PyFunction_Vectorcall\n\t_PyEval_EvalFrameDefault\n\t\n\t\n\t\n\tPyObject_Call\n\t_PyEval_EvalFrameDefault\n\t\n\t\n\tPyObject_Call\n\t_PyEval_EvalFrameDefault\n\t\n\t\n\tPyObject_Call\n\t_PyEval_EvalFrameDefault\n\t\n\t_PyFunction_Vectorcall\n\tPyObject_Call\n\t_PyEval_EvalFrameDefault\n\t\n\t\n\tPyObject_Call\n\t_PyEval_EvalFrameDefault\n\t\n\t_PyFunction_Vectorcall\n\t_PyObject_FastCallDictTstate\n\t_PyObject_Call_Prepend\n\t\n\tPyObject_Call\n\t_PyEval_EvalFrameDefault\n\t\n\t_PyFunction_Vectorcall\n\t_PyEval_EvalFrameDefault\n\t_PyFunction_Vectorcall\n\t_PyEval_EvalFrameDefault\n\t\n\tPyEval_EvalCode\n\t\n\t\n\t\n\t_PyRun_SimpleFileObject\n\t_PyRun_AnyFileObject\n\tPy_RunMain\n\tPy_BytesMain\n\t__libc_start_main\n\t\n*** End stack trace ***\nUnknown PJRT_DEVICE 'CUDA'"
            },
            "pytorch_torch_compile_tvm": {
                "inference_s": 0.015574922561645508,
                "training_s": 0.042959907054901124
            },
            "jax_flax_xla": {
                "inference_s": 0.0005852699279785157,
                "training_s": 0.0038414621353149415
            },
            "tensorflow_eager": {
                "inference_s": 0.1912428045272827,
                "training_s": 0.5356689667701722
            },
            "tensorflow_xla": {
                "inference_s": 0.0026843714714050292,
                "training_s": 0.0068079900741577146
            }
        }
    },
    {
        "GoogleFnet": {
            "pytorch_torch_compile_inductor": {
                "inference_s": 0.030165324211120604,
                "training_s": 0.12705384254455565
            },
            "pytorch_torch_compile_eager": {
                "inference_s": 0.0354352855682373,
                "training_s": 0.14522534847259522
            },
            "pytorch_torch_compile_cudagraphs": {
                "inference_s": 0.04698073863983154,
                "training_s": 0.16468663692474364
            },
            "pytorch_torch_compile_onnxrt": {
                "inference_s": 0.20468347549438476,
                "training_s": 0.3566495561599731
            },
            "pytorch_torch_compile_openxla": {
                "inference_s": 0.038750319480895995,
                "training_s": 0.15033519983291627
            },
            "pytorch_torch_compile_tvm": {
                "inference_s": 0.03907020807266235,
                "training_s": 0.1557848882675171
            },
            "jax_flax_xla": {
                "error": "Unrecognized configuration class <class 'transformers.models.fnet.configuration_fnet.FNetConfig'> for this kind of AutoModel: FlaxAutoModel.\nModel type should be one of AlbertConfig, BartConfig, BeitConfig, BertConfig, BigBirdConfig, BlenderbotConfig, BlenderbotSmallConfig, BloomConfig, CLIPConfig, Dinov2Config, DistilBertConfig, ElectraConfig, GemmaConfig, GPT2Config, GPT2Config, GPTNeoConfig, GPTJConfig, LlamaConfig, LongT5Config, MarianConfig, MBartConfig, MistralConfig, MT5Config, OPTConfig, PegasusConfig, RegNetConfig, ResNetConfig, RobertaConfig, RobertaPreLayerNormConfig, RoFormerConfig, T5Config, VisionTextDualEncoderConfig, ViTConfig, Wav2Vec2Config, WhisperConfig, XGLMConfig, XLMRobertaConfig."
            },
            "tensorflow_eager": {
                "error": "Unrecognized configuration class <class 'transformers.models.fnet.configuration_fnet.FNetConfig'> for this kind of AutoModel: TFAutoModel.\nModel type should be one of AlbertConfig, BartConfig, BertConfig, BlenderbotConfig, BlenderbotSmallConfig, BlipConfig, CamembertConfig, CLIPConfig, ConvBertConfig, ConvNextConfig, ConvNextV2Config, CTRLConfig, CvtConfig, Data2VecVisionConfig, DebertaConfig, DebertaV2Config, DeiTConfig, DistilBertConfig, DPRConfig, EfficientFormerConfig, ElectraConfig, EsmConfig, FlaubertConfig, FunnelConfig, GPT2Config, GPT2Config, GPTJConfig, GroupViTConfig, HubertConfig, IdeficsConfig, LayoutLMConfig, LayoutLMv3Config, LEDConfig, LongformerConfig, LxmertConfig, MarianConfig, MBartConfig, MistralConfig, MobileBertConfig, MobileViTConfig, MPNetConfig, MT5Config, OpenAIGPTConfig, OPTConfig, PegasusConfig, RegNetConfig, RemBertConfig, ResNetConfig, RobertaConfig, RobertaPreLayerNormConfig, RoFormerConfig, SamConfig, SegformerConfig, Speech2TextConfig, SwiftFormerConfig, SwinConfig, T5Config, TapasConfig, TransfoXLConfig, VisionTextDualEncoderConfig, ViTConfig, ViTMAEConfig, Wav2Vec2Config, WhisperConfig, XGLMConfig, XLMConfig, XLMRobertaConfig, XLNetConfig."
            },
            "tensorflow_xla": {
                "error": "Unrecognized configuration class <class 'transformers.models.fnet.configuration_fnet.FNetConfig'> for this kind of AutoModel: TFAutoModel.\nModel type should be one of AlbertConfig, BartConfig, BertConfig, BlenderbotConfig, BlenderbotSmallConfig, BlipConfig, CamembertConfig, CLIPConfig, ConvBertConfig, ConvNextConfig, ConvNextV2Config, CTRLConfig, CvtConfig, Data2VecVisionConfig, DebertaConfig, DebertaV2Config, DeiTConfig, DistilBertConfig, DPRConfig, EfficientFormerConfig, ElectraConfig, EsmConfig, FlaubertConfig, FunnelConfig, GPT2Config, GPT2Config, GPTJConfig, GroupViTConfig, HubertConfig, IdeficsConfig, LayoutLMConfig, LayoutLMv3Config, LEDConfig, LongformerConfig, LxmertConfig, MarianConfig, MBartConfig, MistralConfig, MobileBertConfig, MobileViTConfig, MPNetConfig, MT5Config, OpenAIGPTConfig, OPTConfig, PegasusConfig, RegNetConfig, RemBertConfig, ResNetConfig, RobertaConfig, RobertaPreLayerNormConfig, RoFormerConfig, SamConfig, SegformerConfig, Speech2TextConfig, SwiftFormerConfig, SwinConfig, T5Config, TapasConfig, TransfoXLConfig, VisionTextDualEncoderConfig, ViTConfig, ViTMAEConfig, Wav2Vec2Config, WhisperConfig, XGLMConfig, XLMConfig, XLMRobertaConfig, XLNetConfig."
            }
        }
    },
    {
        "GPT2ForSequenceClassification": {
            "pytorch_torch_compile_inductor": {
                "inference_s": 0.019526662826538085,
                "training_s": 0.12895687103271483
            },
            "pytorch_torch_compile_eager": {
                "inference_s": 0.028576600551605224,
                "training_s": 0.1418810272216797
            },
            "pytorch_torch_compile_cudagraphs": {
                "inference_s": 0.056034998893737795,
                "training_s": 0.17072853326797485
            },
            "pytorch_torch_compile_onnxrt": {
                "inference_s": 0.21467651605606078,
                "training_s": 0.3163098645210266
            },
            "pytorch_torch_compile_openxla": {
                "error": "torch_xla/csrc/runtime/pjrt_registry.cc:214 : Check failed: client \n*** Begin stack trace ***\n\ttsl::CurrentStackTrace()\n\ttorch_xla::runtime::InitializePjRt(std::string const&)\n\ttorch_xla::runtime::PjRtComputationClient::PjRtComputationClient()\n\t\n\ttorch_xla::runtime::GetComputationClient()\n\ttorch_xla::bridge::GetDefaultDevice()\n\ttorch_xla::bridge::GetCurrentDevice()\n\ttorch_xla::bridge::GetCurrentAtenDevice()\n\t\n\t\n\t\n\t_PyObject_MakeTpCall\n\t_PyEval_EvalFrameDefault\n\t\n\t_PyFunction_Vectorcall\n\t_PyEval_EvalFrameDefault\n\t\n\t_PyFunction_Vectorcall\n\t_PyEval_EvalFrameDefault\n\t\n\t_PyFunction_Vectorcall\n\t_PyEval_EvalFrameDefault\n\t\n\t_PyFunction_Vectorcall\n\t_PyEval_EvalFrameDefault\n\t\n\t_PyFunction_Vectorcall\n\t_PyEval_EvalFrameDefault\n\t\n\t_PyFunction_Vectorcall\n\t_PyEval_EvalFrameDefault\n\t\n\t_PyFunction_Vectorcall\n\t_PyEval_EvalFrameDefault\n\t\n\t_PyFunction_Vectorcall\n\t_PyEval_EvalFrameDefault\n\t\n\t_PyFunction_Vectorcall\n\tTHPFunction_apply(_object*, _object*)\n\t\n\tPyObject_Call\n\t_PyEval_EvalFrameDefault\n\t\n\t\n\t_PyEval_EvalFrameDefault\n\t\n\t_PyFunction_Vectorcall\n\t_PyEval_EvalFrameDefault\n\t\n\t_PyFunction_Vectorcall\n\t_PyEval_EvalFrameDefault\n\t\n\t_PyFunction_Vectorcall\n\t_PyEval_EvalFrameDefault\n\t\n\t_PyFunction_Vectorcall\n\t_PyEval_EvalFrameDefault\n\t\n\t_PyFunction_Vectorcall\n\t_PyEval_EvalFrameDefault\n\t\n\t\n\t\n\tPyObject_Call\n\t_PyEval_EvalFrameDefault\n\t\n\t\n\tPyObject_Call\n\t_PyEval_EvalFrameDefault\n\t\n\t\n\tPyObject_Call\n\t_PyEval_EvalFrameDefault\n\t\n\t_PyFunction_Vectorcall\n\tPyObject_Call\n\t_PyEval_EvalFrameDefault\n\t\n\t\n\tPyObject_Call\n\t_PyEval_EvalFrameDefault\n\t\n\t_PyFunction_Vectorcall\n\t_PyObject_FastCallDictTstate\n\t_PyObject_Call_Prepend\n\t\n\tPyObject_Call\n\t_PyEval_EvalFrameDefault\n\t\n\t_PyFunction_Vectorcall\n\t_PyEval_EvalFrameDefault\n\t_PyFunction_Vectorcall\n\t_PyEval_EvalFrameDefault\n\t\n\tPyEval_EvalCode\n\t\n\t\n\t\n\t_PyRun_SimpleFileObject\n\t_PyRun_AnyFileObject\n\tPy_RunMain\n\tPy_BytesMain\n\t__libc_start_main\n\t\n*** End stack trace ***\nUnknown PJRT_DEVICE 'CUDA'"
            },
            "pytorch_torch_compile_tvm": {
                "inference_s": 0.0334324049949646,
                "training_s": 0.14737802028656005
            },
            "jax_flax_xla": {
                "inference_s": 0.0006648182868957519,
                "training_s": 0.0029027652740478515
            },
            "tensorflow_eager": {
                "error": "Asking to pad but the tokenizer does not have a padding token. Please select a token to use as `pad_token` `(tokenizer.pad_token = tokenizer.eos_token e.g.)` or add a new pad token via `tokenizer.add_special_tokens({'pad_token': '[PAD]'})`."
            },
            "tensorflow_xla": {
                "error": "Asking to pad but the tokenizer does not have a padding token. Please select a token to use as `pad_token` `(tokenizer.pad_token = tokenizer.eos_token e.g.)` or add a new pad token via `tokenizer.add_special_tokens({'pad_token': '[PAD]'})`."
            }
        }
    },
    {
        "LayoutLMForMaskedLM": {
            "pytorch_torch_compile_inductor": {
                "inference_s": 0.033179278373718264,
                "training_s": 0.1401208758354187
            },
            "pytorch_torch_compile_eager": {
                "inference_s": 0.03660557746887207,
                "training_s": 0.14602858304977417
            },
            "pytorch_torch_compile_cudagraphs": {
                "inference_s": 0.06548347234725953,
                "training_s": 0.16413050174713134
            },
            "pytorch_torch_compile_onnxrt": {
                "inference_s": 0.12371568441390991,
                "training_s": 0.2541581392288208
            },
            "pytorch_torch_compile_openxla": {
                "error": "torch_xla/csrc/runtime/pjrt_registry.cc:214 : Check failed: client \n*** Begin stack trace ***\n\ttsl::CurrentStackTrace()\n\ttorch_xla::runtime::InitializePjRt(std::string const&)\n\ttorch_xla::runtime::PjRtComputationClient::PjRtComputationClient()\n\t\n\ttorch_xla::runtime::GetComputationClient()\n\ttorch_xla::bridge::GetDefaultDevice()\n\ttorch_xla::bridge::GetCurrentDevice()\n\ttorch_xla::bridge::GetCurrentAtenDevice()\n\t\n\t\n\t\n\t_PyObject_MakeTpCall\n\t_PyEval_EvalFrameDefault\n\t\n\t_PyFunction_Vectorcall\n\t_PyEval_EvalFrameDefault\n\t\n\t_PyFunction_Vectorcall\n\t_PyEval_EvalFrameDefault\n\t\n\t_PyFunction_Vectorcall\n\t_PyEval_EvalFrameDefault\n\t\n\t_PyFunction_Vectorcall\n\t_PyEval_EvalFrameDefault\n\t\n\t_PyFunction_Vectorcall\n\t_PyEval_EvalFrameDefault\n\t\n\t_PyFunction_Vectorcall\n\t_PyEval_EvalFrameDefault\n\t\n\t_PyFunction_Vectorcall\n\t_PyEval_EvalFrameDefault\n\t\n\t_PyFunction_Vectorcall\n\t_PyEval_EvalFrameDefault\n\t\n\t_PyFunction_Vectorcall\n\tTHPFunction_apply(_object*, _object*)\n\t\n\tPyObject_Call\n\t_PyEval_EvalFrameDefault\n\t\n\t\n\t_PyEval_EvalFrameDefault\n\t\n\t_PyFunction_Vectorcall\n\t_PyEval_EvalFrameDefault\n\t\n\t_PyFunction_Vectorcall\n\t_PyEval_EvalFrameDefault\n\t\n\t_PyFunction_Vectorcall\n\t_PyEval_EvalFrameDefault\n\t\n\t_PyFunction_Vectorcall\n\t_PyEval_EvalFrameDefault\n\t\n\t_PyFunction_Vectorcall\n\t_PyEval_EvalFrameDefault\n\t\n\t\n\t\n\tPyObject_Call\n\t_PyEval_EvalFrameDefault\n\t\n\t\n\tPyObject_Call\n\t_PyEval_EvalFrameDefault\n\t\n\t\n\tPyObject_Call\n\t_PyEval_EvalFrameDefault\n\t\n\t_PyFunction_Vectorcall\n\tPyObject_Call\n\t_PyEval_EvalFrameDefault\n\t\n\t\n\tPyObject_Call\n\t_PyEval_EvalFrameDefault\n\t\n\t_PyFunction_Vectorcall\n\t_PyObject_FastCallDictTstate\n\t_PyObject_Call_Prepend\n\t\n\tPyObject_Call\n\t_PyEval_EvalFrameDefault\n\t\n\t_PyFunction_Vectorcall\n\t_PyEval_EvalFrameDefault\n\t_PyFunction_Vectorcall\n\t_PyEval_EvalFrameDefault\n\t\n\tPyEval_EvalCode\n\t\n\t\n\t\n\t_PyRun_SimpleFileObject\n\t_PyRun_AnyFileObject\n\tPy_RunMain\n\tPy_BytesMain\n\t__libc_start_main\n\t\n*** End stack trace ***\nUnknown PJRT_DEVICE 'CUDA'"
            },
            "pytorch_torch_compile_tvm": {
                "inference_s": 0.04178394317626953,
                "training_s": 0.13531235456466675
            },
            "jax_flax_xla": {
                "error": "Unrecognized configuration class <class 'transformers.models.layoutlm.configuration_layoutlm.LayoutLMConfig'> for this kind of AutoModel: FlaxAutoModel.\nModel type should be one of AlbertConfig, BartConfig, BeitConfig, BertConfig, BigBirdConfig, BlenderbotConfig, BlenderbotSmallConfig, BloomConfig, CLIPConfig, Dinov2Config, DistilBertConfig, ElectraConfig, GemmaConfig, GPT2Config, GPT2Config, GPTNeoConfig, GPTJConfig, LlamaConfig, LongT5Config, MarianConfig, MBartConfig, MistralConfig, MT5Config, OPTConfig, PegasusConfig, RegNetConfig, ResNetConfig, RobertaConfig, RobertaPreLayerNormConfig, RoFormerConfig, T5Config, VisionTextDualEncoderConfig, ViTConfig, Wav2Vec2Config, WhisperConfig, XGLMConfig, XLMRobertaConfig."
            },
            "tensorflow_eager": {
                "inference_s": 0.19699535131454468,
                "training_s": 0.5774533128738404
            },
            "tensorflow_xla": {
                "inference_s": 0.002772085666656494,
                "training_s": 0.0074076676368713375
            }
        }
    },
    {
        "LayoutLMForSequenceClassification": {
            "pytorch_torch_compile_inductor": {
                "inference_s": 0.02289893388748169,
                "training_s": 0.09149527072906494
            },
            "pytorch_torch_compile_eager": {
                "inference_s": 0.03094810724258423,
                "training_s": 0.1262506937980652
            },
            "pytorch_torch_compile_cudagraphs": {
                "inference_s": 0.06075786828994751,
                "training_s": 0.1383698105812073
            },
            "pytorch_torch_compile_onnxrt": {
                "inference_s": 0.09610881328582764,
                "training_s": 0.19794790267944337
            },
            "pytorch_torch_compile_openxla": {
                "error": "torch_xla/csrc/runtime/pjrt_registry.cc:214 : Check failed: client \n*** Begin stack trace ***\n\ttsl::CurrentStackTrace()\n\ttorch_xla::runtime::InitializePjRt(std::string const&)\n\ttorch_xla::runtime::PjRtComputationClient::PjRtComputationClient()\n\t\n\ttorch_xla::runtime::GetComputationClient()\n\ttorch_xla::bridge::GetDefaultDevice()\n\ttorch_xla::bridge::GetCurrentDevice()\n\ttorch_xla::bridge::GetCurrentAtenDevice()\n\t\n\t\n\t\n\t_PyObject_MakeTpCall\n\t_PyEval_EvalFrameDefault\n\t\n\t_PyFunction_Vectorcall\n\t_PyEval_EvalFrameDefault\n\t\n\t_PyFunction_Vectorcall\n\t_PyEval_EvalFrameDefault\n\t\n\t_PyFunction_Vectorcall\n\t_PyEval_EvalFrameDefault\n\t\n\t_PyFunction_Vectorcall\n\t_PyEval_EvalFrameDefault\n\t\n\t_PyFunction_Vectorcall\n\t_PyEval_EvalFrameDefault\n\t\n\t_PyFunction_Vectorcall\n\t_PyEval_EvalFrameDefault\n\t\n\t_PyFunction_Vectorcall\n\t_PyEval_EvalFrameDefault\n\t\n\t_PyFunction_Vectorcall\n\t_PyEval_EvalFrameDefault\n\t\n\t_PyFunction_Vectorcall\n\tTHPFunction_apply(_object*, _object*)\n\t\n\tPyObject_Call\n\t_PyEval_EvalFrameDefault\n\t\n\t\n\t_PyEval_EvalFrameDefault\n\t\n\t_PyFunction_Vectorcall\n\t_PyEval_EvalFrameDefault\n\t\n\t_PyFunction_Vectorcall\n\t_PyEval_EvalFrameDefault\n\t\n\t_PyFunction_Vectorcall\n\t_PyEval_EvalFrameDefault\n\t\n\t_PyFunction_Vectorcall\n\t_PyEval_EvalFrameDefault\n\t\n\t_PyFunction_Vectorcall\n\t_PyEval_EvalFrameDefault\n\t\n\t\n\t\n\tPyObject_Call\n\t_PyEval_EvalFrameDefault\n\t\n\t\n\tPyObject_Call\n\t_PyEval_EvalFrameDefault\n\t\n\t\n\tPyObject_Call\n\t_PyEval_EvalFrameDefault\n\t\n\t_PyFunction_Vectorcall\n\tPyObject_Call\n\t_PyEval_EvalFrameDefault\n\t\n\t\n\tPyObject_Call\n\t_PyEval_EvalFrameDefault\n\t\n\t_PyFunction_Vectorcall\n\t_PyObject_FastCallDictTstate\n\t_PyObject_Call_Prepend\n\t\n\tPyObject_Call\n\t_PyEval_EvalFrameDefault\n\t\n\t_PyFunction_Vectorcall\n\t_PyEval_EvalFrameDefault\n\t_PyFunction_Vectorcall\n\t_PyEval_EvalFrameDefault\n\t\n\tPyEval_EvalCode\n\t\n\t\n\t\n\t_PyRun_SimpleFileObject\n\t_PyRun_AnyFileObject\n\tPy_RunMain\n\tPy_BytesMain\n\t__libc_start_main\n\t\n*** End stack trace ***\nUnknown PJRT_DEVICE 'CUDA'"
            },
            "pytorch_torch_compile_tvm": {
                "inference_s": 0.03443866491317749,
                "training_s": 0.11109065532684326
            },
            "jax_flax_xla": {
                "error": "Unrecognized configuration class <class 'transformers.models.layoutlm.configuration_layoutlm.LayoutLMConfig'> for this kind of AutoModel: FlaxAutoModel.\nModel type should be one of AlbertConfig, BartConfig, BeitConfig, BertConfig, BigBirdConfig, BlenderbotConfig, BlenderbotSmallConfig, BloomConfig, CLIPConfig, Dinov2Config, DistilBertConfig, ElectraConfig, GemmaConfig, GPT2Config, GPT2Config, GPTNeoConfig, GPTJConfig, LlamaConfig, LongT5Config, MarianConfig, MBartConfig, MistralConfig, MT5Config, OPTConfig, PegasusConfig, RegNetConfig, ResNetConfig, RobertaConfig, RobertaPreLayerNormConfig, RoFormerConfig, T5Config, VisionTextDualEncoderConfig, ViTConfig, Wav2Vec2Config, WhisperConfig, XGLMConfig, XLMRobertaConfig."
            },
            "tensorflow_eager": {
                "inference_s": 0.19187570333480836,
                "training_s": 0.580088620185852
            },
            "tensorflow_xla": {
                "inference_s": 0.0027742528915405272,
                "training_s": 0.008033795356750488
            }
        }
    },
    {
        "M2M100ForConditionalGeneration": {
            "pytorch_torch_compile_inductor": {
                "inference_s": 0.15205262660980223,
                "training_s": 0.616232750415802
            },
            "pytorch_torch_compile_eager": {
                "inference_s": 0.16960955619812013,
                "training_s": 0.6132322478294373
            },
            "pytorch_torch_compile_cudagraphs": {
                "inference_s": 0.24121008872985839,
                "training_s": 0.7200110340118409
            },
            "pytorch_torch_compile_onnxrt": {
                "inference_s": 0.49413926124572755,
                "training_s": 1.1104913568496704
            },
            "pytorch_torch_compile_openxla": {
                "inference_s": 0.1632990336418152,
                "training_s": 0.6642979311943055
            },
            "pytorch_torch_compile_tvm": {
                "inference_s": 0.16753698825836183,
                "training_s": 0.6616211748123169
            },
            "jax_flax_xla": {
                "error": "Unrecognized configuration class <class 'transformers.models.m2m_100.configuration_m2m_100.M2M100Config'> for this kind of AutoModel: FlaxAutoModel.\nModel type should be one of AlbertConfig, BartConfig, BeitConfig, BertConfig, BigBirdConfig, BlenderbotConfig, BlenderbotSmallConfig, BloomConfig, CLIPConfig, Dinov2Config, DistilBertConfig, ElectraConfig, GemmaConfig, GPT2Config, GPT2Config, GPTNeoConfig, GPTJConfig, LlamaConfig, LongT5Config, MarianConfig, MBartConfig, MistralConfig, MT5Config, OPTConfig, PegasusConfig, RegNetConfig, ResNetConfig, RobertaConfig, RobertaPreLayerNormConfig, RoFormerConfig, T5Config, VisionTextDualEncoderConfig, ViTConfig, Wav2Vec2Config, WhisperConfig, XGLMConfig, XLMRobertaConfig."
            },
            "error": "AttributeError(\"'M2M100ForConditionalGeneration' object has no attribute '_shift_right'\")"
        }
    },
    {
        "MBartForConditionalGeneration": {
            "pytorch_torch_compile_inductor": {
                "inference_s": 0.15664658308029175,
                "training_s": 0.7857049942016602
            },
            "pytorch_torch_compile_eager": {
                "inference_s": 0.18289826869964598,
                "training_s": 0.8584749746322632
            },
            "pytorch_torch_compile_cudagraphs": {
                "inference_s": 0.2642602801322937,
                "training_s": 0.9660069155693054
            },
            "pytorch_torch_compile_onnxrt": {
                "inference_s": 0.6657650780677795,
                "training_s": 1.5562545990943908
            },
            "pytorch_torch_compile_openxla": {
                "inference_s": 0.18511594772338869,
                "training_s": 0.9230837059020996
            },
            "pytorch_torch_compile_tvm": {
                "inference_s": 0.18424973726272584,
                "training_s": 0.9192810201644898
            },
            "jax_flax_xla": {
                "error": "facebook/mbart-large-50 does not appear to have a file named flax_model.msgpack but there is a file for PyTorch weights. Use `from_pt=True` to load this model from those weights."
            },
            "error": "AttributeError(\"'MBartForConditionalGeneration' object has no attribute '_shift_right'\")"
        }
    },
    {
        "MegatronBertForCausalLM": {
            "pytorch_torch_compile_inductor": {
                "inference_s": 0.08046668529510498,
                "training_s": 0.39887774229049683
            },
            "pytorch_torch_compile_eager": {
                "inference_s": 0.097542724609375,
                "training_s": 0.3476486802101135
            },
            "pytorch_torch_compile_cudagraphs": {
                "inference_s": 0.1489427161216736,
                "training_s": 0.4028339171409607
            },
            "pytorch_torch_compile_onnxrt": {
                "inference_s": 0.3564783477783203,
                "training_s": 0.6369002866744995
            },
            "pytorch_torch_compile_openxla": {
                "inference_s": 0.11076748132705688,
                "training_s": 0.3889051127433777
            },
            "pytorch_torch_compile_tvm": {
                "inference_s": 0.11110765457153321,
                "training_s": 0.35285813808441163
            },
            "jax_flax_xla": {
                "error": "Unrecognized configuration class <class 'transformers.models.megatron_bert.configuration_megatron_bert.MegatronBertConfig'> for this kind of AutoModel: FlaxAutoModel.\nModel type should be one of AlbertConfig, BartConfig, BeitConfig, BertConfig, BigBirdConfig, BlenderbotConfig, BlenderbotSmallConfig, BloomConfig, CLIPConfig, Dinov2Config, DistilBertConfig, ElectraConfig, GemmaConfig, GPT2Config, GPT2Config, GPTNeoConfig, GPTJConfig, LlamaConfig, LongT5Config, MarianConfig, MBartConfig, MistralConfig, MT5Config, OPTConfig, PegasusConfig, RegNetConfig, ResNetConfig, RobertaConfig, RobertaPreLayerNormConfig, RoFormerConfig, T5Config, VisionTextDualEncoderConfig, ViTConfig, Wav2Vec2Config, WhisperConfig, XGLMConfig, XLMRobertaConfig."
            },
            "tensorflow_eager": {
                "error": "Unrecognized configuration class <class 'transformers.models.megatron_bert.configuration_megatron_bert.MegatronBertConfig'> for this kind of AutoModel: TFAutoModel.\nModel type should be one of AlbertConfig, BartConfig, BertConfig, BlenderbotConfig, BlenderbotSmallConfig, BlipConfig, CamembertConfig, CLIPConfig, ConvBertConfig, ConvNextConfig, ConvNextV2Config, CTRLConfig, CvtConfig, Data2VecVisionConfig, DebertaConfig, DebertaV2Config, DeiTConfig, DistilBertConfig, DPRConfig, EfficientFormerConfig, ElectraConfig, EsmConfig, FlaubertConfig, FunnelConfig, GPT2Config, GPT2Config, GPTJConfig, GroupViTConfig, HubertConfig, IdeficsConfig, LayoutLMConfig, LayoutLMv3Config, LEDConfig, LongformerConfig, LxmertConfig, MarianConfig, MBartConfig, MistralConfig, MobileBertConfig, MobileViTConfig, MPNetConfig, MT5Config, OpenAIGPTConfig, OPTConfig, PegasusConfig, RegNetConfig, RemBertConfig, ResNetConfig, RobertaConfig, RobertaPreLayerNormConfig, RoFormerConfig, SamConfig, SegformerConfig, Speech2TextConfig, SwiftFormerConfig, SwinConfig, T5Config, TapasConfig, TransfoXLConfig, VisionTextDualEncoderConfig, ViTConfig, ViTMAEConfig, Wav2Vec2Config, WhisperConfig, XGLMConfig, XLMConfig, XLMRobertaConfig, XLNetConfig."
            },
            "tensorflow_xla": {
                "error": "Unrecognized configuration class <class 'transformers.models.megatron_bert.configuration_megatron_bert.MegatronBertConfig'> for this kind of AutoModel: TFAutoModel.\nModel type should be one of AlbertConfig, BartConfig, BertConfig, BlenderbotConfig, BlenderbotSmallConfig, BlipConfig, CamembertConfig, CLIPConfig, ConvBertConfig, ConvNextConfig, ConvNextV2Config, CTRLConfig, CvtConfig, Data2VecVisionConfig, DebertaConfig, DebertaV2Config, DeiTConfig, DistilBertConfig, DPRConfig, EfficientFormerConfig, ElectraConfig, EsmConfig, FlaubertConfig, FunnelConfig, GPT2Config, GPT2Config, GPTJConfig, GroupViTConfig, HubertConfig, IdeficsConfig, LayoutLMConfig, LayoutLMv3Config, LEDConfig, LongformerConfig, LxmertConfig, MarianConfig, MBartConfig, MistralConfig, MobileBertConfig, MobileViTConfig, MPNetConfig, MT5Config, OpenAIGPTConfig, OPTConfig, PegasusConfig, RegNetConfig, RemBertConfig, ResNetConfig, RobertaConfig, RobertaPreLayerNormConfig, RoFormerConfig, SamConfig, SegformerConfig, Speech2TextConfig, SwiftFormerConfig, SwinConfig, T5Config, TapasConfig, TransfoXLConfig, VisionTextDualEncoderConfig, ViTConfig, ViTMAEConfig, Wav2Vec2Config, WhisperConfig, XGLMConfig, XLMConfig, XLMRobertaConfig, XLNetConfig."
            }
        }
    },
    {
        "MegatronBertForQuestionAnswering": {
            "pytorch_torch_compile_inductor": {
                "inference_s": 0.07222132921218873,
                "training_s": 0.4467200040817261
            },
            "pytorch_torch_compile_eager": {
                "inference_s": 0.09053476810455323,
                "training_s": 0.39230018854141235
            },
            "pytorch_torch_compile_cudagraphs": {
                "inference_s": 0.14044231176376343,
                "training_s": 0.44627482175827027
            },
            "pytorch_torch_compile_onnxrt": {
                "inference_s": 0.32253570795059205,
                "training_s": 0.5591227388381959
            },
            "pytorch_torch_compile_openxla": {
                "inference_s": 0.10293773412704468,
                "training_s": 0.3532972812652588
            },
            "pytorch_torch_compile_tvm": {
                "inference_s": 0.10002121210098266,
                "training_s": 0.3553775238990784
            },
            "jax_flax_xla": {
                "error": "Unrecognized configuration class <class 'transformers.models.megatron_bert.configuration_megatron_bert.MegatronBertConfig'> for this kind of AutoModel: FlaxAutoModel.\nModel type should be one of AlbertConfig, BartConfig, BeitConfig, BertConfig, BigBirdConfig, BlenderbotConfig, BlenderbotSmallConfig, BloomConfig, CLIPConfig, Dinov2Config, DistilBertConfig, ElectraConfig, GemmaConfig, GPT2Config, GPT2Config, GPTNeoConfig, GPTJConfig, LlamaConfig, LongT5Config, MarianConfig, MBartConfig, MistralConfig, MT5Config, OPTConfig, PegasusConfig, RegNetConfig, ResNetConfig, RobertaConfig, RobertaPreLayerNormConfig, RoFormerConfig, T5Config, VisionTextDualEncoderConfig, ViTConfig, Wav2Vec2Config, WhisperConfig, XGLMConfig, XLMRobertaConfig."
            },
            "tensorflow_eager": {
                "error": "Unrecognized configuration class <class 'transformers.models.megatron_bert.configuration_megatron_bert.MegatronBertConfig'> for this kind of AutoModel: TFAutoModel.\nModel type should be one of AlbertConfig, BartConfig, BertConfig, BlenderbotConfig, BlenderbotSmallConfig, BlipConfig, CamembertConfig, CLIPConfig, ConvBertConfig, ConvNextConfig, ConvNextV2Config, CTRLConfig, CvtConfig, Data2VecVisionConfig, DebertaConfig, DebertaV2Config, DeiTConfig, DistilBertConfig, DPRConfig, EfficientFormerConfig, ElectraConfig, EsmConfig, FlaubertConfig, FunnelConfig, GPT2Config, GPT2Config, GPTJConfig, GroupViTConfig, HubertConfig, IdeficsConfig, LayoutLMConfig, LayoutLMv3Config, LEDConfig, LongformerConfig, LxmertConfig, MarianConfig, MBartConfig, MistralConfig, MobileBertConfig, MobileViTConfig, MPNetConfig, MT5Config, OpenAIGPTConfig, OPTConfig, PegasusConfig, RegNetConfig, RemBertConfig, ResNetConfig, RobertaConfig, RobertaPreLayerNormConfig, RoFormerConfig, SamConfig, SegformerConfig, Speech2TextConfig, SwiftFormerConfig, SwinConfig, T5Config, TapasConfig, TransfoXLConfig, VisionTextDualEncoderConfig, ViTConfig, ViTMAEConfig, Wav2Vec2Config, WhisperConfig, XGLMConfig, XLMConfig, XLMRobertaConfig, XLNetConfig."
            },
            "tensorflow_xla": {
                "error": "Unrecognized configuration class <class 'transformers.models.megatron_bert.configuration_megatron_bert.MegatronBertConfig'> for this kind of AutoModel: TFAutoModel.\nModel type should be one of AlbertConfig, BartConfig, BertConfig, BlenderbotConfig, BlenderbotSmallConfig, BlipConfig, CamembertConfig, CLIPConfig, ConvBertConfig, ConvNextConfig, ConvNextV2Config, CTRLConfig, CvtConfig, Data2VecVisionConfig, DebertaConfig, DebertaV2Config, DeiTConfig, DistilBertConfig, DPRConfig, EfficientFormerConfig, ElectraConfig, EsmConfig, FlaubertConfig, FunnelConfig, GPT2Config, GPT2Config, GPTJConfig, GroupViTConfig, HubertConfig, IdeficsConfig, LayoutLMConfig, LayoutLMv3Config, LEDConfig, LongformerConfig, LxmertConfig, MarianConfig, MBartConfig, MistralConfig, MobileBertConfig, MobileViTConfig, MPNetConfig, MT5Config, OpenAIGPTConfig, OPTConfig, PegasusConfig, RegNetConfig, RemBertConfig, ResNetConfig, RobertaConfig, RobertaPreLayerNormConfig, RoFormerConfig, SamConfig, SegformerConfig, Speech2TextConfig, SwiftFormerConfig, SwinConfig, T5Config, TapasConfig, TransfoXLConfig, VisionTextDualEncoderConfig, ViTConfig, ViTMAEConfig, Wav2Vec2Config, WhisperConfig, XGLMConfig, XLMConfig, XLMRobertaConfig, XLNetConfig."
            }
        }
    },
    {
        "MobileBertForMaskedLM": {
            "pytorch_torch_compile_inductor": {
                "inference_s": 0.04917330265045166,
                "training_s": 0.13390392541885376
            },
            "pytorch_torch_compile_eager": {
                "inference_s": 0.05417840003967285,
                "training_s": 0.1805603051185608
            },
            "pytorch_torch_compile_cudagraphs": {
                "inference_s": 0.14286931991577148,
                "training_s": 0.27750051975250245
            },
            "pytorch_torch_compile_onnxrt": {
                "error": "backend='onnxrt' raised:\nAssertionError: Please convert all Tensors to FakeTensors first or instantiate FakeTensorMode with 'allow_non_fake_inputs'. Found in aten.alias.default(tensor(1000, size=()))\n\nWhile executing %tensor : [num_users=0] = call_function[target=torch.tensor](args = (1000,), kwargs = {})\nOriginal traceback:\n  File \"/opt/conda/envs/xla-env/lib/python3.10/site-packages/transformers/models/mobilebert/modeling_mobilebert.py\", line 1091, in forward\n    outputs = self.mobilebert(\n  File \"/opt/conda/envs/xla-env/lib/python3.10/site-packages/transformers/models/mobilebert/modeling_mobilebert.py\", line 898, in forward\n    encoder_outputs = self.encoder(\n  File \"/opt/conda/envs/xla-env/lib/python3.10/site-packages/transformers/models/mobilebert/modeling_mobilebert.py\", line 580, in forward\n    layer_outputs = layer_module(\n  File \"/opt/conda/envs/xla-env/lib/python3.10/site-packages/transformers/models/mobilebert/modeling_mobilebert.py\", line 547, in forward\n    torch.tensor(1000),\n\n\nSet TORCH_LOGS=\"+dynamo\" and TORCHDYNAMO_VERBOSE=1 for more information\n\n\nYou can suppress this exception and fall back to eager by setting:\n    import torch._dynamo\n    torch._dynamo.config.suppress_errors = True\n"
            },
            "pytorch_torch_compile_openxla": {
                "error": "torch_xla/csrc/runtime/pjrt_registry.cc:214 : Check failed: client \n*** Begin stack trace ***\n\ttsl::CurrentStackTrace()\n\ttorch_xla::runtime::InitializePjRt(std::string const&)\n\ttorch_xla::runtime::PjRtComputationClient::PjRtComputationClient()\n\t\n\ttorch_xla::runtime::GetComputationClient()\n\ttorch_xla::bridge::GetDefaultDevice()\n\ttorch_xla::bridge::GetCurrentDevice()\n\ttorch_xla::bridge::GetCurrentAtenDevice()\n\t\n\t\n\t\n\t_PyObject_MakeTpCall\n\t_PyEval_EvalFrameDefault\n\t\n\t_PyFunction_Vectorcall\n\t_PyEval_EvalFrameDefault\n\t\n\t_PyFunction_Vectorcall\n\t_PyEval_EvalFrameDefault\n\t\n\t_PyFunction_Vectorcall\n\t_PyEval_EvalFrameDefault\n\t\n\t_PyFunction_Vectorcall\n\t_PyEval_EvalFrameDefault\n\t\n\t_PyFunction_Vectorcall\n\t_PyEval_EvalFrameDefault\n\t\n\t_PyFunction_Vectorcall\n\t_PyEval_EvalFrameDefault\n\t\n\t_PyFunction_Vectorcall\n\t_PyEval_EvalFrameDefault\n\t\n\t_PyFunction_Vectorcall\n\t_PyEval_EvalFrameDefault\n\t\n\t_PyFunction_Vectorcall\n\tTHPFunction_apply(_object*, _object*)\n\t\n\tPyObject_Call\n\t_PyEval_EvalFrameDefault\n\t\n\t\n\t_PyEval_EvalFrameDefault\n\t\n\t_PyFunction_Vectorcall\n\t_PyEval_EvalFrameDefault\n\t\n\t_PyFunction_Vectorcall\n\t_PyEval_EvalFrameDefault\n\t\n\t_PyFunction_Vectorcall\n\t_PyEval_EvalFrameDefault\n\t\n\t_PyFunction_Vectorcall\n\t_PyEval_EvalFrameDefault\n\t\n\t_PyFunction_Vectorcall\n\t_PyEval_EvalFrameDefault\n\t\n\t\n\t\n\tPyObject_Call\n\t_PyEval_EvalFrameDefault\n\t\n\t\n\tPyObject_Call\n\t_PyEval_EvalFrameDefault\n\t\n\t\n\tPyObject_Call\n\t_PyEval_EvalFrameDefault\n\t\n\t_PyFunction_Vectorcall\n\tPyObject_Call\n\t_PyEval_EvalFrameDefault\n\t\n\t\n\tPyObject_Call\n\t_PyEval_EvalFrameDefault\n\t\n\t_PyFunction_Vectorcall\n\t_PyObject_FastCallDictTstate\n\t_PyObject_Call_Prepend\n\t\n\tPyObject_Call\n\t_PyEval_EvalFrameDefault\n\t\n\t_PyFunction_Vectorcall\n\t_PyEval_EvalFrameDefault\n\t_PyFunction_Vectorcall\n\t_PyEval_EvalFrameDefault\n\t\n\tPyEval_EvalCode\n\t\n\t\n\t\n\t_PyRun_SimpleFileObject\n\t_PyRun_AnyFileObject\n\tPy_RunMain\n\tPy_BytesMain\n\t__libc_start_main\n\t\n*** End stack trace ***\nUnknown PJRT_DEVICE 'CUDA'"
            },
            "pytorch_torch_compile_tvm": {
                "inference_s": 0.06403514146804809,
                "training_s": 0.18799235582351684
            },
            "jax_flax_xla": {
                "error": "Unrecognized configuration class <class 'transformers.models.mobilebert.configuration_mobilebert.MobileBertConfig'> for this kind of AutoModel: FlaxAutoModel.\nModel type should be one of AlbertConfig, BartConfig, BeitConfig, BertConfig, BigBirdConfig, BlenderbotConfig, BlenderbotSmallConfig, BloomConfig, CLIPConfig, Dinov2Config, DistilBertConfig, ElectraConfig, GemmaConfig, GPT2Config, GPT2Config, GPTNeoConfig, GPTJConfig, LlamaConfig, LongT5Config, MarianConfig, MBartConfig, MistralConfig, MT5Config, OPTConfig, PegasusConfig, RegNetConfig, ResNetConfig, RobertaConfig, RobertaPreLayerNormConfig, RoFormerConfig, T5Config, VisionTextDualEncoderConfig, ViTConfig, Wav2Vec2Config, WhisperConfig, XGLMConfig, XLMRobertaConfig."
            },
            "tensorflow_eager": {
                "inference_s": 0.6880912613868714,
                "training_s": 41.8042470574379
            },
            "tensorflow_xla": {
                "inference_s": 0.008297488689422608,
                "training_s": 0.030272259712219238
            }
        }
    },
    {
        "MobileBertForQuestionAnswering": {
            "pytorch_torch_compile_inductor": {
                "inference_s": 0.03745289087295532,
                "training_s": 0.11164246082305908
            },
            "pytorch_torch_compile_eager": {
                "inference_s": 0.0378567910194397,
                "training_s": 0.14260390758514405
            },
            "pytorch_torch_compile_cudagraphs": {
                "inference_s": 0.12879945516586302,
                "training_s": 0.2490171480178833
            },
            "pytorch_torch_compile_onnxrt": {
                "error": "backend='onnxrt' raised:\nAssertionError: Please convert all Tensors to FakeTensors first or instantiate FakeTensorMode with 'allow_non_fake_inputs'. Found in aten.alias.default(tensor(1000, size=()))\n\nWhile executing %tensor : [num_users=0] = call_function[target=torch.tensor](args = (1000,), kwargs = {})\nOriginal traceback:\n  File \"/opt/conda/envs/xla-env/lib/python3.10/site-packages/transformers/models/mobilebert/modeling_mobilebert.py\", line 1392, in forward\n    outputs = self.mobilebert(\n  File \"/opt/conda/envs/xla-env/lib/python3.10/site-packages/transformers/models/mobilebert/modeling_mobilebert.py\", line 898, in forward\n    encoder_outputs = self.encoder(\n  File \"/opt/conda/envs/xla-env/lib/python3.10/site-packages/transformers/models/mobilebert/modeling_mobilebert.py\", line 580, in forward\n    layer_outputs = layer_module(\n  File \"/opt/conda/envs/xla-env/lib/python3.10/site-packages/transformers/models/mobilebert/modeling_mobilebert.py\", line 547, in forward\n    torch.tensor(1000),\n\n\nSet TORCH_LOGS=\"+dynamo\" and TORCHDYNAMO_VERBOSE=1 for more information\n\n\nYou can suppress this exception and fall back to eager by setting:\n    import torch._dynamo\n    torch._dynamo.config.suppress_errors = True\n"
            },
            "pytorch_torch_compile_openxla": {
                "error": "torch_xla/csrc/runtime/pjrt_registry.cc:214 : Check failed: client \n*** Begin stack trace ***\n\ttsl::CurrentStackTrace()\n\ttorch_xla::runtime::InitializePjRt(std::string const&)\n\ttorch_xla::runtime::PjRtComputationClient::PjRtComputationClient()\n\t\n\ttorch_xla::runtime::GetComputationClient()\n\ttorch_xla::bridge::GetDefaultDevice()\n\ttorch_xla::bridge::GetCurrentDevice()\n\ttorch_xla::bridge::GetCurrentAtenDevice()\n\t\n\t\n\t\n\t_PyObject_MakeTpCall\n\t_PyEval_EvalFrameDefault\n\t\n\t_PyFunction_Vectorcall\n\t_PyEval_EvalFrameDefault\n\t\n\t_PyFunction_Vectorcall\n\t_PyEval_EvalFrameDefault\n\t\n\t_PyFunction_Vectorcall\n\t_PyEval_EvalFrameDefault\n\t\n\t_PyFunction_Vectorcall\n\t_PyEval_EvalFrameDefault\n\t\n\t_PyFunction_Vectorcall\n\t_PyEval_EvalFrameDefault\n\t\n\t_PyFunction_Vectorcall\n\t_PyEval_EvalFrameDefault\n\t\n\t_PyFunction_Vectorcall\n\t_PyEval_EvalFrameDefault\n\t\n\t_PyFunction_Vectorcall\n\t_PyEval_EvalFrameDefault\n\t\n\t_PyFunction_Vectorcall\n\tTHPFunction_apply(_object*, _object*)\n\t\n\tPyObject_Call\n\t_PyEval_EvalFrameDefault\n\t\n\t\n\t_PyEval_EvalFrameDefault\n\t\n\t_PyFunction_Vectorcall\n\t_PyEval_EvalFrameDefault\n\t\n\t_PyFunction_Vectorcall\n\t_PyEval_EvalFrameDefault\n\t\n\t_PyFunction_Vectorcall\n\t_PyEval_EvalFrameDefault\n\t\n\t_PyFunction_Vectorcall\n\t_PyEval_EvalFrameDefault\n\t\n\t_PyFunction_Vectorcall\n\t_PyEval_EvalFrameDefault\n\t\n\t\n\t\n\tPyObject_Call\n\t_PyEval_EvalFrameDefault\n\t\n\t\n\tPyObject_Call\n\t_PyEval_EvalFrameDefault\n\t\n\t\n\tPyObject_Call\n\t_PyEval_EvalFrameDefault\n\t\n\t_PyFunction_Vectorcall\n\tPyObject_Call\n\t_PyEval_EvalFrameDefault\n\t\n\t\n\tPyObject_Call\n\t_PyEval_EvalFrameDefault\n\t\n\t_PyFunction_Vectorcall\n\t_PyObject_FastCallDictTstate\n\t_PyObject_Call_Prepend\n\t\n\tPyObject_Call\n\t_PyEval_EvalFrameDefault\n\t\n\t_PyFunction_Vectorcall\n\t_PyEval_EvalFrameDefault\n\t_PyFunction_Vectorcall\n\t_PyEval_EvalFrameDefault\n\t\n\tPyEval_EvalCode\n\t\n\t\n\t\n\t_PyRun_SimpleFileObject\n\t_PyRun_AnyFileObject\n\tPy_RunMain\n\tPy_BytesMain\n\t__libc_start_main\n\t\n*** End stack trace ***\nUnknown PJRT_DEVICE 'CUDA'"
            },
            "pytorch_torch_compile_tvm": {
                "inference_s": 0.050071516036987306,
                "training_s": 0.1549700403213501
            },
            "jax_flax_xla": {
                "error": "Unrecognized configuration class <class 'transformers.models.mobilebert.configuration_mobilebert.MobileBertConfig'> for this kind of AutoModel: FlaxAutoModel.\nModel type should be one of AlbertConfig, BartConfig, BeitConfig, BertConfig, BigBirdConfig, BlenderbotConfig, BlenderbotSmallConfig, BloomConfig, CLIPConfig, Dinov2Config, DistilBertConfig, ElectraConfig, GemmaConfig, GPT2Config, GPT2Config, GPTNeoConfig, GPTJConfig, LlamaConfig, LongT5Config, MarianConfig, MBartConfig, MistralConfig, MT5Config, OPTConfig, PegasusConfig, RegNetConfig, ResNetConfig, RobertaConfig, RobertaPreLayerNormConfig, RoFormerConfig, T5Config, VisionTextDualEncoderConfig, ViTConfig, Wav2Vec2Config, WhisperConfig, XGLMConfig, XLMRobertaConfig."
            },
            "tensorflow_eager": {
                "inference_s": 0.6397053837776184,
                "training_s": 41.58196019411087
            },
            "tensorflow_xla": {
                "inference_s": 0.008844094276428223,
                "training_s": 0.03232104539871216
            }
        }
    },
    {
        "MT5ForConditionalGeneration": {
            "pytorch_torch_compile_inductor": {
                "inference_s": 0.09638539552688599,
                "training_s": 0.5657290363311768
            },
            "pytorch_torch_compile_eager": {
                "inference_s": 0.11396507024765015,
                "training_s": 0.6166904783248901
            },
            "pytorch_torch_compile_cudagraphs": {
                "inference_s": 0.2027975845336914,
                "training_s": 0.7169177031517029
            },
            "pytorch_torch_compile_onnxrt": {
                "inference_s": 1.4761250972747804,
                "training_s": 3.456305775642395
            },
            "pytorch_torch_compile_openxla": {
                "inference_s": 0.12309100389480591,
                "training_s": 0.6485016417503356
            },
            "pytorch_torch_compile_tvm": {
                "inference_s": 0.12728022813796996,
                "training_s": 0.6476497077941894
            },
            "jax_flax_xla": {
                "error": "Make sure to provide both `input_ids` and `decoder_input_ids`. `decoder_input_ids` is not passed here."
            },
            "tensorflow_eager": {
                "error": "Exception encountered when calling layer 'decoder' (type TFT5MainLayer).\n\nYou have to specify either decoder_input_ids or decoder_inputs_embeds\n\nCall arguments received by layer 'decoder' (type TFT5MainLayer):\n  \u2022 input_ids=None\n  \u2022 attention_mask=None\n  \u2022 encoder_hidden_states=tf.Tensor(shape=(1, 32, 768), dtype=float32)\n  \u2022 encoder_attention_mask=tf.Tensor(shape=(1, 32), dtype=int32)\n  \u2022 inputs_embeds=None\n  \u2022 head_mask=None\n  \u2022 encoder_head_mask=None\n  \u2022 past_key_values=None\n  \u2022 use_cache=True\n  \u2022 output_attentions=False\n  \u2022 output_hidden_states=False\n  \u2022 return_dict=True\n  \u2022 training=False"
            },
            "tensorflow_xla": {
                "error": "in user code:\n\n    File \"/home/mmoffatt/jax_vs_pytorch/benchmark.py\", line 189, in fwd  *\n        return model(**inp)[0]\n    File \"/opt/conda/envs/xla-env/lib/python3.10/site-packages/tf_keras/src/utils/traceback_utils.py\", line 70, in error_handler  **\n        raise e.with_traceback(filtered_tb) from None\n    File \"/var/tmp/__autograph_generated_fileu7kf6q6f.py\", line 37, in tf__run_call_with_unpacked_inputs\n        retval_ = ag__.converted_call(ag__.ld(func), (ag__.ld(self),), dict(**ag__.ld(unpacked_inputs)), fscope)\n    File \"/var/tmp/__autograph_generated_file1ttgrc2j.py\", line 91, in tf__call\n        decoder_outputs = ag__.converted_call(ag__.ld(self).decoder, (ag__.ld(decoder_input_ids),), dict(attention_mask=ag__.ld(decoder_attention_mask), encoder_hidden_states=ag__.ld(hidden_states), encoder_attention_mask=ag__.ld(attention_mask), inputs_embeds=ag__.ld(decoder_inputs_embeds), head_mask=ag__.ld(decoder_head_mask), past_key_values=ag__.ld(past_key_values), use_cache=ag__.ld(use_cache), output_attentions=ag__.ld(output_attentions), output_hidden_states=ag__.ld(output_hidden_states), return_dict=ag__.ld(return_dict), training=ag__.ld(training)), fscope)\n    File \"/var/tmp/__autograph_generated_fileu7kf6q6f.py\", line 37, in tf__run_call_with_unpacked_inputs\n        retval_ = ag__.converted_call(ag__.ld(func), (ag__.ld(self),), dict(**ag__.ld(unpacked_inputs)), fscope)\n    File \"/var/tmp/__autograph_generated_filekrol4awi.py\", line 65, in tf__call\n        ag__.if_stmt(ag__.and_(lambda : ag__.ld(input_ids) is not None, lambda : ag__.ld(inputs_embeds) is not None), if_body_2, else_body_2, get_state_2, set_state_2, ('input_ids', 'input_shape'), 2)\n    File \"/var/tmp/__autograph_generated_filekrol4awi.py\", line 62, in else_body_2\n        ag__.if_stmt(ag__.ld(input_ids) is not None, if_body_1, else_body_1, get_state_1, set_state_1, ('input_ids', 'input_shape'), 2)\n    File \"/var/tmp/__autograph_generated_filekrol4awi.py\", line 59, in else_body_1\n        ag__.if_stmt(ag__.ld(inputs_embeds) is not None, if_body, else_body, get_state, set_state, ('input_shape',), 1)\n    File \"/var/tmp/__autograph_generated_filekrol4awi.py\", line 56, in else_body\n        raise ag__.converted_call(ag__.ld(ValueError), (f'You have to specify either {ag__.ld(err_msg_prefix)}input_ids or {ag__.ld(err_msg_prefix)}inputs_embeds',), None, fscope)\n\n    ValueError: Exception encountered when calling layer 'tfmt5_for_conditional_generation_1' (type TFMT5ForConditionalGeneration).\n    \n    in user code:\n    \n        File \"/opt/conda/envs/xla-env/lib/python3.10/site-packages/transformers/modeling_tf_utils.py\", line 1372, in run_call_with_unpacked_inputs  *\n            return func(self, **unpacked_inputs)\n        File \"/opt/conda/envs/xla-env/lib/python3.10/site-packages/transformers/models/t5/modeling_tf_t5.py\", line 1455, in call  *\n            decoder_outputs = self.decoder(\n        File \"/opt/conda/envs/xla-env/lib/python3.10/site-packages/tf_keras/src/utils/traceback_utils.py\", line 70, in error_handler  **\n            raise e.with_traceback(filtered_tb) from None\n        File \"/var/tmp/__autograph_generated_fileu7kf6q6f.py\", line 37, in tf__run_call_with_unpacked_inputs\n            retval_ = ag__.converted_call(ag__.ld(func), (ag__.ld(self),), dict(**ag__.ld(unpacked_inputs)), fscope)\n        File \"/var/tmp/__autograph_generated_filekrol4awi.py\", line 65, in tf__call\n            ag__.if_stmt(ag__.and_(lambda : ag__.ld(input_ids) is not None, lambda : ag__.ld(inputs_embeds) is not None), if_body_2, else_body_2, get_state_2, set_state_2, ('input_ids', 'input_shape'), 2)\n        File \"/var/tmp/__autograph_generated_filekrol4awi.py\", line 62, in else_body_2\n            ag__.if_stmt(ag__.ld(input_ids) is not None, if_body_1, else_body_1, get_state_1, set_state_1, ('input_ids', 'input_shape'), 2)\n        File \"/var/tmp/__autograph_generated_filekrol4awi.py\", line 59, in else_body_1\n            ag__.if_stmt(ag__.ld(inputs_embeds) is not None, if_body, else_body, get_state, set_state, ('input_shape',), 1)\n        File \"/var/tmp/__autograph_generated_filekrol4awi.py\", line 56, in else_body\n            raise ag__.converted_call(ag__.ld(ValueError), (f'You have to specify either {ag__.ld(err_msg_prefix)}input_ids or {ag__.ld(err_msg_prefix)}inputs_embeds',), None, fscope)\n    \n        ValueError: Exception encountered when calling layer 'decoder' (type TFT5MainLayer).\n        \n        in user code:\n        \n            File \"/opt/conda/envs/xla-env/lib/python3.10/site-packages/transformers/modeling_tf_utils.py\", line 1372, in run_call_with_unpacked_inputs  *\n                return func(self, **unpacked_inputs)\n            File \"/opt/conda/envs/xla-env/lib/python3.10/site-packages/transformers/models/t5/modeling_tf_t5.py\", line 754, in call  *\n                raise ValueError(f\"You have to specify either {err_msg_prefix}input_ids or {err_msg_prefix}inputs_embeds\")\n        \n            ValueError: You have to specify either decoder_input_ids or decoder_inputs_embeds\n        \n        \n        Call arguments received by layer 'decoder' (type TFT5MainLayer):\n          \u2022 input_ids=None\n          \u2022 attention_mask=None\n          \u2022 encoder_hidden_states=tf.Tensor(shape=(1, 32, 768), dtype=float32)\n          \u2022 encoder_attention_mask=tf.Tensor(shape=(1, 32), dtype=int32)\n          \u2022 inputs_embeds=None\n          \u2022 head_mask=None\n          \u2022 encoder_head_mask=None\n          \u2022 past_key_values=None\n          \u2022 use_cache=True\n          \u2022 output_attentions=False\n          \u2022 output_hidden_states=False\n          \u2022 return_dict=True\n          \u2022 training=False\n    \n    \n    Call arguments received by layer 'tfmt5_for_conditional_generation_1' (type TFMT5ForConditionalGeneration):\n      \u2022 input_ids=tf.Tensor(shape=(1, 32), dtype=int32)\n      \u2022 attention_mask=tf.Tensor(shape=(1, 32), dtype=int32)\n      \u2022 decoder_input_ids=None\n      \u2022 decoder_attention_mask=None\n      \u2022 head_mask=None\n      \u2022 decoder_head_mask=None\n      \u2022 encoder_outputs=None\n      \u2022 past_key_values=None\n      \u2022 inputs_embeds=None\n      \u2022 decoder_inputs_embeds=None\n      \u2022 labels=None\n      \u2022 use_cache=None\n      \u2022 output_attentions=None\n      \u2022 output_hidden_states=None\n      \u2022 return_dict=None\n      \u2022 training=False\n"
            }
        }
    },
    {
        "OPTForCausalLM": {
            "error": "OSError('facebook/opt-125m does not appear to have a file named pytorch_model.bin but there is a file for TensorFlow weights. Use `from_tf=True` to load this model from those weights.')",
            "jax_flax_xla": {
                "error": "FlaxOPTPreTrainedModel.__call__() got an unexpected keyword argument 'train'"
            },
            "tensorflow_eager": {
                "inference_s": 0.2104659152030945,
                "training_s": 0.5347495222091675
            },
            "tensorflow_xla": {
                "inference_s": 0.0024450016021728515,
                "training_s": 0.005969657897949219
            }
        }
    },
    {
        "PegasusForCausalLM": {
            "pytorch_torch_compile_inductor": {
                "inference_s": 0.07337441444396972,
                "training_s": 0.3545255184173584
            },
            "pytorch_torch_compile_eager": {
                "inference_s": 0.08066541194915772,
                "training_s": 0.39761847019195556
            },
            "pytorch_torch_compile_cudagraphs": {
                "inference_s": 0.12721861362457276,
                "training_s": 0.4523041391372681
            },
            "pytorch_torch_compile_onnxrt": {
                "inference_s": 0.36880207777023316,
                "training_s": 0.6502600407600403
            },
            "pytorch_torch_compile_openxla": {
                "inference_s": 0.07963517427444458,
                "training_s": 0.4731329870223999
            },
            "pytorch_torch_compile_tvm": {
                "inference_s": 0.08888456821441651,
                "training_s": 0.4659328675270081
            },
            "jax_flax_xla": {
                "inference_s": 0.002892282009124756,
                "training_s": 0.01327815294265747
            },
            "tensorflow_eager": {
                "error": "Exception encountered when calling layer 'decoder' (type TFPegasusDecoder).\n\nYou have to specify either decoder_input_ids or decoder_inputs_embeds\n\nCall arguments received by layer 'decoder' (type TFPegasusDecoder):\n  \u2022 input_ids=None\n  \u2022 inputs_embeds=None\n  \u2022 attention_mask=None\n  \u2022 position_ids=None\n  \u2022 encoder_hidden_states=tf.Tensor(shape=(1, 32, 1024), dtype=float32)\n  \u2022 encoder_attention_mask=tf.Tensor(shape=(1, 32), dtype=int32)\n  \u2022 head_mask=None\n  \u2022 cross_attn_head_mask=None\n  \u2022 past_key_values=None\n  \u2022 use_cache=False\n  \u2022 output_attentions=False\n  \u2022 output_hidden_states=False\n  \u2022 return_dict=True\n  \u2022 training=False"
            },
            "tensorflow_xla": {
                "error": "in user code:\n\n    File \"/home/mmoffatt/jax_vs_pytorch/benchmark.py\", line 189, in fwd  *\n        return model(**inp)[0]\n    File \"/opt/conda/envs/xla-env/lib/python3.10/site-packages/tf_keras/src/utils/traceback_utils.py\", line 70, in error_handler  **\n        raise e.with_traceback(filtered_tb) from None\n    File \"/var/tmp/__autograph_generated_fileu7kf6q6f.py\", line 37, in tf__run_call_with_unpacked_inputs\n        retval_ = ag__.converted_call(ag__.ld(func), (ag__.ld(self),), dict(**ag__.ld(unpacked_inputs)), fscope)\n    File \"/var/tmp/__autograph_generated_file8sxgf1gf.py\", line 11, in tf__call\n        outputs = ag__.converted_call(ag__.ld(self).model, (), dict(input_ids=ag__.ld(input_ids), attention_mask=ag__.ld(attention_mask), decoder_input_ids=ag__.ld(decoder_input_ids), decoder_attention_mask=ag__.ld(decoder_attention_mask), decoder_position_ids=ag__.ld(decoder_position_ids), head_mask=ag__.ld(head_mask), decoder_head_mask=ag__.ld(decoder_head_mask), cross_attn_head_mask=ag__.ld(cross_attn_head_mask), encoder_outputs=ag__.ld(encoder_outputs), past_key_values=ag__.ld(past_key_values), inputs_embeds=ag__.ld(inputs_embeds), decoder_inputs_embeds=ag__.ld(decoder_inputs_embeds), use_cache=ag__.ld(use_cache), output_attentions=ag__.ld(output_attentions), output_hidden_states=ag__.ld(output_hidden_states), return_dict=ag__.ld(return_dict), training=ag__.ld(training)), fscope)\n    File \"/var/tmp/__autograph_generated_fileu7kf6q6f.py\", line 37, in tf__run_call_with_unpacked_inputs\n        retval_ = ag__.converted_call(ag__.ld(func), (ag__.ld(self),), dict(**ag__.ld(unpacked_inputs)), fscope)\n    File \"/var/tmp/__autograph_generated_filevjsqqekg.py\", line 74, in tf__call\n        decoder_outputs = ag__.converted_call(ag__.ld(self).decoder, (ag__.ld(decoder_input_ids),), dict(attention_mask=ag__.ld(decoder_attention_mask), position_ids=ag__.ld(decoder_position_ids), encoder_hidden_states=ag__.ld(encoder_outputs)[0], encoder_attention_mask=ag__.ld(attention_mask), head_mask=ag__.ld(decoder_head_mask), cross_attn_head_mask=ag__.ld(cross_attn_head_mask), past_key_values=ag__.ld(past_key_values), inputs_embeds=ag__.ld(decoder_inputs_embeds), use_cache=ag__.ld(use_cache), output_attentions=ag__.ld(output_attentions), output_hidden_states=ag__.ld(output_hidden_states), return_dict=ag__.ld(return_dict), training=ag__.ld(training)), fscope)\n    File \"/var/tmp/__autograph_generated_fileu7kf6q6f.py\", line 37, in tf__run_call_with_unpacked_inputs\n        retval_ = ag__.converted_call(ag__.ld(func), (ag__.ld(self),), dict(**ag__.ld(unpacked_inputs)), fscope)\n    File \"/var/tmp/__autograph_generated_file405tf2x8.py\", line 128, in tf__call\n        ag__.if_stmt(ag__.and_(lambda : ag__.ld(input_ids) is not None, lambda : ag__.ld(inputs_embeds) is not None), if_body_2, else_body_2, get_state_2, set_state_2, ('input_shape',), 1)\n    File \"/var/tmp/__autograph_generated_file405tf2x8.py\", line 126, in else_body_2\n        ag__.if_stmt(ag__.ld(input_ids) is not None, if_body_1, else_body_1, get_state_1, set_state_1, ('input_shape',), 1)\n    File \"/var/tmp/__autograph_generated_file405tf2x8.py\", line 124, in else_body_1\n        ag__.if_stmt(ag__.ld(inputs_embeds) is not None, if_body, else_body, get_state, set_state, ('input_shape',), 1)\n    File \"/var/tmp/__autograph_generated_file405tf2x8.py\", line 122, in else_body\n        raise ag__.converted_call(ag__.ld(ValueError), ('You have to specify either decoder_input_ids or decoder_inputs_embeds',), None, fscope)\n\n    ValueError: Exception encountered when calling layer 'tf_pegasus_model_1' (type TFPegasusModel).\n    \n    in user code:\n    \n        File \"/opt/conda/envs/xla-env/lib/python3.10/site-packages/transformers/modeling_tf_utils.py\", line 1372, in run_call_with_unpacked_inputs  *\n            return func(self, **unpacked_inputs)\n        File \"/opt/conda/envs/xla-env/lib/python3.10/site-packages/transformers/models/pegasus/modeling_tf_pegasus.py\", line 1304, in call  *\n            outputs = self.model(\n        File \"/opt/conda/envs/xla-env/lib/python3.10/site-packages/tf_keras/src/utils/traceback_utils.py\", line 70, in error_handler  **\n            raise e.with_traceback(filtered_tb) from None\n        File \"/var/tmp/__autograph_generated_fileu7kf6q6f.py\", line 37, in tf__run_call_with_unpacked_inputs\n            retval_ = ag__.converted_call(ag__.ld(func), (ag__.ld(self),), dict(**ag__.ld(unpacked_inputs)), fscope)\n        File \"/var/tmp/__autograph_generated_filevjsqqekg.py\", line 74, in tf__call\n            decoder_outputs = ag__.converted_call(ag__.ld(self).decoder, (ag__.ld(decoder_input_ids),), dict(attention_mask=ag__.ld(decoder_attention_mask), position_ids=ag__.ld(decoder_position_ids), encoder_hidden_states=ag__.ld(encoder_outputs)[0], encoder_attention_mask=ag__.ld(attention_mask), head_mask=ag__.ld(decoder_head_mask), cross_attn_head_mask=ag__.ld(cross_attn_head_mask), past_key_values=ag__.ld(past_key_values), inputs_embeds=ag__.ld(decoder_inputs_embeds), use_cache=ag__.ld(use_cache), output_attentions=ag__.ld(output_attentions), output_hidden_states=ag__.ld(output_hidden_states), return_dict=ag__.ld(return_dict), training=ag__.ld(training)), fscope)\n        File \"/var/tmp/__autograph_generated_fileu7kf6q6f.py\", line 37, in tf__run_call_with_unpacked_inputs\n            retval_ = ag__.converted_call(ag__.ld(func), (ag__.ld(self),), dict(**ag__.ld(unpacked_inputs)), fscope)\n        File \"/var/tmp/__autograph_generated_file405tf2x8.py\", line 128, in tf__call\n            ag__.if_stmt(ag__.and_(lambda : ag__.ld(input_ids) is not None, lambda : ag__.ld(inputs_embeds) is not None), if_body_2, else_body_2, get_state_2, set_state_2, ('input_shape',), 1)\n        File \"/var/tmp/__autograph_generated_file405tf2x8.py\", line 126, in else_body_2\n            ag__.if_stmt(ag__.ld(input_ids) is not None, if_body_1, else_body_1, get_state_1, set_state_1, ('input_shape',), 1)\n        File \"/var/tmp/__autograph_generated_file405tf2x8.py\", line 124, in else_body_1\n            ag__.if_stmt(ag__.ld(inputs_embeds) is not None, if_body, else_body, get_state, set_state, ('input_shape',), 1)\n        File \"/var/tmp/__autograph_generated_file405tf2x8.py\", line 122, in else_body\n            raise ag__.converted_call(ag__.ld(ValueError), ('You have to specify either decoder_input_ids or decoder_inputs_embeds',), None, fscope)\n    \n        ValueError: Exception encountered when calling layer 'model' (type TFPegasusMainLayer).\n        \n        in user code:\n        \n            File \"/opt/conda/envs/xla-env/lib/python3.10/site-packages/transformers/modeling_tf_utils.py\", line 1372, in run_call_with_unpacked_inputs  *\n                return func(self, **unpacked_inputs)\n            File \"/opt/conda/envs/xla-env/lib/python3.10/site-packages/transformers/models/pegasus/modeling_tf_pegasus.py\", line 1212, in call  *\n                decoder_outputs = self.decoder(\n            File \"/opt/conda/envs/xla-env/lib/python3.10/site-packages/tf_keras/src/utils/traceback_utils.py\", line 70, in error_handler  **\n                raise e.with_traceback(filtered_tb) from None\n            File \"/var/tmp/__autograph_generated_fileu7kf6q6f.py\", line 37, in tf__run_call_with_unpacked_inputs\n                retval_ = ag__.converted_call(ag__.ld(func), (ag__.ld(self),), dict(**ag__.ld(unpacked_inputs)), fscope)\n            File \"/var/tmp/__autograph_generated_file405tf2x8.py\", line 128, in tf__call\n                ag__.if_stmt(ag__.and_(lambda : ag__.ld(input_ids) is not None, lambda : ag__.ld(inputs_embeds) is not None), if_body_2, else_body_2, get_state_2, set_state_2, ('input_shape',), 1)\n            File \"/var/tmp/__autograph_generated_file405tf2x8.py\", line 126, in else_body_2\n                ag__.if_stmt(ag__.ld(input_ids) is not None, if_body_1, else_body_1, get_state_1, set_state_1, ('input_shape',), 1)\n            File \"/var/tmp/__autograph_generated_file405tf2x8.py\", line 124, in else_body_1\n                ag__.if_stmt(ag__.ld(inputs_embeds) is not None, if_body, else_body, get_state, set_state, ('input_shape',), 1)\n            File \"/var/tmp/__autograph_generated_file405tf2x8.py\", line 122, in else_body\n                raise ag__.converted_call(ag__.ld(ValueError), ('You have to specify either decoder_input_ids or decoder_inputs_embeds',), None, fscope)\n        \n            ValueError: Exception encountered when calling layer 'decoder' (type TFPegasusDecoder).\n            \n            in user code:\n            \n                File \"/opt/conda/envs/xla-env/lib/python3.10/site-packages/transformers/modeling_tf_utils.py\", line 1372, in run_call_with_unpacked_inputs  *\n                    return func(self, **unpacked_inputs)\n                File \"/opt/conda/envs/xla-env/lib/python3.10/site-packages/transformers/models/pegasus/modeling_tf_pegasus.py\", line 1020, in call  *\n                    raise ValueError(\"You have to specify either decoder_input_ids or decoder_inputs_embeds\")\n            \n                ValueError: You have to specify either decoder_input_ids or decoder_inputs_embeds\n            \n            \n            Call arguments received by layer 'decoder' (type TFPegasusDecoder):\n              \u2022 input_ids=None\n              \u2022 inputs_embeds=None\n              \u2022 attention_mask=None\n              \u2022 position_ids=None\n              \u2022 encoder_hidden_states=tf.Tensor(shape=(1, 32, 1024), dtype=float32)\n              \u2022 encoder_attention_mask=tf.Tensor(shape=(1, 32), dtype=int32)\n              \u2022 head_mask=None\n              \u2022 cross_attn_head_mask=None\n              \u2022 past_key_values=None\n              \u2022 use_cache=False\n              \u2022 output_attentions=False\n              \u2022 output_hidden_states=False\n              \u2022 return_dict=True\n              \u2022 training=False\n        \n        \n        Call arguments received by layer 'model' (type TFPegasusMainLayer):\n          \u2022 input_ids=tf.Tensor(shape=(1, 32), dtype=int32)\n          \u2022 attention_mask=tf.Tensor(shape=(1, 32), dtype=int32)\n          \u2022 decoder_input_ids=None\n          \u2022 decoder_attention_mask=None\n          \u2022 decoder_position_ids=None\n          \u2022 head_mask=None\n          \u2022 decoder_head_mask=None\n          \u2022 cross_attn_head_mask=None\n          \u2022 encoder_outputs=None\n          \u2022 past_key_values=None\n          \u2022 inputs_embeds=None\n          \u2022 decoder_inputs_embeds=None\n          \u2022 use_cache=True\n          \u2022 output_attentions=False\n          \u2022 output_hidden_states=False\n          \u2022 return_dict=True\n          \u2022 training=False\n          \u2022 kwargs=<class 'inspect._empty'>\n    \n    \n    Call arguments received by layer 'tf_pegasus_model_1' (type TFPegasusModel):\n      \u2022 input_ids=tf.Tensor(shape=(1, 32), dtype=int32)\n      \u2022 attention_mask=tf.Tensor(shape=(1, 32), dtype=int32)\n      \u2022 decoder_input_ids=None\n      \u2022 decoder_attention_mask=None\n      \u2022 decoder_position_ids=None\n      \u2022 head_mask=None\n      \u2022 decoder_head_mask=None\n      \u2022 cross_attn_head_mask=None\n      \u2022 encoder_outputs=None\n      \u2022 past_key_values=None\n      \u2022 inputs_embeds=None\n      \u2022 decoder_inputs_embeds=None\n      \u2022 use_cache=None\n      \u2022 output_attentions=None\n      \u2022 output_hidden_states=None\n      \u2022 return_dict=None\n      \u2022 training=False\n      \u2022 kwargs=<class 'inspect._empty'>\n"
            }
        }
    },
    {
        "PegasusForConditionalGeneration": {
            "pytorch_torch_compile_inductor": {
                "inference_s": 0.11896573305130005,
                "training_s": 0.6108526039123535
            },
            "pytorch_torch_compile_eager": {
                "inference_s": 0.1492159152030945,
                "training_s": 0.6706930136680603
            },
            "pytorch_torch_compile_cudagraphs": {
                "inference_s": 0.2596094560623169,
                "training_s": 0.8134930419921875
            },
            "pytorch_torch_compile_onnxrt": {
                "inference_s": 0.6868885731697083,
                "training_s": 1.1568407464027404
            },
            "pytorch_torch_compile_openxla": {
                "inference_s": 0.1578865885734558,
                "training_s": 0.7331786155700684
            },
            "pytorch_torch_compile_tvm": {
                "inference_s": 0.15782861709594725,
                "training_s": 0.7272067451477051
            },
            "jax_flax_xla": {
                "inference_s": 0.0031201958656311033,
                "training_s": 0.013759968280792236
            },
            "error": "AttributeError(\"'PegasusForConditionalGeneration' object has no attribute '_shift_right'\")"
        }
    },
    {
        "PLBartForConditionalGeneration": {
            "pytorch_torch_compile_inductor": {
                "inference_s": 0.03934596061706543,
                "training_s": 0.1720280933380127
            },
            "pytorch_torch_compile_eager": {
                "inference_s": 0.05059962034225464,
                "training_s": 0.2022915768623352
            },
            "pytorch_torch_compile_cudagraphs": {
                "inference_s": 0.09388178825378418,
                "training_s": 0.2588030457496643
            },
            "pytorch_torch_compile_onnxrt": {
                "inference_s": 0.18021971702575684,
                "training_s": 0.39225454807281496
            },
            "pytorch_torch_compile_openxla": {
                "inference_s": 0.050260372161865234,
                "training_s": 0.26184859037399294
            },
            "pytorch_torch_compile_tvm": {
                "inference_s": 0.051166810989379884,
                "training_s": 0.25945497274398804
            },
            "jax_flax_xla": {
                "error": "Unrecognized configuration class <class 'transformers.models.plbart.configuration_plbart.PLBartConfig'> for this kind of AutoModel: FlaxAutoModel.\nModel type should be one of AlbertConfig, BartConfig, BeitConfig, BertConfig, BigBirdConfig, BlenderbotConfig, BlenderbotSmallConfig, BloomConfig, CLIPConfig, Dinov2Config, DistilBertConfig, ElectraConfig, GemmaConfig, GPT2Config, GPT2Config, GPTNeoConfig, GPTJConfig, LlamaConfig, LongT5Config, MarianConfig, MBartConfig, MistralConfig, MT5Config, OPTConfig, PegasusConfig, RegNetConfig, ResNetConfig, RobertaConfig, RobertaPreLayerNormConfig, RoFormerConfig, T5Config, VisionTextDualEncoderConfig, ViTConfig, Wav2Vec2Config, WhisperConfig, XGLMConfig, XLMRobertaConfig."
            },
            "error": "AttributeError(\"'PLBartForConditionalGeneration' object has no attribute '_shift_right'\")"
        }
    },
    {
        "RobertaForCausalLM": {
            "pytorch_torch_compile_inductor": {
                "inference_s": 0.036044204235076906,
                "training_s": 0.1580580711364746
            },
            "pytorch_torch_compile_eager": {
                "inference_s": 0.04105679750442505,
                "training_s": 0.16344421863555908
            },
            "pytorch_torch_compile_cudagraphs": {
                "inference_s": 0.07302878379821777,
                "training_s": 0.2069071936607361
            },
            "pytorch_torch_compile_onnxrt": {
                "inference_s": 0.1485906195640564,
                "training_s": 0.29111231088638306
            },
            "pytorch_torch_compile_openxla": {
                "error": "torch_xla/csrc/runtime/pjrt_registry.cc:214 : Check failed: client \n*** Begin stack trace ***\n\ttsl::CurrentStackTrace()\n\ttorch_xla::runtime::InitializePjRt(std::string const&)\n\ttorch_xla::runtime::PjRtComputationClient::PjRtComputationClient()\n\t\n\ttorch_xla::runtime::GetComputationClient()\n\ttorch_xla::bridge::GetDefaultDevice()\n\ttorch_xla::bridge::GetCurrentDevice()\n\ttorch_xla::bridge::GetCurrentAtenDevice()\n\t\n\t\n\t\n\t_PyObject_MakeTpCall\n\t_PyEval_EvalFrameDefault\n\t\n\t_PyFunction_Vectorcall\n\t_PyEval_EvalFrameDefault\n\t\n\t_PyFunction_Vectorcall\n\t_PyEval_EvalFrameDefault\n\t\n\t_PyFunction_Vectorcall\n\t_PyEval_EvalFrameDefault\n\t\n\t_PyFunction_Vectorcall\n\t_PyEval_EvalFrameDefault\n\t\n\t_PyFunction_Vectorcall\n\t_PyEval_EvalFrameDefault\n\t\n\t_PyFunction_Vectorcall\n\t_PyEval_EvalFrameDefault\n\t\n\t_PyFunction_Vectorcall\n\t_PyEval_EvalFrameDefault\n\t\n\t_PyFunction_Vectorcall\n\t_PyEval_EvalFrameDefault\n\t\n\t_PyFunction_Vectorcall\n\tTHPFunction_apply(_object*, _object*)\n\t\n\tPyObject_Call\n\t_PyEval_EvalFrameDefault\n\t\n\t\n\t_PyEval_EvalFrameDefault\n\t\n\t_PyFunction_Vectorcall\n\t_PyEval_EvalFrameDefault\n\t\n\t_PyFunction_Vectorcall\n\t_PyEval_EvalFrameDefault\n\t\n\t_PyFunction_Vectorcall\n\t_PyEval_EvalFrameDefault\n\t\n\t_PyFunction_Vectorcall\n\t_PyEval_EvalFrameDefault\n\t\n\t_PyFunction_Vectorcall\n\t_PyEval_EvalFrameDefault\n\t\n\t\n\t\n\tPyObject_Call\n\t_PyEval_EvalFrameDefault\n\t\n\t\n\tPyObject_Call\n\t_PyEval_EvalFrameDefault\n\t\n\t\n\tPyObject_Call\n\t_PyEval_EvalFrameDefault\n\t\n\t_PyFunction_Vectorcall\n\tPyObject_Call\n\t_PyEval_EvalFrameDefault\n\t\n\t\n\tPyObject_Call\n\t_PyEval_EvalFrameDefault\n\t\n\t_PyFunction_Vectorcall\n\t_PyObject_FastCallDictTstate\n\t_PyObject_Call_Prepend\n\t\n\tPyObject_Call\n\t_PyEval_EvalFrameDefault\n\t\n\t_PyFunction_Vectorcall\n\t_PyEval_EvalFrameDefault\n\t_PyFunction_Vectorcall\n\t_PyEval_EvalFrameDefault\n\t\n\tPyEval_EvalCode\n\t\n\t\n\t\n\t_PyRun_SimpleFileObject\n\t_PyRun_AnyFileObject\n\tPy_RunMain\n\tPy_BytesMain\n\t__libc_start_main\n\t\n*** End stack trace ***\nUnknown PJRT_DEVICE 'CUDA'"
            },
            "pytorch_torch_compile_tvm": {
                "inference_s": 0.04607032060623169,
                "training_s": 0.1887372589111328
            },
            "jax_flax_xla": {
                "inference_s": 0.0006990957260131835,
                "training_s": 0.0038555455207824707
            },
            "tensorflow_eager": {
                "inference_s": 0.18058518171310425,
                "training_s": 0.5207266283035278
            },
            "tensorflow_xla": {
                "inference_s": 0.0026284241676330566,
                "training_s": 0.006586523056030274
            }
        }
    },
    {
        "RobertaForQuestionAnswering": {
            "pytorch_torch_compile_inductor": {
                "inference_s": 0.022240300178527832,
                "training_s": 0.1081736707687378
            },
            "pytorch_torch_compile_eager": {
                "inference_s": 0.028608567714691162,
                "training_s": 0.10960547685623169
            },
            "pytorch_torch_compile_cudagraphs": {
                "inference_s": 0.05662806510925293,
                "training_s": 0.13856037616729736
            },
            "pytorch_torch_compile_onnxrt": {
                "inference_s": 0.10119266748428345,
                "training_s": 0.2026730513572693
            },
            "pytorch_torch_compile_openxla": {
                "error": "torch_xla/csrc/runtime/pjrt_registry.cc:214 : Check failed: client \n*** Begin stack trace ***\n\ttsl::CurrentStackTrace()\n\ttorch_xla::runtime::InitializePjRt(std::string const&)\n\ttorch_xla::runtime::PjRtComputationClient::PjRtComputationClient()\n\t\n\ttorch_xla::runtime::GetComputationClient()\n\ttorch_xla::bridge::GetDefaultDevice()\n\ttorch_xla::bridge::GetCurrentDevice()\n\ttorch_xla::bridge::GetCurrentAtenDevice()\n\t\n\t\n\t\n\t_PyObject_MakeTpCall\n\t_PyEval_EvalFrameDefault\n\t\n\t_PyFunction_Vectorcall\n\t_PyEval_EvalFrameDefault\n\t\n\t_PyFunction_Vectorcall\n\t_PyEval_EvalFrameDefault\n\t\n\t_PyFunction_Vectorcall\n\t_PyEval_EvalFrameDefault\n\t\n\t_PyFunction_Vectorcall\n\t_PyEval_EvalFrameDefault\n\t\n\t_PyFunction_Vectorcall\n\t_PyEval_EvalFrameDefault\n\t\n\t_PyFunction_Vectorcall\n\t_PyEval_EvalFrameDefault\n\t\n\t_PyFunction_Vectorcall\n\t_PyEval_EvalFrameDefault\n\t\n\t_PyFunction_Vectorcall\n\t_PyEval_EvalFrameDefault\n\t\n\t_PyFunction_Vectorcall\n\tTHPFunction_apply(_object*, _object*)\n\t\n\tPyObject_Call\n\t_PyEval_EvalFrameDefault\n\t\n\t\n\t_PyEval_EvalFrameDefault\n\t\n\t_PyFunction_Vectorcall\n\t_PyEval_EvalFrameDefault\n\t\n\t_PyFunction_Vectorcall\n\t_PyEval_EvalFrameDefault\n\t\n\t_PyFunction_Vectorcall\n\t_PyEval_EvalFrameDefault\n\t\n\t_PyFunction_Vectorcall\n\t_PyEval_EvalFrameDefault\n\t\n\t_PyFunction_Vectorcall\n\t_PyEval_EvalFrameDefault\n\t\n\t\n\t\n\tPyObject_Call\n\t_PyEval_EvalFrameDefault\n\t\n\t\n\tPyObject_Call\n\t_PyEval_EvalFrameDefault\n\t\n\t\n\tPyObject_Call\n\t_PyEval_EvalFrameDefault\n\t\n\t_PyFunction_Vectorcall\n\tPyObject_Call\n\t_PyEval_EvalFrameDefault\n\t\n\t\n\tPyObject_Call\n\t_PyEval_EvalFrameDefault\n\t\n\t_PyFunction_Vectorcall\n\t_PyObject_FastCallDictTstate\n\t_PyObject_Call_Prepend\n\t\n\tPyObject_Call\n\t_PyEval_EvalFrameDefault\n\t\n\t_PyFunction_Vectorcall\n\t_PyEval_EvalFrameDefault\n\t_PyFunction_Vectorcall\n\t_PyEval_EvalFrameDefault\n\t\n\tPyEval_EvalCode\n\t\n\t\n\t\n\t_PyRun_SimpleFileObject\n\t_PyRun_AnyFileObject\n\tPy_RunMain\n\tPy_BytesMain\n\t__libc_start_main\n\t\n*** End stack trace ***\nUnknown PJRT_DEVICE 'CUDA'"
            },
            "pytorch_torch_compile_tvm": {
                "inference_s": 0.03392775297164917,
                "training_s": 0.11609891176223755
            },
            "jax_flax_xla": {
                "inference_s": 0.0006959939002990723,
                "training_s": 0.003915333747863769
            },
            "tensorflow_eager": {
                "inference_s": 0.18051615238189697,
                "training_s": 0.5079639434814454
            },
            "tensorflow_xla": {
                "inference_s": 0.0024611687660217284,
                "training_s": 0.006444740295410156
            }
        }
    },
    {
        "Speech2Text2ForCausalLM": {
            "pytorch_torch_compile_inductor": {
                "inference_s": 0.007846369743347167,
                "training_s": 0.029085745811462404
            },
            "pytorch_torch_compile_eager": {
                "inference_s": 0.011511642932891846,
                "training_s": 0.030616462230682373
            },
            "pytorch_torch_compile_cudagraphs": {
                "inference_s": 0.02619802951812744,
                "training_s": 0.04883721351623535
            },
            "pytorch_torch_compile_onnxrt": {
                "inference_s": 0.05608471393585205,
                "training_s": 0.1134431266784668
            },
            "pytorch_torch_compile_openxla": {
                "inference_s": 0.011628043651580811,
                "training_s": 0.03270037651062012
            },
            "pytorch_torch_compile_tvm": {
                "inference_s": 0.011801295280456543,
                "training_s": 0.03126561164855957
            },
            "jax_flax_xla": {
                "error": "Unrecognized configuration class <class 'transformers.models.speech_to_text.configuration_speech_to_text.Speech2TextConfig'> for this kind of AutoModel: FlaxAutoModel.\nModel type should be one of AlbertConfig, BartConfig, BeitConfig, BertConfig, BigBirdConfig, BlenderbotConfig, BlenderbotSmallConfig, BloomConfig, CLIPConfig, Dinov2Config, DistilBertConfig, ElectraConfig, GemmaConfig, GPT2Config, GPT2Config, GPTNeoConfig, GPTJConfig, LlamaConfig, LongT5Config, MarianConfig, MBartConfig, MistralConfig, MT5Config, OPTConfig, PegasusConfig, RegNetConfig, ResNetConfig, RobertaConfig, RobertaPreLayerNormConfig, RoFormerConfig, T5Config, VisionTextDualEncoderConfig, ViTConfig, Wav2Vec2Config, WhisperConfig, XGLMConfig, XLMRobertaConfig."
            },
            "tensorflow_eager": {
                "error": "The first argument to `Layer.call` must always be passed."
            },
            "tensorflow_xla": {
                "error": "in user code:\n\n    File \"/home/mmoffatt/jax_vs_pytorch/benchmark.py\", line 189, in fwd  *\n        return model(**inp)[0]\n    File \"/opt/conda/envs/xla-env/lib/python3.10/site-packages/tf_keras/src/utils/traceback_utils.py\", line 70, in error_handler  **\n        raise e.with_traceback(filtered_tb) from None\n    File \"/opt/conda/envs/xla-env/lib/python3.10/site-packages/tf_keras/src/utils/layer_utils.py\", line 971, in split_out_first_arg\n        raise ValueError(\n\n    ValueError: The first argument to `Layer.call` must always be passed.\n"
            }
        }
    },
    {
        "T5ForConditionalGeneration": {
            "pytorch_torch_compile_inductor": {
                "inference_s": 0.06302282810211182,
                "training_s": 0.3017886757850647
            },
            "pytorch_torch_compile_eager": {
                "inference_s": 0.08840781211853027,
                "training_s": 0.3290767288208008
            },
            "pytorch_torch_compile_cudagraphs": {
                "inference_s": 0.16448455333709716,
                "training_s": 0.4407279777526856
            },
            "pytorch_torch_compile_onnxrt": {
                "inference_s": 0.2872833299636841,
                "training_s": 0.5357684421539307
            },
            "pytorch_torch_compile_openxla": {
                "inference_s": 0.09766252040863037,
                "training_s": 0.36821441650390624
            },
            "pytorch_torch_compile_tvm": {
                "inference_s": 0.09635816812515259,
                "training_s": 0.3716914296150208
            },
            "jax_flax_xla": {
                "inference_s": 0.0017744874954223632,
                "training_s": 0.007834587097167969
            },
            "tensorflow_eager": {
                "error": "Exception encountered when calling layer 'decoder' (type TFT5MainLayer).\n\nYou have to specify either decoder_input_ids or decoder_inputs_embeds\n\nCall arguments received by layer 'decoder' (type TFT5MainLayer):\n  \u2022 input_ids=None\n  \u2022 attention_mask=None\n  \u2022 encoder_hidden_states=tf.Tensor(shape=(1, 32, 768), dtype=float32)\n  \u2022 encoder_attention_mask=tf.Tensor(shape=(1, 32), dtype=int32)\n  \u2022 inputs_embeds=None\n  \u2022 head_mask=None\n  \u2022 encoder_head_mask=None\n  \u2022 past_key_values=None\n  \u2022 use_cache=True\n  \u2022 output_attentions=False\n  \u2022 output_hidden_states=False\n  \u2022 return_dict=True\n  \u2022 training=False"
            },
            "tensorflow_xla": {
                "error": "in user code:\n\n    File \"/home/mmoffatt/jax_vs_pytorch/benchmark.py\", line 189, in fwd  *\n        return model(**inp)[0]\n    File \"/opt/conda/envs/xla-env/lib/python3.10/site-packages/tf_keras/src/utils/traceback_utils.py\", line 70, in error_handler  **\n        raise e.with_traceback(filtered_tb) from None\n    File \"/var/tmp/__autograph_generated_fileu7kf6q6f.py\", line 37, in tf__run_call_with_unpacked_inputs\n        retval_ = ag__.converted_call(ag__.ld(func), (ag__.ld(self),), dict(**ag__.ld(unpacked_inputs)), fscope)\n    File \"/var/tmp/__autograph_generated_file1ttgrc2j.py\", line 91, in tf__call\n        decoder_outputs = ag__.converted_call(ag__.ld(self).decoder, (ag__.ld(decoder_input_ids),), dict(attention_mask=ag__.ld(decoder_attention_mask), encoder_hidden_states=ag__.ld(hidden_states), encoder_attention_mask=ag__.ld(attention_mask), inputs_embeds=ag__.ld(decoder_inputs_embeds), head_mask=ag__.ld(decoder_head_mask), past_key_values=ag__.ld(past_key_values), use_cache=ag__.ld(use_cache), output_attentions=ag__.ld(output_attentions), output_hidden_states=ag__.ld(output_hidden_states), return_dict=ag__.ld(return_dict), training=ag__.ld(training)), fscope)\n    File \"/var/tmp/__autograph_generated_fileu7kf6q6f.py\", line 37, in tf__run_call_with_unpacked_inputs\n        retval_ = ag__.converted_call(ag__.ld(func), (ag__.ld(self),), dict(**ag__.ld(unpacked_inputs)), fscope)\n    File \"/var/tmp/__autograph_generated_filekrol4awi.py\", line 65, in tf__call\n        ag__.if_stmt(ag__.and_(lambda : ag__.ld(input_ids) is not None, lambda : ag__.ld(inputs_embeds) is not None), if_body_2, else_body_2, get_state_2, set_state_2, ('input_ids', 'input_shape'), 2)\n    File \"/var/tmp/__autograph_generated_filekrol4awi.py\", line 62, in else_body_2\n        ag__.if_stmt(ag__.ld(input_ids) is not None, if_body_1, else_body_1, get_state_1, set_state_1, ('input_ids', 'input_shape'), 2)\n    File \"/var/tmp/__autograph_generated_filekrol4awi.py\", line 59, in else_body_1\n        ag__.if_stmt(ag__.ld(inputs_embeds) is not None, if_body, else_body, get_state, set_state, ('input_shape',), 1)\n    File \"/var/tmp/__autograph_generated_filekrol4awi.py\", line 56, in else_body\n        raise ag__.converted_call(ag__.ld(ValueError), (f'You have to specify either {ag__.ld(err_msg_prefix)}input_ids or {ag__.ld(err_msg_prefix)}inputs_embeds',), None, fscope)\n\n    ValueError: Exception encountered when calling layer 'tft5_for_conditional_generation_1' (type TFT5ForConditionalGeneration).\n    \n    in user code:\n    \n        File \"/opt/conda/envs/xla-env/lib/python3.10/site-packages/transformers/modeling_tf_utils.py\", line 1372, in run_call_with_unpacked_inputs  *\n            return func(self, **unpacked_inputs)\n        File \"/opt/conda/envs/xla-env/lib/python3.10/site-packages/transformers/models/t5/modeling_tf_t5.py\", line 1455, in call  *\n            decoder_outputs = self.decoder(\n        File \"/opt/conda/envs/xla-env/lib/python3.10/site-packages/tf_keras/src/utils/traceback_utils.py\", line 70, in error_handler  **\n            raise e.with_traceback(filtered_tb) from None\n        File \"/var/tmp/__autograph_generated_fileu7kf6q6f.py\", line 37, in tf__run_call_with_unpacked_inputs\n            retval_ = ag__.converted_call(ag__.ld(func), (ag__.ld(self),), dict(**ag__.ld(unpacked_inputs)), fscope)\n        File \"/var/tmp/__autograph_generated_filekrol4awi.py\", line 65, in tf__call\n            ag__.if_stmt(ag__.and_(lambda : ag__.ld(input_ids) is not None, lambda : ag__.ld(inputs_embeds) is not None), if_body_2, else_body_2, get_state_2, set_state_2, ('input_ids', 'input_shape'), 2)\n        File \"/var/tmp/__autograph_generated_filekrol4awi.py\", line 62, in else_body_2\n            ag__.if_stmt(ag__.ld(input_ids) is not None, if_body_1, else_body_1, get_state_1, set_state_1, ('input_ids', 'input_shape'), 2)\n        File \"/var/tmp/__autograph_generated_filekrol4awi.py\", line 59, in else_body_1\n            ag__.if_stmt(ag__.ld(inputs_embeds) is not None, if_body, else_body, get_state, set_state, ('input_shape',), 1)\n        File \"/var/tmp/__autograph_generated_filekrol4awi.py\", line 56, in else_body\n            raise ag__.converted_call(ag__.ld(ValueError), (f'You have to specify either {ag__.ld(err_msg_prefix)}input_ids or {ag__.ld(err_msg_prefix)}inputs_embeds',), None, fscope)\n    \n        ValueError: Exception encountered when calling layer 'decoder' (type TFT5MainLayer).\n        \n        in user code:\n        \n            File \"/opt/conda/envs/xla-env/lib/python3.10/site-packages/transformers/modeling_tf_utils.py\", line 1372, in run_call_with_unpacked_inputs  *\n                return func(self, **unpacked_inputs)\n            File \"/opt/conda/envs/xla-env/lib/python3.10/site-packages/transformers/models/t5/modeling_tf_t5.py\", line 754, in call  *\n                raise ValueError(f\"You have to specify either {err_msg_prefix}input_ids or {err_msg_prefix}inputs_embeds\")\n        \n            ValueError: You have to specify either decoder_input_ids or decoder_inputs_embeds\n        \n        \n        Call arguments received by layer 'decoder' (type TFT5MainLayer):\n          \u2022 input_ids=None\n          \u2022 attention_mask=None\n          \u2022 encoder_hidden_states=tf.Tensor(shape=(1, 32, 768), dtype=float32)\n          \u2022 encoder_attention_mask=tf.Tensor(shape=(1, 32), dtype=int32)\n          \u2022 inputs_embeds=None\n          \u2022 head_mask=None\n          \u2022 encoder_head_mask=None\n          \u2022 past_key_values=None\n          \u2022 use_cache=True\n          \u2022 output_attentions=False\n          \u2022 output_hidden_states=False\n          \u2022 return_dict=True\n          \u2022 training=False\n    \n    \n    Call arguments received by layer 'tft5_for_conditional_generation_1' (type TFT5ForConditionalGeneration):\n      \u2022 input_ids=tf.Tensor(shape=(1, 32), dtype=int32)\n      \u2022 attention_mask=tf.Tensor(shape=(1, 32), dtype=int32)\n      \u2022 decoder_input_ids=None\n      \u2022 decoder_attention_mask=None\n      \u2022 head_mask=None\n      \u2022 decoder_head_mask=None\n      \u2022 encoder_outputs=None\n      \u2022 past_key_values=None\n      \u2022 inputs_embeds=None\n      \u2022 decoder_inputs_embeds=None\n      \u2022 labels=None\n      \u2022 use_cache=None\n      \u2022 output_attentions=None\n      \u2022 output_hidden_states=None\n      \u2022 return_dict=None\n      \u2022 training=False\n"
            }
        }
    },
    {
        "T5Small": {
            "pytorch_torch_compile_inductor": {
                "inference_s": 0.03629147291183472,
                "training_s": 0.13520629167556764
            },
            "pytorch_torch_compile_eager": {
                "inference_s": 0.040062830448150635,
                "training_s": 0.13860891580581666
            },
            "pytorch_torch_compile_cudagraphs": {
                "inference_s": 0.04034661054611206,
                "training_s": 0.1372301173210144
            },
            "pytorch_torch_compile_onnxrt": {
                "inference_s": 0.04042085886001587,
                "training_s": 0.13626365423202513
            },
            "pytorch_torch_compile_openxla": {
                "inference_s": 0.0400326418876648,
                "training_s": 0.1392365312576294
            },
            "pytorch_torch_compile_tvm": {
                "inference_s": 0.040136899948120114,
                "training_s": 0.13641860723495483
            },
            "jax_flax_xla": {
                "inference_s": 0.0008004474639892579,
                "training_s": 0.0041927933692932125
            },
            "tensorflow_eager": {
                "error": "Exception encountered when calling layer 'decoder' (type TFT5MainLayer).\n\nYou have to specify either decoder_input_ids or decoder_inputs_embeds\n\nCall arguments received by layer 'decoder' (type TFT5MainLayer):\n  \u2022 input_ids=None\n  \u2022 attention_mask=None\n  \u2022 encoder_hidden_states=tf.Tensor(shape=(1, 32, 512), dtype=float32)\n  \u2022 encoder_attention_mask=tf.Tensor(shape=(1, 32), dtype=int32)\n  \u2022 inputs_embeds=None\n  \u2022 head_mask=None\n  \u2022 encoder_head_mask=None\n  \u2022 past_key_values=None\n  \u2022 use_cache=True\n  \u2022 output_attentions=False\n  \u2022 output_hidden_states=False\n  \u2022 return_dict=True\n  \u2022 training=False"
            },
            "tensorflow_xla": {
                "error": "in user code:\n\n    File \"/home/mmoffatt/jax_vs_pytorch/benchmark.py\", line 189, in fwd  *\n        return model(**inp)[0]\n    File \"/opt/conda/envs/xla-env/lib/python3.10/site-packages/tf_keras/src/utils/traceback_utils.py\", line 70, in error_handler  **\n        raise e.with_traceback(filtered_tb) from None\n    File \"/var/tmp/__autograph_generated_fileu7kf6q6f.py\", line 37, in tf__run_call_with_unpacked_inputs\n        retval_ = ag__.converted_call(ag__.ld(func), (ag__.ld(self),), dict(**ag__.ld(unpacked_inputs)), fscope)\n    File \"/var/tmp/__autograph_generated_file1ttgrc2j.py\", line 91, in tf__call\n        decoder_outputs = ag__.converted_call(ag__.ld(self).decoder, (ag__.ld(decoder_input_ids),), dict(attention_mask=ag__.ld(decoder_attention_mask), encoder_hidden_states=ag__.ld(hidden_states), encoder_attention_mask=ag__.ld(attention_mask), inputs_embeds=ag__.ld(decoder_inputs_embeds), head_mask=ag__.ld(decoder_head_mask), past_key_values=ag__.ld(past_key_values), use_cache=ag__.ld(use_cache), output_attentions=ag__.ld(output_attentions), output_hidden_states=ag__.ld(output_hidden_states), return_dict=ag__.ld(return_dict), training=ag__.ld(training)), fscope)\n    File \"/var/tmp/__autograph_generated_fileu7kf6q6f.py\", line 37, in tf__run_call_with_unpacked_inputs\n        retval_ = ag__.converted_call(ag__.ld(func), (ag__.ld(self),), dict(**ag__.ld(unpacked_inputs)), fscope)\n    File \"/var/tmp/__autograph_generated_filekrol4awi.py\", line 65, in tf__call\n        ag__.if_stmt(ag__.and_(lambda : ag__.ld(input_ids) is not None, lambda : ag__.ld(inputs_embeds) is not None), if_body_2, else_body_2, get_state_2, set_state_2, ('input_ids', 'input_shape'), 2)\n    File \"/var/tmp/__autograph_generated_filekrol4awi.py\", line 62, in else_body_2\n        ag__.if_stmt(ag__.ld(input_ids) is not None, if_body_1, else_body_1, get_state_1, set_state_1, ('input_ids', 'input_shape'), 2)\n    File \"/var/tmp/__autograph_generated_filekrol4awi.py\", line 59, in else_body_1\n        ag__.if_stmt(ag__.ld(inputs_embeds) is not None, if_body, else_body, get_state, set_state, ('input_shape',), 1)\n    File \"/var/tmp/__autograph_generated_filekrol4awi.py\", line 56, in else_body\n        raise ag__.converted_call(ag__.ld(ValueError), (f'You have to specify either {ag__.ld(err_msg_prefix)}input_ids or {ag__.ld(err_msg_prefix)}inputs_embeds',), None, fscope)\n\n    ValueError: Exception encountered when calling layer 'tft5_for_conditional_generation_3' (type TFT5ForConditionalGeneration).\n    \n    in user code:\n    \n        File \"/opt/conda/envs/xla-env/lib/python3.10/site-packages/transformers/modeling_tf_utils.py\", line 1372, in run_call_with_unpacked_inputs  *\n            return func(self, **unpacked_inputs)\n        File \"/opt/conda/envs/xla-env/lib/python3.10/site-packages/transformers/models/t5/modeling_tf_t5.py\", line 1455, in call  *\n            decoder_outputs = self.decoder(\n        File \"/opt/conda/envs/xla-env/lib/python3.10/site-packages/tf_keras/src/utils/traceback_utils.py\", line 70, in error_handler  **\n            raise e.with_traceback(filtered_tb) from None\n        File \"/var/tmp/__autograph_generated_fileu7kf6q6f.py\", line 37, in tf__run_call_with_unpacked_inputs\n            retval_ = ag__.converted_call(ag__.ld(func), (ag__.ld(self),), dict(**ag__.ld(unpacked_inputs)), fscope)\n        File \"/var/tmp/__autograph_generated_filekrol4awi.py\", line 65, in tf__call\n            ag__.if_stmt(ag__.and_(lambda : ag__.ld(input_ids) is not None, lambda : ag__.ld(inputs_embeds) is not None), if_body_2, else_body_2, get_state_2, set_state_2, ('input_ids', 'input_shape'), 2)\n        File \"/var/tmp/__autograph_generated_filekrol4awi.py\", line 62, in else_body_2\n            ag__.if_stmt(ag__.ld(input_ids) is not None, if_body_1, else_body_1, get_state_1, set_state_1, ('input_ids', 'input_shape'), 2)\n        File \"/var/tmp/__autograph_generated_filekrol4awi.py\", line 59, in else_body_1\n            ag__.if_stmt(ag__.ld(inputs_embeds) is not None, if_body, else_body, get_state, set_state, ('input_shape',), 1)\n        File \"/var/tmp/__autograph_generated_filekrol4awi.py\", line 56, in else_body\n            raise ag__.converted_call(ag__.ld(ValueError), (f'You have to specify either {ag__.ld(err_msg_prefix)}input_ids or {ag__.ld(err_msg_prefix)}inputs_embeds',), None, fscope)\n    \n        ValueError: Exception encountered when calling layer 'decoder' (type TFT5MainLayer).\n        \n        in user code:\n        \n            File \"/opt/conda/envs/xla-env/lib/python3.10/site-packages/transformers/modeling_tf_utils.py\", line 1372, in run_call_with_unpacked_inputs  *\n                return func(self, **unpacked_inputs)\n            File \"/opt/conda/envs/xla-env/lib/python3.10/site-packages/transformers/models/t5/modeling_tf_t5.py\", line 754, in call  *\n                raise ValueError(f\"You have to specify either {err_msg_prefix}input_ids or {err_msg_prefix}inputs_embeds\")\n        \n            ValueError: You have to specify either decoder_input_ids or decoder_inputs_embeds\n        \n        \n        Call arguments received by layer 'decoder' (type TFT5MainLayer):\n          \u2022 input_ids=None\n          \u2022 attention_mask=None\n          \u2022 encoder_hidden_states=tf.Tensor(shape=(1, 32, 512), dtype=float32)\n          \u2022 encoder_attention_mask=tf.Tensor(shape=(1, 32), dtype=int32)\n          \u2022 inputs_embeds=None\n          \u2022 head_mask=None\n          \u2022 encoder_head_mask=None\n          \u2022 past_key_values=None\n          \u2022 use_cache=True\n          \u2022 output_attentions=False\n          \u2022 output_hidden_states=False\n          \u2022 return_dict=True\n          \u2022 training=False\n    \n    \n    Call arguments received by layer 'tft5_for_conditional_generation_3' (type TFT5ForConditionalGeneration):\n      \u2022 input_ids=tf.Tensor(shape=(1, 32), dtype=int32)\n      \u2022 attention_mask=tf.Tensor(shape=(1, 32), dtype=int32)\n      \u2022 decoder_input_ids=None\n      \u2022 decoder_attention_mask=None\n      \u2022 head_mask=None\n      \u2022 decoder_head_mask=None\n      \u2022 encoder_outputs=None\n      \u2022 past_key_values=None\n      \u2022 inputs_embeds=None\n      \u2022 decoder_inputs_embeds=None\n      \u2022 labels=None\n      \u2022 use_cache=None\n      \u2022 output_attentions=None\n      \u2022 output_hidden_states=None\n      \u2022 return_dict=None\n      \u2022 training=False\n"
            }
        }
    },
    {
        "TrOCRForCausalLM": {
            "pytorch_torch_compile_inductor": {
                "error": "You have to specify pixel_values"
            },
            "pytorch_torch_compile_eager": {
                "error": "You have to specify pixel_values"
            },
            "pytorch_torch_compile_cudagraphs": {
                "error": "You have to specify pixel_values"
            },
            "pytorch_torch_compile_onnxrt": {
                "error": "You have to specify pixel_values"
            },
            "pytorch_torch_compile_openxla": {
                "error": "You have to specify pixel_values"
            },
            "pytorch_torch_compile_tvm": {
                "error": "You have to specify pixel_values"
            },
            "jax_flax_xla": {
                "error": "Unrecognized configuration class <class 'transformers.models.vision_encoder_decoder.configuration_vision_encoder_decoder.VisionEncoderDecoderConfig'> for this kind of AutoModel: FlaxAutoModel.\nModel type should be one of AlbertConfig, BartConfig, BeitConfig, BertConfig, BigBirdConfig, BlenderbotConfig, BlenderbotSmallConfig, BloomConfig, CLIPConfig, Dinov2Config, DistilBertConfig, ElectraConfig, GemmaConfig, GPT2Config, GPT2Config, GPTNeoConfig, GPTJConfig, LlamaConfig, LongT5Config, MarianConfig, MBartConfig, MistralConfig, MT5Config, OPTConfig, PegasusConfig, RegNetConfig, ResNetConfig, RobertaConfig, RobertaPreLayerNormConfig, RoFormerConfig, T5Config, VisionTextDualEncoderConfig, ViTConfig, Wav2Vec2Config, WhisperConfig, XGLMConfig, XLMRobertaConfig."
            },
            "error": "AttributeError(\"'VisionEncoderDecoderModel' object has no attribute '_shift_right'\")"
        }
    },
    {
        "XGLMForCausalLM": {
            "pytorch_torch_compile_inductor": {
                "inference_s": 0.17264623880386354,
                "training_s": 0.893746063709259
            },
            "pytorch_torch_compile_eager": {
                "inference_s": 0.1742038059234619,
                "training_s": 1.075943455696106
            },
            "pytorch_torch_compile_cudagraphs": {
                "inference_s": 0.2392353320121765,
                "training_s": 1.1810668301582337
            },
            "pytorch_torch_compile_onnxrt": {
                "error": "backend='onnxrt' raised:\nAssertionError: Please convert all Tensors to FakeTensors first or instantiate FakeTensorMode with 'allow_non_fake_inputs'. Found in aten.alias.default(tensor(-3.4028e+38, size=()))\n\nWhile executing %tensor : [num_users=1] = call_function[target=torch.tensor](args = (-3.4028234663852886e+38,), kwargs = {device: cpu})\nOriginal traceback:\n  File \"/opt/conda/envs/xla-env/lib/python3.10/site-packages/transformers/models/xglm/modeling_xglm.py\", line 413, in forward\n    hidden_states, self_attn_weights, present_key_value = self.self_attn(\n  File \"/opt/conda/envs/xla-env/lib/python3.10/site-packages/transformers/models/xglm/modeling_xglm.py\", line 295, in forward\n    attn_weights, torch.tensor(torch.finfo(attn_weights.dtype).min, device=attn_weights.device)\n\n\nSet TORCH_LOGS=\"+dynamo\" and TORCHDYNAMO_VERBOSE=1 for more information\n\n\nYou can suppress this exception and fall back to eager by setting:\n    import torch._dynamo\n    torch._dynamo.config.suppress_errors = True\n"
            },
            "pytorch_torch_compile_openxla": {
                "inference_s": 0.17184276819229127,
                "training_s": 1.057679569721222
            },
            "pytorch_torch_compile_tvm": {
                "inference_s": 0.17337477922439576,
                "training_s": 1.020522150993347
            },
            "jax_flax_xla": {
                "inference_s": 0.0016668772697448731,
                "training_s": 0.007786545753479004
            },
            "tensorflow_eager": {
                "inference_s": 0.42988832712173464,
                "training_s": 1.0785448527336121
            },
            "tensorflow_xla": {
                "inference_s": 0.004191761016845703,
                "training_s": 0.02243908405303955
            }
        }
    },
    {
        "XLNetLMHeadModel": {
            "pytorch_torch_compile_inductor": {
                "inference_s": 0.03272564172744751,
                "training_s": 0.13295984268188477
            },
            "pytorch_torch_compile_eager": {
                "inference_s": 0.05149167060852051,
                "training_s": 0.1736732816696167
            },
            "pytorch_torch_compile_cudagraphs": {
                "inference_s": 0.12713451862335204,
                "training_s": 0.23758399724960327
            },
            "pytorch_torch_compile_onnxrt": {
                "error": "Unsupported attribute type '<class 'torch.dtype'>' for attribute 'to' in node=%908 : Tensor = onnx::Cast(%_val_907)\n, value is torch.float32"
            },
            "pytorch_torch_compile_openxla": {
                "error": "torch_xla/csrc/runtime/pjrt_registry.cc:214 : Check failed: client \n*** Begin stack trace ***\n\ttsl::CurrentStackTrace()\n\ttorch_xla::runtime::InitializePjRt(std::string const&)\n\ttorch_xla::runtime::PjRtComputationClient::PjRtComputationClient()\n\t\n\ttorch_xla::runtime::GetComputationClient()\n\ttorch_xla::bridge::GetDefaultDevice()\n\ttorch_xla::bridge::GetCurrentDevice()\n\ttorch_xla::bridge::GetCurrentAtenDevice()\n\t\n\t\n\t\n\t_PyObject_MakeTpCall\n\t_PyEval_EvalFrameDefault\n\t\n\t_PyFunction_Vectorcall\n\t_PyEval_EvalFrameDefault\n\t\n\t_PyFunction_Vectorcall\n\t_PyEval_EvalFrameDefault\n\t\n\t_PyFunction_Vectorcall\n\t_PyEval_EvalFrameDefault\n\t\n\t_PyFunction_Vectorcall\n\t_PyEval_EvalFrameDefault\n\t\n\t_PyFunction_Vectorcall\n\t_PyEval_EvalFrameDefault\n\t\n\t_PyFunction_Vectorcall\n\t_PyEval_EvalFrameDefault\n\t\n\t_PyFunction_Vectorcall\n\t_PyEval_EvalFrameDefault\n\t\n\t_PyFunction_Vectorcall\n\t_PyEval_EvalFrameDefault\n\t\n\t_PyFunction_Vectorcall\n\tTHPFunction_apply(_object*, _object*)\n\t\n\tPyObject_Call\n\t_PyEval_EvalFrameDefault\n\t\n\t\n\t_PyEval_EvalFrameDefault\n\t\n\t_PyFunction_Vectorcall\n\t_PyEval_EvalFrameDefault\n\t\n\t_PyFunction_Vectorcall\n\t_PyEval_EvalFrameDefault\n\t\n\t_PyFunction_Vectorcall\n\t_PyEval_EvalFrameDefault\n\t\n\t_PyFunction_Vectorcall\n\t_PyEval_EvalFrameDefault\n\t\n\t_PyFunction_Vectorcall\n\t_PyEval_EvalFrameDefault\n\t\n\t\n\t\n\tPyObject_Call\n\t_PyEval_EvalFrameDefault\n\t\n\t\n\tPyObject_Call\n\t_PyEval_EvalFrameDefault\n\t\n\t\n\tPyObject_Call\n\t_PyEval_EvalFrameDefault\n\t\n\t_PyFunction_Vectorcall\n\tPyObject_Call\n\t_PyEval_EvalFrameDefault\n\t\n\t\n\tPyObject_Call\n\t_PyEval_EvalFrameDefault\n\t\n\t_PyFunction_Vectorcall\n\t_PyObject_FastCallDictTstate\n\t_PyObject_Call_Prepend\n\t\n\tPyObject_Call\n\t_PyEval_EvalFrameDefault\n\t\n\t_PyFunction_Vectorcall\n\t_PyEval_EvalFrameDefault\n\t_PyFunction_Vectorcall\n\t_PyEval_EvalFrameDefault\n\t\n\tPyEval_EvalCode\n\t\n\t\n\t\n\t_PyRun_SimpleFileObject\n\t_PyRun_AnyFileObject\n\tPy_RunMain\n\tPy_BytesMain\n\t__libc_start_main\n\t\n*** End stack trace ***\nUnknown PJRT_DEVICE 'CUDA'"
            },
            "pytorch_torch_compile_tvm": {
                "inference_s": 0.0585669207572937,
                "training_s": 0.17586587190628053
            },
            "jax_flax_xla": {
                "error": "Unrecognized configuration class <class 'transformers.models.xlnet.configuration_xlnet.XLNetConfig'> for this kind of AutoModel: FlaxAutoModel.\nModel type should be one of AlbertConfig, BartConfig, BeitConfig, BertConfig, BigBirdConfig, BlenderbotConfig, BlenderbotSmallConfig, BloomConfig, CLIPConfig, Dinov2Config, DistilBertConfig, ElectraConfig, GemmaConfig, GPT2Config, GPT2Config, GPTNeoConfig, GPTJConfig, LlamaConfig, LongT5Config, MarianConfig, MBartConfig, MistralConfig, MT5Config, OPTConfig, PegasusConfig, RegNetConfig, ResNetConfig, RobertaConfig, RobertaPreLayerNormConfig, RoFormerConfig, T5Config, VisionTextDualEncoderConfig, ViTConfig, Wav2Vec2Config, WhisperConfig, XGLMConfig, XLMRobertaConfig."
            },
            "tensorflow_eager": {
                "inference_s": 0.16456023693084718,
                "training_s": 0.539556531906128
            },
            "tensorflow_xla": {
                "inference_s": 0.002726559638977051,
                "training_s": 0.006387968063354493
            }
        }
    },
    {
        "YituTechConvBert": {
            "pytorch_torch_compile_onnxrt": {
                "inference_s": 0.05654417991638184,
                "training_s": 0.3789751434326172
            },
            "jax_flax_xla": {
                "error": "Unrecognized configuration class <class 'transformers.models.convbert.configuration_convbert.ConvBertConfig'> for this kind of AutoModel: FlaxAutoModel.\nModel type should be one of AlbertConfig, BartConfig, BeitConfig, BertConfig, BigBirdConfig, BlenderbotConfig, BlenderbotSmallConfig, BloomConfig, CLIPConfig, Dinov2Config, DistilBertConfig, ElectraConfig, GemmaConfig, GPT2Config, GPT2Config, GPTNeoConfig, GPTJConfig, LlamaConfig, LongT5Config, MarianConfig, MBartConfig, MistralConfig, MT5Config, OPTConfig, PegasusConfig, RegNetConfig, ResNetConfig, RobertaConfig, RobertaPreLayerNormConfig, RoFormerConfig, T5Config, VisionTextDualEncoderConfig, ViTConfig, Wav2Vec2Config, WhisperConfig, XGLMConfig, XLMRobertaConfig."
            },
            "tensorflow_eager": {
                "inference_s": 0.28926988363265993,
                "training_s": 0.8356155943870545
            },
            "tensorflow_xla": {
                "inference_s": 0.0034669876098632813,
                "training_s": 0.00942129373550415
            }
        }
    }
]