[
    {
        "AlbertForMaskedLM": {
            "pytorch_torch_compile_inductor": {
                "ttft_s": 15.111132860183716
            },
            "pytorch_torch_compile_eager": {
                "ttft_s": 1.3878977298736572
            },
            "pytorch_torch_compile_cudagraphs": {
                "ttft_s": 6.111937522888184
            },
            "pytorch_torch_compile_onnxrt": {
                "ttft_s": 11.48500370979309
            },
            "pytorch_torch_compile_openxla": {
                "error": "torch_xla/csrc/runtime/pjrt_registry.cc:214 : Check failed: client \n*** Begin stack trace ***\n\ttsl::CurrentStackTrace()\n\ttorch_xla::runtime::InitializePjRt(std::string const&)\n\ttorch_xla::runtime::PjRtComputationClient::PjRtComputationClient()\n\t\n\ttorch_xla::runtime::GetComputationClient()\n\ttorch_xla::bridge::GetDefaultDevice()\n\ttorch_xla::bridge::GetCurrentDevice()\n\ttorch_xla::bridge::GetCurrentAtenDevice()\n\t\n\t\n\t\n\t_PyObject_MakeTpCall\n\t_PyEval_EvalFrameDefault\n\t_PyFunction_Vectorcall\n\t_PyEval_EvalFrameDefault\n\t_PyFunction_Vectorcall\n\t_PyEval_EvalFrameDefault\n\t_PyFunction_Vectorcall\n\t_PyEval_EvalFrameDefault\n\t_PyFunction_Vectorcall\n\t_PyEval_EvalFrameDefault\n\t_PyFunction_Vectorcall\n\t_PyEval_EvalFrameDefault\n\t_PyFunction_Vectorcall\n\t_PyEval_EvalFrameDefault\n\t_PyFunction_Vectorcall\n\t_PyEval_EvalFrameDefault\n\t_PyFunction_Vectorcall\n\t_PyEval_EvalFrameDefault\n\t_PyFunction_Vectorcall\n\tTHPFunction_apply(_object*, _object*)\n\t\n\tPyObject_Call\n\t_PyEval_EvalFrameDefault\n\t\n\t_PyEval_EvalFrameDefault\n\t_PyFunction_Vectorcall\n\t_PyEval_EvalFrameDefault\n\t_PyFunction_Vectorcall\n\t_PyEval_EvalFrameDefault\n\t_PyFunction_Vectorcall\n\t_PyEval_EvalFrameDefault\n\t_PyFunction_Vectorcall\n\t_PyEval_EvalFrameDefault\n\t\n\t_PyFunction_Vectorcall\n\t_PyEval_EvalFrameDefault\n\t\n\t\n\t\n\tPyObject_Call\n\t_PyEval_EvalFrameDefault\n\t\n\t\n\tPyObject_Call\n\t_PyEval_EvalFrameDefault\n\t\n\t\n\tPyObject_Call\n\t_PyEval_EvalFrameDefault\n\t_PyFunction_Vectorcall\n\tPyObject_Call\n\t_PyEval_EvalFrameDefault\n\t\n\tPyObject_Call\n\t_PyEval_EvalFrameDefault\n\t_PyFunction_Vectorcall\n\t_PyObject_FastCallDictTstate\n\t_PyObject_Call_Prepend\n\t\n\tPyObject_Call\n\t_PyEval_EvalFrameDefault\n\t_PyFunction_Vectorcall\n\t_PyEval_EvalFrameDefault\n\t_PyFunction_Vectorcall\n\t_PyEval_EvalFrameDefault\n\t\n\tPyEval_EvalCode\n\t\n\t\n\t\n\t_PyRun_SimpleFileObject\n\t_PyRun_AnyFileObject\n\tPy_RunMain\n\tPy_BytesMain\n\t__libc_start_main\n\t\n*** End stack trace ***\nUnknown PJRT_DEVICE 'CUDA'"
            },
            "pytorch_torch_compile_tvm": {
                "error": "backend='tvm' raised:\nModuleNotFoundError: No module named 'tvm'\n\nSet TORCH_LOGS=\"+dynamo\" and TORCHDYNAMO_VERBOSE=1 for more information\n\n\nYou can suppress this exception and fall back to eager by setting:\n    import torch._dynamo\n    torch._dynamo.config.suppress_errors = True\n"
            },
            "jax_flax_xla": {
                "ttft_s": 15.304303884506226
            },
            "tensorflow_eager": {
                "ttft_s": 1.1673996448516846
            },
            "tensorflow_xla": {
                "ttft_s": 10.069652795791626
            },
            "jax_flax_eager": {
                "ttft_s": 15.042686462402344
            }
        }
    },
    {
        "AlbertForQuestionAnswering": {
            "pytorch_torch_compile_inductor": {
                "ttft_s": 8.981879472732544
            },
            "pytorch_torch_compile_eager": {
                "ttft_s": 1.3854436874389648
            },
            "pytorch_torch_compile_cudagraphs": {
                "ttft_s": 6.2380921840667725
            },
            "pytorch_torch_compile_onnxrt": {
                "ttft_s": 10.686684131622314
            },
            "pytorch_torch_compile_openxla": {
                "error": "torch_xla/csrc/runtime/pjrt_registry.cc:214 : Check failed: client \n*** Begin stack trace ***\n\ttsl::CurrentStackTrace()\n\ttorch_xla::runtime::InitializePjRt(std::string const&)\n\ttorch_xla::runtime::PjRtComputationClient::PjRtComputationClient()\n\t\n\ttorch_xla::runtime::GetComputationClient()\n\ttorch_xla::bridge::GetDefaultDevice()\n\ttorch_xla::bridge::GetCurrentDevice()\n\ttorch_xla::bridge::GetCurrentAtenDevice()\n\t\n\t\n\t\n\t_PyObject_MakeTpCall\n\t_PyEval_EvalFrameDefault\n\t\n\t_PyFunction_Vectorcall\n\t_PyEval_EvalFrameDefault\n\t\n\t_PyFunction_Vectorcall\n\t_PyEval_EvalFrameDefault\n\t\n\t_PyFunction_Vectorcall\n\t_PyEval_EvalFrameDefault\n\t\n\t_PyFunction_Vectorcall\n\t_PyEval_EvalFrameDefault\n\t\n\t_PyFunction_Vectorcall\n\t_PyEval_EvalFrameDefault\n\t\n\t_PyFunction_Vectorcall\n\t_PyEval_EvalFrameDefault\n\t\n\t_PyFunction_Vectorcall\n\t_PyEval_EvalFrameDefault\n\t\n\t_PyFunction_Vectorcall\n\t_PyEval_EvalFrameDefault\n\t\n\t_PyFunction_Vectorcall\n\tTHPFunction_apply(_object*, _object*)\n\t\n\tPyObject_Call\n\t_PyEval_EvalFrameDefault\n\t\n\t\n\t_PyEval_EvalFrameDefault\n\t\n\t_PyFunction_Vectorcall\n\t_PyEval_EvalFrameDefault\n\t\n\t_PyFunction_Vectorcall\n\t_PyEval_EvalFrameDefault\n\t\n\t_PyFunction_Vectorcall\n\t_PyEval_EvalFrameDefault\n\t\n\t_PyFunction_Vectorcall\n\t_PyEval_EvalFrameDefault\n\t\n\t_PyFunction_Vectorcall\n\t_PyEval_EvalFrameDefault\n\t\n\t\n\t\n\tPyObject_Call\n\t_PyEval_EvalFrameDefault\n\t\n\t\n\tPyObject_Call\n\t_PyEval_EvalFrameDefault\n\t\n\t\n\tPyObject_Call\n\t_PyEval_EvalFrameDefault\n\t\n\t_PyFunction_Vectorcall\n\tPyObject_Call\n\t_PyEval_EvalFrameDefault\n\t\n\t\n\tPyObject_Call\n\t_PyEval_EvalFrameDefault\n\t\n\t_PyFunction_Vectorcall\n\t_PyObject_FastCallDictTstate\n\t_PyObject_Call_Prepend\n\t\n\tPyObject_Call\n\t_PyEval_EvalFrameDefault\n\t\n\t_PyFunction_Vectorcall\n\t_PyEval_EvalFrameDefault\n\t_PyFunction_Vectorcall\n\t_PyEval_EvalFrameDefault\n\t\n\tPyEval_EvalCode\n\t\n\t\n\t\n\t_PyRun_SimpleFileObject\n\t_PyRun_AnyFileObject\n\tPy_RunMain\n\tPy_BytesMain\n\t__libc_start_main\n\t\n*** End stack trace ***\nUnknown PJRT_DEVICE 'CUDA'"
            },
            "pytorch_torch_compile_tvm": {
                "error": "backend='tvm' raised:\nModuleNotFoundError: No module named 'tvm'\n\nSet TORCH_LOGS=\"+dynamo\" and TORCHDYNAMO_VERBOSE=1 for more information\n\n\nYou can suppress this exception and fall back to eager by setting:\n    import torch._dynamo\n    torch._dynamo.config.suppress_errors = True\n"
            },
            "jax_flax_xla": {
                "error": "'jaxlib.xla_extension.ArrayImpl' object has no attribute 'split'"
            },
            "tensorflow_eager": {
                "ttft_s": 0.20815706253051758
            },
            "tensorflow_xla": {
                "ttft_s": 3.2517521381378174
            }
        }
    },
    {
        "AllenaiLongformerBase": {
            "pytorch_torch_compile_inductor": {
                "ttft_s": 38.149978160858154
            },
            "pytorch_torch_compile_eager": {
                "ttft_s": 7.477518558502197
            },
            "pytorch_torch_compile_cudagraphs": {
                "ttft_s": 28.08529806137085
            },
            "pytorch_torch_compile_onnxrt": {
                "ttft_s": 66.16055059432983
            },
            "pytorch_torch_compile_openxla": {
                "error": "torch_xla/csrc/runtime/pjrt_registry.cc:214 : Check failed: client \n*** Begin stack trace ***\n\ttsl::CurrentStackTrace()\n\ttorch_xla::runtime::InitializePjRt(std::string const&)\n\ttorch_xla::runtime::PjRtComputationClient::PjRtComputationClient()\n\t\n\ttorch_xla::runtime::GetComputationClient()\n\ttorch_xla::bridge::GetDefaultDevice()\n\ttorch_xla::bridge::GetCurrentDevice()\n\ttorch_xla::bridge::GetCurrentAtenDevice()\n\t\n\t\n\t\n\t_PyObject_MakeTpCall\n\t_PyEval_EvalFrameDefault\n\t\n\t_PyFunction_Vectorcall\n\t_PyEval_EvalFrameDefault\n\t\n\t_PyFunction_Vectorcall\n\t_PyEval_EvalFrameDefault\n\t\n\t_PyFunction_Vectorcall\n\t_PyEval_EvalFrameDefault\n\t\n\t_PyFunction_Vectorcall\n\t_PyEval_EvalFrameDefault\n\t\n\t_PyFunction_Vectorcall\n\t_PyEval_EvalFrameDefault\n\t\n\t_PyFunction_Vectorcall\n\t_PyEval_EvalFrameDefault\n\t\n\t_PyFunction_Vectorcall\n\t_PyEval_EvalFrameDefault\n\t\n\t_PyFunction_Vectorcall\n\t_PyEval_EvalFrameDefault\n\t\n\t_PyFunction_Vectorcall\n\t_PyEval_EvalFrameDefault\n\t\n\t_PyFunction_Vectorcall\n\t_PyEval_EvalFrameDefault\n\t\n\t_PyFunction_Vectorcall\n\t_PyEval_EvalFrameDefault\n\t\n\t\n\t\n\tPyObject_Call\n\t_PyEval_EvalFrameDefault\n\t\n\t\n\tPyObject_Call\n\t_PyEval_EvalFrameDefault\n\t\n\t_PyFunction_Vectorcall\n\t_PyObject_FastCallDictTstate\n\t_PyObject_Call_Prepend\n\t\n\t_PyObject_MakeTpCall\n\t_PyEval_EvalFrameDefault\n\t\n\t\n\t\n\tPyObject_Call\n\t_PyEval_EvalFrameDefault\n\t\n\t\n\tPyObject_Call\n\t_PyEval_EvalFrameDefault\n\t\n\t\n\tPyObject_Call\n\t_PyEval_EvalFrameDefault\n\t\n\t_PyFunction_Vectorcall\n\tPyObject_Call\n\t_PyEval_EvalFrameDefault\n\t\n\t\n\tPyObject_Call\n\t_PyEval_EvalFrameDefault\n\t\n\t_PyFunction_Vectorcall\n\t_PyObject_FastCallDictTstate\n\t_PyObject_Call_Prepend\n\t\n\tPyObject_Call\n\t_PyEval_EvalFrameDefault\n\t\n\t_PyFunction_Vectorcall\n\t_PyEval_EvalFrameDefault\n\t_PyFunction_Vectorcall\n\t_PyEval_EvalFrameDefault\n\t\n\tPyEval_EvalCode\n\t\n\t\n\t\n\t_PyRun_SimpleFileObject\n\t_PyRun_AnyFileObject\n\tPy_RunMain\n\tPy_BytesMain\n\t__libc_start_main\n\t\n*** End stack trace ***\nUnknown PJRT_DEVICE 'CUDA'"
            },
            "pytorch_torch_compile_tvm": {
                "error": "backend='tvm' raised:\nModuleNotFoundError: No module named 'tvm'\n\nSet TORCH_LOGS=\"+dynamo\" and TORCHDYNAMO_VERBOSE=1 for more information\n\n\nYou can suppress this exception and fall back to eager by setting:\n    import torch._dynamo\n    torch._dynamo.config.suppress_errors = True\n"
            },
            "jax_flax_xla": {
                "error": "Unrecognized configuration class <class 'transformers.models.longformer.configuration_longformer.LongformerConfig'> for this kind of AutoModel: FlaxAutoModel.\nModel type should be one of AlbertConfig, BartConfig, BeitConfig, BertConfig, BigBirdConfig, BlenderbotConfig, BlenderbotSmallConfig, BloomConfig, CLIPConfig, Dinov2Config, DistilBertConfig, ElectraConfig, GemmaConfig, GPT2Config, GPT2Config, GPTNeoConfig, GPTJConfig, LlamaConfig, LongT5Config, MarianConfig, MBartConfig, MistralConfig, MT5Config, OPTConfig, PegasusConfig, RegNetConfig, ResNetConfig, RobertaConfig, RobertaPreLayerNormConfig, RoFormerConfig, T5Config, VisionTextDualEncoderConfig, ViTConfig, Wav2Vec2Config, WhisperConfig, XGLMConfig, XLMRobertaConfig."
            },
            "tensorflow_eager": {
                "ttft_s": 2.207275152206421
            },
            "tensorflow_xla": {
                "ttft_s": 26.997106075286865
            }
        }
    },
    {
        "BartForCausalLM": {
            "pytorch_torch_compile_inductor": {
                "ttft_s": 12.159012079238892
            },
            "pytorch_torch_compile_eager": {
                "ttft_s": 2.051081418991089
            },
            "pytorch_torch_compile_cudagraphs": {
                "ttft_s": 7.084300518035889
            },
            "pytorch_torch_compile_onnxrt": {
                "ttft_s": 14.295673608779907
            },
            "pytorch_torch_compile_openxla": {
                "error": "torch_xla/csrc/runtime/pjrt_registry.cc:214 : Check failed: client \n*** Begin stack trace ***\n\ttsl::CurrentStackTrace()\n\ttorch_xla::runtime::InitializePjRt(std::string const&)\n\ttorch_xla::runtime::PjRtComputationClient::PjRtComputationClient()\n\t\n\ttorch_xla::runtime::GetComputationClient()\n\ttorch_xla::bridge::GetDefaultDevice()\n\ttorch_xla::bridge::GetCurrentDevice()\n\ttorch_xla::bridge::GetCurrentAtenDevice()\n\t\n\t\n\t\n\t_PyObject_MakeTpCall\n\t_PyEval_EvalFrameDefault\n\t\n\t_PyFunction_Vectorcall\n\t_PyEval_EvalFrameDefault\n\t\n\t_PyFunction_Vectorcall\n\t_PyEval_EvalFrameDefault\n\t\n\t_PyFunction_Vectorcall\n\t_PyEval_EvalFrameDefault\n\t\n\t_PyFunction_Vectorcall\n\t_PyEval_EvalFrameDefault\n\t\n\t_PyFunction_Vectorcall\n\t_PyEval_EvalFrameDefault\n\t\n\t_PyFunction_Vectorcall\n\t_PyEval_EvalFrameDefault\n\t\n\t_PyFunction_Vectorcall\n\t_PyEval_EvalFrameDefault\n\t\n\t_PyFunction_Vectorcall\n\t_PyEval_EvalFrameDefault\n\t\n\t_PyFunction_Vectorcall\n\tTHPFunction_apply(_object*, _object*)\n\t\n\tPyObject_Call\n\t_PyEval_EvalFrameDefault\n\t\n\t\n\t_PyEval_EvalFrameDefault\n\t\n\t_PyFunction_Vectorcall\n\t_PyEval_EvalFrameDefault\n\t\n\t_PyFunction_Vectorcall\n\t_PyEval_EvalFrameDefault\n\t\n\t_PyFunction_Vectorcall\n\t_PyEval_EvalFrameDefault\n\t\n\t_PyFunction_Vectorcall\n\t_PyEval_EvalFrameDefault\n\t\n\t_PyFunction_Vectorcall\n\t_PyEval_EvalFrameDefault\n\t\n\t\n\t\n\tPyObject_Call\n\t_PyEval_EvalFrameDefault\n\t\n\t\n\tPyObject_Call\n\t_PyEval_EvalFrameDefault\n\t\n\t\n\tPyObject_Call\n\t_PyEval_EvalFrameDefault\n\t\n\t_PyFunction_Vectorcall\n\tPyObject_Call\n\t_PyEval_EvalFrameDefault\n\t\n\t\n\tPyObject_Call\n\t_PyEval_EvalFrameDefault\n\t\n\t_PyFunction_Vectorcall\n\t_PyObject_FastCallDictTstate\n\t_PyObject_Call_Prepend\n\t\n\tPyObject_Call\n\t_PyEval_EvalFrameDefault\n\t\n\t_PyFunction_Vectorcall\n\t_PyEval_EvalFrameDefault\n\t_PyFunction_Vectorcall\n\t_PyEval_EvalFrameDefault\n\t\n\tPyEval_EvalCode\n\t\n\t\n\t\n\t_PyRun_SimpleFileObject\n\t_PyRun_AnyFileObject\n\tPy_RunMain\n\tPy_BytesMain\n\t__libc_start_main\n\t\n*** End stack trace ***\nUnknown PJRT_DEVICE 'CUDA'"
            },
            "pytorch_torch_compile_tvm": {
                "error": "backend='tvm' raised:\nModuleNotFoundError: No module named 'tvm'\n\nSet TORCH_LOGS=\"+dynamo\" and TORCHDYNAMO_VERBOSE=1 for more information\n\n\nYou can suppress this exception and fall back to eager by setting:\n    import torch._dynamo\n    torch._dynamo.config.suppress_errors = True\n"
            },
            "jax_flax_xla": {
                "ttft_s": 11.052516222000122
            },
            "tensorflow_eager": {
                "ttft_s": 1.5242037773132324
            },
            "tensorflow_xla": {
                "ttft_s": 15.405196905136108
            },
            "jax_flax_eager": {
                "ttft_s": 11.940893650054932
            }
        }
    },
    {
        "BartForConditionalGeneration": {
            "error": "AttributeError(\"'BartForConditionalGeneration' object has no attribute '_shift_right'\")"
        }
    },
    {
        "BertForMaskedLM": {
            "pytorch_torch_compile_inductor": {
                "ttft_s": 10.369994163513184
            },
            "pytorch_torch_compile_eager": {
                "ttft_s": 2.9472644329071045
            },
            "pytorch_torch_compile_cudagraphs": {
                "ttft_s": 8.626973390579224
            },
            "pytorch_torch_compile_onnxrt": {
                "ttft_s": 12.256417989730835
            },
            "pytorch_torch_compile_openxla": {
                "error": "torch_xla/csrc/runtime/pjrt_registry.cc:214 : Check failed: client \n*** Begin stack trace ***\n\ttsl::CurrentStackTrace()\n\ttorch_xla::runtime::InitializePjRt(std::string const&)\n\ttorch_xla::runtime::PjRtComputationClient::PjRtComputationClient()\n\t\n\ttorch_xla::runtime::GetComputationClient()\n\ttorch_xla::bridge::GetDefaultDevice()\n\ttorch_xla::bridge::GetCurrentDevice()\n\ttorch_xla::bridge::GetCurrentAtenDevice()\n\t\n\t\n\t\n\t_PyObject_MakeTpCall\n\t_PyEval_EvalFrameDefault\n\t\n\t_PyFunction_Vectorcall\n\t_PyEval_EvalFrameDefault\n\t\n\t_PyFunction_Vectorcall\n\t_PyEval_EvalFrameDefault\n\t\n\t_PyFunction_Vectorcall\n\t_PyEval_EvalFrameDefault\n\t\n\t_PyFunction_Vectorcall\n\t_PyEval_EvalFrameDefault\n\t\n\t_PyFunction_Vectorcall\n\t_PyEval_EvalFrameDefault\n\t\n\t_PyFunction_Vectorcall\n\t_PyEval_EvalFrameDefault\n\t\n\t_PyFunction_Vectorcall\n\t_PyEval_EvalFrameDefault\n\t\n\t_PyFunction_Vectorcall\n\t_PyEval_EvalFrameDefault\n\t\n\t_PyFunction_Vectorcall\n\tTHPFunction_apply(_object*, _object*)\n\t\n\tPyObject_Call\n\t_PyEval_EvalFrameDefault\n\t\n\t\n\t_PyEval_EvalFrameDefault\n\t\n\t_PyFunction_Vectorcall\n\t_PyEval_EvalFrameDefault\n\t\n\t_PyFunction_Vectorcall\n\t_PyEval_EvalFrameDefault\n\t\n\t_PyFunction_Vectorcall\n\t_PyEval_EvalFrameDefault\n\t\n\t_PyFunction_Vectorcall\n\t_PyEval_EvalFrameDefault\n\t\n\t_PyFunction_Vectorcall\n\t_PyEval_EvalFrameDefault\n\t\n\t\n\t\n\tPyObject_Call\n\t_PyEval_EvalFrameDefault\n\t\n\t\n\tPyObject_Call\n\t_PyEval_EvalFrameDefault\n\t\n\t\n\tPyObject_Call\n\t_PyEval_EvalFrameDefault\n\t\n\t_PyFunction_Vectorcall\n\tPyObject_Call\n\t_PyEval_EvalFrameDefault\n\t\n\t\n\tPyObject_Call\n\t_PyEval_EvalFrameDefault\n\t\n\t_PyFunction_Vectorcall\n\t_PyObject_FastCallDictTstate\n\t_PyObject_Call_Prepend\n\t\n\tPyObject_Call\n\t_PyEval_EvalFrameDefault\n\t\n\t_PyFunction_Vectorcall\n\t_PyEval_EvalFrameDefault\n\t_PyFunction_Vectorcall\n\t_PyEval_EvalFrameDefault\n\t\n\tPyEval_EvalCode\n\t\n\t\n\t\n\t_PyRun_SimpleFileObject\n\t_PyRun_AnyFileObject\n\tPy_RunMain\n\tPy_BytesMain\n\t__libc_start_main\n\t\n*** End stack trace ***\nUnknown PJRT_DEVICE 'CUDA'"
            },
            "pytorch_torch_compile_tvm": {
                "error": "backend='tvm' raised:\nModuleNotFoundError: No module named 'tvm'\n\nSet TORCH_LOGS=\"+dynamo\" and TORCHDYNAMO_VERBOSE=1 for more information\n\n\nYou can suppress this exception and fall back to eager by setting:\n    import torch._dynamo\n    torch._dynamo.config.suppress_errors = True\n"
            },
            "jax_flax_xla": {
                "ttft_s": 12.581246852874756
            },
            "tensorflow_eager": {
                "ttft_s": 0.23584747314453125
            },
            "tensorflow_xla": {
                "ttft_s": 4.924352645874023
            },
            "jax_flax_eager": {
                "ttft_s": 12.220993041992188
            }
        }
    },
    {
        "BertForQuestionAnswering": {
            "pytorch_torch_compile_inductor": {
                "ttft_s": 10.056368350982666
            },
            "pytorch_torch_compile_eager": {
                "ttft_s": 4.6082611083984375
            },
            "pytorch_torch_compile_cudagraphs": {
                "ttft_s": 6.940948724746704
            },
            "pytorch_torch_compile_onnxrt": {
                "ttft_s": 13.713898658752441
            },
            "pytorch_torch_compile_openxla": {
                "error": "torch_xla/csrc/runtime/pjrt_registry.cc:214 : Check failed: client \n*** Begin stack trace ***\n\ttsl::CurrentStackTrace()\n\ttorch_xla::runtime::InitializePjRt(std::string const&)\n\ttorch_xla::runtime::PjRtComputationClient::PjRtComputationClient()\n\t\n\ttorch_xla::runtime::GetComputationClient()\n\ttorch_xla::bridge::GetDefaultDevice()\n\ttorch_xla::bridge::GetCurrentDevice()\n\ttorch_xla::bridge::GetCurrentAtenDevice()\n\t\n\t\n\t\n\t_PyObject_MakeTpCall\n\t_PyEval_EvalFrameDefault\n\t\n\t_PyFunction_Vectorcall\n\t_PyEval_EvalFrameDefault\n\t\n\t_PyFunction_Vectorcall\n\t_PyEval_EvalFrameDefault\n\t\n\t_PyFunction_Vectorcall\n\t_PyEval_EvalFrameDefault\n\t\n\t_PyFunction_Vectorcall\n\t_PyEval_EvalFrameDefault\n\t\n\t_PyFunction_Vectorcall\n\t_PyEval_EvalFrameDefault\n\t\n\t_PyFunction_Vectorcall\n\t_PyEval_EvalFrameDefault\n\t\n\t_PyFunction_Vectorcall\n\t_PyEval_EvalFrameDefault\n\t\n\t_PyFunction_Vectorcall\n\t_PyEval_EvalFrameDefault\n\t\n\t_PyFunction_Vectorcall\n\tTHPFunction_apply(_object*, _object*)\n\t\n\tPyObject_Call\n\t_PyEval_EvalFrameDefault\n\t\n\t\n\t_PyEval_EvalFrameDefault\n\t\n\t_PyFunction_Vectorcall\n\t_PyEval_EvalFrameDefault\n\t\n\t_PyFunction_Vectorcall\n\t_PyEval_EvalFrameDefault\n\t\n\t_PyFunction_Vectorcall\n\t_PyEval_EvalFrameDefault\n\t\n\t_PyFunction_Vectorcall\n\t_PyEval_EvalFrameDefault\n\t\n\t_PyFunction_Vectorcall\n\t_PyEval_EvalFrameDefault\n\t\n\t\n\t\n\tPyObject_Call\n\t_PyEval_EvalFrameDefault\n\t\n\t\n\tPyObject_Call\n\t_PyEval_EvalFrameDefault\n\t\n\t\n\tPyObject_Call\n\t_PyEval_EvalFrameDefault\n\t\n\t_PyFunction_Vectorcall\n\tPyObject_Call\n\t_PyEval_EvalFrameDefault\n\t\n\t\n\tPyObject_Call\n\t_PyEval_EvalFrameDefault\n\t\n\t_PyFunction_Vectorcall\n\t_PyObject_FastCallDictTstate\n\t_PyObject_Call_Prepend\n\t\n\tPyObject_Call\n\t_PyEval_EvalFrameDefault\n\t\n\t_PyFunction_Vectorcall\n\t_PyEval_EvalFrameDefault\n\t_PyFunction_Vectorcall\n\t_PyEval_EvalFrameDefault\n\t\n\tPyEval_EvalCode\n\t\n\t\n\t\n\t_PyRun_SimpleFileObject\n\t_PyRun_AnyFileObject\n\tPy_RunMain\n\tPy_BytesMain\n\t__libc_start_main\n\t\n*** End stack trace ***\nUnknown PJRT_DEVICE 'CUDA'"
            },
            "pytorch_torch_compile_tvm": {
                "error": "backend='tvm' raised:\nModuleNotFoundError: No module named 'tvm'\n\nSet TORCH_LOGS=\"+dynamo\" and TORCHDYNAMO_VERBOSE=1 for more information\n\n\nYou can suppress this exception and fall back to eager by setting:\n    import torch._dynamo\n    torch._dynamo.config.suppress_errors = True\n"
            },
            "jax_flax_xla": {
                "ttft_s": 4.112539052963257
            },
            "tensorflow_eager": {
                "ttft_s": 0.22531843185424805
            },
            "tensorflow_xla": {
                "ttft_s": 3.4175643920898438
            },
            "jax_flax_eager": {
                "ttft_s": 1.5365121364593506
            }
        }
    },
    {
        "BigBird": {
            "pytorch_torch_compile_inductor": {
                "ttft_s": 12.835593700408936
            },
            "pytorch_torch_compile_eager": {
                "ttft_s": 3.1251699924468994
            },
            "pytorch_torch_compile_cudagraphs": {
                "ttft_s": 10.413816213607788
            },
            "pytorch_torch_compile_onnxrt": {
                "ttft_s": 13.244446516036987
            },
            "pytorch_torch_compile_openxla": {
                "error": "torch_xla/csrc/runtime/pjrt_registry.cc:214 : Check failed: client \n*** Begin stack trace ***\n\ttsl::CurrentStackTrace()\n\ttorch_xla::runtime::InitializePjRt(std::string const&)\n\ttorch_xla::runtime::PjRtComputationClient::PjRtComputationClient()\n\t\n\ttorch_xla::runtime::GetComputationClient()\n\ttorch_xla::bridge::GetDefaultDevice()\n\ttorch_xla::bridge::GetCurrentDevice()\n\ttorch_xla::bridge::GetCurrentAtenDevice()\n\t\n\t\n\t\n\t_PyObject_MakeTpCall\n\t_PyEval_EvalFrameDefault\n\t\n\t_PyFunction_Vectorcall\n\t_PyEval_EvalFrameDefault\n\t\n\t_PyFunction_Vectorcall\n\t_PyEval_EvalFrameDefault\n\t\n\t_PyFunction_Vectorcall\n\t_PyEval_EvalFrameDefault\n\t\n\t_PyFunction_Vectorcall\n\t_PyEval_EvalFrameDefault\n\t\n\t_PyFunction_Vectorcall\n\t_PyEval_EvalFrameDefault\n\t\n\t_PyFunction_Vectorcall\n\t_PyEval_EvalFrameDefault\n\t\n\t_PyFunction_Vectorcall\n\t_PyEval_EvalFrameDefault\n\t\n\t_PyFunction_Vectorcall\n\t_PyEval_EvalFrameDefault\n\t\n\t_PyFunction_Vectorcall\n\tTHPFunction_apply(_object*, _object*)\n\t\n\tPyObject_Call\n\t_PyEval_EvalFrameDefault\n\t\n\t\n\t_PyEval_EvalFrameDefault\n\t\n\t_PyFunction_Vectorcall\n\t_PyEval_EvalFrameDefault\n\t\n\t_PyFunction_Vectorcall\n\t_PyEval_EvalFrameDefault\n\t\n\t_PyFunction_Vectorcall\n\t_PyEval_EvalFrameDefault\n\t\n\t_PyFunction_Vectorcall\n\t_PyEval_EvalFrameDefault\n\t\n\t_PyFunction_Vectorcall\n\t_PyEval_EvalFrameDefault\n\t\n\t\n\t\n\tPyObject_Call\n\t_PyEval_EvalFrameDefault\n\t\n\t\n\tPyObject_Call\n\t_PyEval_EvalFrameDefault\n\t\n\t\n\tPyObject_Call\n\t_PyEval_EvalFrameDefault\n\t\n\t_PyFunction_Vectorcall\n\tPyObject_Call\n\t_PyEval_EvalFrameDefault\n\t\n\t\n\tPyObject_Call\n\t_PyEval_EvalFrameDefault\n\t\n\t_PyFunction_Vectorcall\n\t_PyObject_FastCallDictTstate\n\t_PyObject_Call_Prepend\n\t\n\tPyObject_Call\n\t_PyEval_EvalFrameDefault\n\t\n\t_PyFunction_Vectorcall\n\t_PyEval_EvalFrameDefault\n\t_PyFunction_Vectorcall\n\t_PyEval_EvalFrameDefault\n\t\n\tPyEval_EvalCode\n\t\n\t\n\t\n\t_PyRun_SimpleFileObject\n\t_PyRun_AnyFileObject\n\tPy_RunMain\n\tPy_BytesMain\n\t__libc_start_main\n\t\n*** End stack trace ***\nUnknown PJRT_DEVICE 'CUDA'"
            },
            "pytorch_torch_compile_tvm": {
                "error": "backend='tvm' raised:\nModuleNotFoundError: No module named 'tvm'\n\nSet TORCH_LOGS=\"+dynamo\" and TORCHDYNAMO_VERBOSE=1 for more information\n\n\nYou can suppress this exception and fall back to eager by setting:\n    import torch._dynamo\n    torch._dynamo.config.suppress_errors = True\n"
            },
            "jax_flax_xla": {
                "error": "Sequence length must be multiple of block size, but sequence length is 12, while block size is 64."
            },
            "tensorflow_eager": {
                "error": "Unrecognized configuration class <class 'transformers.models.big_bird.configuration_big_bird.BigBirdConfig'> for this kind of AutoModel: TFAutoModel.\nModel type should be one of AlbertConfig, BartConfig, BertConfig, BlenderbotConfig, BlenderbotSmallConfig, BlipConfig, CamembertConfig, CLIPConfig, ConvBertConfig, ConvNextConfig, ConvNextV2Config, CTRLConfig, CvtConfig, Data2VecVisionConfig, DebertaConfig, DebertaV2Config, DeiTConfig, DistilBertConfig, DPRConfig, EfficientFormerConfig, ElectraConfig, EsmConfig, FlaubertConfig, FunnelConfig, GPT2Config, GPT2Config, GPTJConfig, GroupViTConfig, HubertConfig, IdeficsConfig, LayoutLMConfig, LayoutLMv3Config, LEDConfig, LongformerConfig, LxmertConfig, MarianConfig, MBartConfig, MistralConfig, MobileBertConfig, MobileViTConfig, MPNetConfig, MT5Config, OpenAIGPTConfig, OPTConfig, PegasusConfig, RegNetConfig, RemBertConfig, ResNetConfig, RobertaConfig, RobertaPreLayerNormConfig, RoFormerConfig, SamConfig, SegformerConfig, Speech2TextConfig, SwiftFormerConfig, SwinConfig, T5Config, TapasConfig, TransfoXLConfig, VisionTextDualEncoderConfig, ViTConfig, ViTMAEConfig, Wav2Vec2Config, WhisperConfig, XGLMConfig, XLMConfig, XLMRobertaConfig, XLNetConfig."
            },
            "tensorflow_xla": {
                "error": "Unrecognized configuration class <class 'transformers.models.big_bird.configuration_big_bird.BigBirdConfig'> for this kind of AutoModel: TFAutoModel.\nModel type should be one of AlbertConfig, BartConfig, BertConfig, BlenderbotConfig, BlenderbotSmallConfig, BlipConfig, CamembertConfig, CLIPConfig, ConvBertConfig, ConvNextConfig, ConvNextV2Config, CTRLConfig, CvtConfig, Data2VecVisionConfig, DebertaConfig, DebertaV2Config, DeiTConfig, DistilBertConfig, DPRConfig, EfficientFormerConfig, ElectraConfig, EsmConfig, FlaubertConfig, FunnelConfig, GPT2Config, GPT2Config, GPTJConfig, GroupViTConfig, HubertConfig, IdeficsConfig, LayoutLMConfig, LayoutLMv3Config, LEDConfig, LongformerConfig, LxmertConfig, MarianConfig, MBartConfig, MistralConfig, MobileBertConfig, MobileViTConfig, MPNetConfig, MT5Config, OpenAIGPTConfig, OPTConfig, PegasusConfig, RegNetConfig, RemBertConfig, ResNetConfig, RobertaConfig, RobertaPreLayerNormConfig, RoFormerConfig, SamConfig, SegformerConfig, Speech2TextConfig, SwiftFormerConfig, SwinConfig, T5Config, TapasConfig, TransfoXLConfig, VisionTextDualEncoderConfig, ViTConfig, ViTMAEConfig, Wav2Vec2Config, WhisperConfig, XGLMConfig, XLMConfig, XLMRobertaConfig, XLNetConfig."
            }
        }
    },
    {
        "BlenderbotForCausalLM": {
            "pytorch_torch_compile_inductor": {
                "ttft_s": 9.136570453643799
            },
            "pytorch_torch_compile_eager": {
                "ttft_s": 1.9596989154815674
            },
            "pytorch_torch_compile_cudagraphs": {
                "ttft_s": 6.671370267868042
            },
            "pytorch_torch_compile_onnxrt": {
                "ttft_s": 14.74839997291565
            },
            "pytorch_torch_compile_openxla": {
                "error": "torch_xla/csrc/runtime/pjrt_registry.cc:214 : Check failed: client \n*** Begin stack trace ***\n\ttsl::CurrentStackTrace()\n\ttorch_xla::runtime::InitializePjRt(std::string const&)\n\ttorch_xla::runtime::PjRtComputationClient::PjRtComputationClient()\n\t\n\ttorch_xla::runtime::GetComputationClient()\n\ttorch_xla::bridge::GetDefaultDevice()\n\ttorch_xla::bridge::GetCurrentDevice()\n\ttorch_xla::bridge::GetCurrentAtenDevice()\n\t\n\t\n\t\n\t_PyObject_MakeTpCall\n\t_PyEval_EvalFrameDefault\n\t\n\t_PyFunction_Vectorcall\n\t_PyEval_EvalFrameDefault\n\t\n\t_PyFunction_Vectorcall\n\t_PyEval_EvalFrameDefault\n\t\n\t_PyFunction_Vectorcall\n\t_PyEval_EvalFrameDefault\n\t\n\t_PyFunction_Vectorcall\n\t_PyEval_EvalFrameDefault\n\t\n\t_PyFunction_Vectorcall\n\t_PyEval_EvalFrameDefault\n\t\n\t_PyFunction_Vectorcall\n\t_PyEval_EvalFrameDefault\n\t\n\t_PyFunction_Vectorcall\n\t_PyEval_EvalFrameDefault\n\t\n\t_PyFunction_Vectorcall\n\t_PyEval_EvalFrameDefault\n\t\n\t_PyFunction_Vectorcall\n\tTHPFunction_apply(_object*, _object*)\n\t\n\tPyObject_Call\n\t_PyEval_EvalFrameDefault\n\t\n\t\n\t_PyEval_EvalFrameDefault\n\t\n\t_PyFunction_Vectorcall\n\t_PyEval_EvalFrameDefault\n\t\n\t_PyFunction_Vectorcall\n\t_PyEval_EvalFrameDefault\n\t\n\t_PyFunction_Vectorcall\n\t_PyEval_EvalFrameDefault\n\t\n\t_PyFunction_Vectorcall\n\t_PyEval_EvalFrameDefault\n\t\n\t_PyFunction_Vectorcall\n\t_PyEval_EvalFrameDefault\n\t\n\t\n\t\n\tPyObject_Call\n\t_PyEval_EvalFrameDefault\n\t\n\t\n\tPyObject_Call\n\t_PyEval_EvalFrameDefault\n\t\n\t\n\tPyObject_Call\n\t_PyEval_EvalFrameDefault\n\t\n\t_PyFunction_Vectorcall\n\tPyObject_Call\n\t_PyEval_EvalFrameDefault\n\t\n\t\n\tPyObject_Call\n\t_PyEval_EvalFrameDefault\n\t\n\t_PyFunction_Vectorcall\n\t_PyObject_FastCallDictTstate\n\t_PyObject_Call_Prepend\n\t\n\tPyObject_Call\n\t_PyEval_EvalFrameDefault\n\t\n\t_PyFunction_Vectorcall\n\t_PyEval_EvalFrameDefault\n\t_PyFunction_Vectorcall\n\t_PyEval_EvalFrameDefault\n\t\n\tPyEval_EvalCode\n\t\n\t\n\t\n\t_PyRun_SimpleFileObject\n\t_PyRun_AnyFileObject\n\tPy_RunMain\n\tPy_BytesMain\n\t__libc_start_main\n\t\n*** End stack trace ***\nUnknown PJRT_DEVICE 'CUDA'"
            },
            "pytorch_torch_compile_tvm": {
                "error": "backend='tvm' raised:\nModuleNotFoundError: No module named 'tvm'\n\nSet TORCH_LOGS=\"+dynamo\" and TORCHDYNAMO_VERBOSE=1 for more information\n\n\nYou can suppress this exception and fall back to eager by setting:\n    import torch._dynamo\n    torch._dynamo.config.suppress_errors = True\n"
            },
            "jax_flax_xla": {
                "ttft_s": 14.499439716339111
            },
            "tensorflow_eager": {
                "error": "Exception encountered when calling layer 'decoder' (type TFBlenderbotDecoder).\n\nYou have to specify either decoder_input_ids or decoder_inputs_embeds\n\nCall arguments received by layer 'decoder' (type TFBlenderbotDecoder):\n  \u2022 input_ids=None\n  \u2022 inputs_embeds=None\n  \u2022 attention_mask=None\n  \u2022 position_ids=None\n  \u2022 encoder_hidden_states=tf.Tensor(shape=(1, 32, 1280), dtype=float32)\n  \u2022 encoder_attention_mask=tf.Tensor(shape=(1, 32), dtype=int32)\n  \u2022 head_mask=None\n  \u2022 cross_attn_head_mask=None\n  \u2022 past_key_values=None\n  \u2022 use_cache=True\n  \u2022 output_attentions=False\n  \u2022 output_hidden_states=False\n  \u2022 return_dict=True\n  \u2022 training=False"
            },
            "tensorflow_xla": {
                "error": "in user code:\n\n    File \"/home/mmoffatt/jax_vs_pytorch/benchmark_ttft.py\", line 176, in fwd  *\n        return model(**inp)[0]\n    File \"/opt/conda/envs/xla-env/lib/python3.10/site-packages/tf_keras/src/utils/traceback_utils.py\", line 70, in error_handler  **\n        raise e.with_traceback(filtered_tb) from None\n    File \"/var/tmp/__autograph_generated_file138ue72o.py\", line 37, in tf__run_call_with_unpacked_inputs\n        retval_ = ag__.converted_call(ag__.ld(func), (ag__.ld(self),), dict(**ag__.ld(unpacked_inputs)), fscope)\n    File \"/var/tmp/__autograph_generated_filei5wyc0yd.py\", line 11, in tf__call\n        outputs = ag__.converted_call(ag__.ld(self).model, (), dict(input_ids=ag__.ld(input_ids), attention_mask=ag__.ld(attention_mask), decoder_input_ids=ag__.ld(decoder_input_ids), decoder_attention_mask=ag__.ld(decoder_attention_mask), decoder_position_ids=ag__.ld(decoder_position_ids), head_mask=ag__.ld(head_mask), decoder_head_mask=ag__.ld(decoder_head_mask), cross_attn_head_mask=ag__.ld(cross_attn_head_mask), encoder_outputs=ag__.ld(encoder_outputs), past_key_values=ag__.ld(past_key_values), inputs_embeds=ag__.ld(inputs_embeds), decoder_inputs_embeds=ag__.ld(decoder_inputs_embeds), use_cache=ag__.ld(use_cache), output_attentions=ag__.ld(output_attentions), output_hidden_states=ag__.ld(output_hidden_states), return_dict=ag__.ld(return_dict), training=ag__.ld(training)), fscope)\n    File \"/var/tmp/__autograph_generated_file138ue72o.py\", line 37, in tf__run_call_with_unpacked_inputs\n        retval_ = ag__.converted_call(ag__.ld(func), (ag__.ld(self),), dict(**ag__.ld(unpacked_inputs)), fscope)\n    File \"/var/tmp/__autograph_generated_filezdeih75x.py\", line 58, in tf__call\n        decoder_outputs = ag__.converted_call(ag__.ld(self).decoder, (ag__.ld(decoder_input_ids),), dict(attention_mask=ag__.ld(decoder_attention_mask), position_ids=ag__.ld(decoder_position_ids), encoder_hidden_states=ag__.ld(encoder_outputs)[0], encoder_attention_mask=ag__.ld(attention_mask), head_mask=ag__.ld(decoder_head_mask), cross_attn_head_mask=ag__.ld(cross_attn_head_mask), past_key_values=ag__.ld(past_key_values), inputs_embeds=ag__.ld(decoder_inputs_embeds), use_cache=ag__.ld(use_cache), output_attentions=ag__.ld(output_attentions), output_hidden_states=ag__.ld(output_hidden_states), return_dict=ag__.ld(return_dict), training=ag__.ld(training)), fscope)\n    File \"/var/tmp/__autograph_generated_file138ue72o.py\", line 37, in tf__run_call_with_unpacked_inputs\n        retval_ = ag__.converted_call(ag__.ld(func), (ag__.ld(self),), dict(**ag__.ld(unpacked_inputs)), fscope)\n    File \"/var/tmp/__autograph_generated_file5lcjad4c.py\", line 128, in tf__call\n        ag__.if_stmt(ag__.and_(lambda : ag__.ld(input_ids) is not None, lambda : ag__.ld(inputs_embeds) is not None), if_body_2, else_body_2, get_state_2, set_state_2, ('input_shape',), 1)\n    File \"/var/tmp/__autograph_generated_file5lcjad4c.py\", line 126, in else_body_2\n        ag__.if_stmt(ag__.ld(input_ids) is not None, if_body_1, else_body_1, get_state_1, set_state_1, ('input_shape',), 1)\n    File \"/var/tmp/__autograph_generated_file5lcjad4c.py\", line 124, in else_body_1\n        ag__.if_stmt(ag__.ld(inputs_embeds) is not None, if_body, else_body, get_state, set_state, ('input_shape',), 1)\n    File \"/var/tmp/__autograph_generated_file5lcjad4c.py\", line 122, in else_body\n        raise ag__.converted_call(ag__.ld(ValueError), ('You have to specify either decoder_input_ids or decoder_inputs_embeds',), None, fscope)\n\n    ValueError: Exception encountered when calling layer 'tf_blenderbot_model_1' (type TFBlenderbotModel).\n    \n    in user code:\n    \n        File \"/opt/conda/envs/xla-env/lib/python3.10/site-packages/transformers/modeling_tf_utils.py\", line 1079, in run_call_with_unpacked_inputs  *\n            return func(self, **unpacked_inputs)\n        File \"/opt/conda/envs/xla-env/lib/python3.10/site-packages/transformers/models/blenderbot/modeling_tf_blenderbot.py\", line 1276, in call  *\n            outputs = self.model(\n        File \"/opt/conda/envs/xla-env/lib/python3.10/site-packages/tf_keras/src/utils/traceback_utils.py\", line 70, in error_handler  **\n            raise e.with_traceback(filtered_tb) from None\n        File \"/var/tmp/__autograph_generated_file138ue72o.py\", line 37, in tf__run_call_with_unpacked_inputs\n            retval_ = ag__.converted_call(ag__.ld(func), (ag__.ld(self),), dict(**ag__.ld(unpacked_inputs)), fscope)\n        File \"/var/tmp/__autograph_generated_filezdeih75x.py\", line 58, in tf__call\n            decoder_outputs = ag__.converted_call(ag__.ld(self).decoder, (ag__.ld(decoder_input_ids),), dict(attention_mask=ag__.ld(decoder_attention_mask), position_ids=ag__.ld(decoder_position_ids), encoder_hidden_states=ag__.ld(encoder_outputs)[0], encoder_attention_mask=ag__.ld(attention_mask), head_mask=ag__.ld(decoder_head_mask), cross_attn_head_mask=ag__.ld(cross_attn_head_mask), past_key_values=ag__.ld(past_key_values), inputs_embeds=ag__.ld(decoder_inputs_embeds), use_cache=ag__.ld(use_cache), output_attentions=ag__.ld(output_attentions), output_hidden_states=ag__.ld(output_hidden_states), return_dict=ag__.ld(return_dict), training=ag__.ld(training)), fscope)\n        File \"/var/tmp/__autograph_generated_file138ue72o.py\", line 37, in tf__run_call_with_unpacked_inputs\n            retval_ = ag__.converted_call(ag__.ld(func), (ag__.ld(self),), dict(**ag__.ld(unpacked_inputs)), fscope)\n        File \"/var/tmp/__autograph_generated_file5lcjad4c.py\", line 128, in tf__call\n            ag__.if_stmt(ag__.and_(lambda : ag__.ld(input_ids) is not None, lambda : ag__.ld(inputs_embeds) is not None), if_body_2, else_body_2, get_state_2, set_state_2, ('input_shape',), 1)\n        File \"/var/tmp/__autograph_generated_file5lcjad4c.py\", line 126, in else_body_2\n            ag__.if_stmt(ag__.ld(input_ids) is not None, if_body_1, else_body_1, get_state_1, set_state_1, ('input_shape',), 1)\n        File \"/var/tmp/__autograph_generated_file5lcjad4c.py\", line 124, in else_body_1\n            ag__.if_stmt(ag__.ld(inputs_embeds) is not None, if_body, else_body, get_state, set_state, ('input_shape',), 1)\n        File \"/var/tmp/__autograph_generated_file5lcjad4c.py\", line 122, in else_body\n            raise ag__.converted_call(ag__.ld(ValueError), ('You have to specify either decoder_input_ids or decoder_inputs_embeds',), None, fscope)\n    \n        ValueError: Exception encountered when calling layer 'model' (type TFBlenderbotMainLayer).\n        \n        in user code:\n        \n            File \"/opt/conda/envs/xla-env/lib/python3.10/site-packages/transformers/modeling_tf_utils.py\", line 1079, in run_call_with_unpacked_inputs  *\n                return func(self, **unpacked_inputs)\n            File \"/opt/conda/envs/xla-env/lib/python3.10/site-packages/transformers/models/blenderbot/modeling_tf_blenderbot.py\", line 1168, in call  *\n                decoder_outputs = self.decoder(\n            File \"/opt/conda/envs/xla-env/lib/python3.10/site-packages/tf_keras/src/utils/traceback_utils.py\", line 70, in error_handler  **\n                raise e.with_traceback(filtered_tb) from None\n            File \"/var/tmp/__autograph_generated_file138ue72o.py\", line 37, in tf__run_call_with_unpacked_inputs\n                retval_ = ag__.converted_call(ag__.ld(func), (ag__.ld(self),), dict(**ag__.ld(unpacked_inputs)), fscope)\n            File \"/var/tmp/__autograph_generated_file5lcjad4c.py\", line 128, in tf__call\n                ag__.if_stmt(ag__.and_(lambda : ag__.ld(input_ids) is not None, lambda : ag__.ld(inputs_embeds) is not None), if_body_2, else_body_2, get_state_2, set_state_2, ('input_shape',), 1)\n            File \"/var/tmp/__autograph_generated_file5lcjad4c.py\", line 126, in else_body_2\n                ag__.if_stmt(ag__.ld(input_ids) is not None, if_body_1, else_body_1, get_state_1, set_state_1, ('input_shape',), 1)\n            File \"/var/tmp/__autograph_generated_file5lcjad4c.py\", line 124, in else_body_1\n                ag__.if_stmt(ag__.ld(inputs_embeds) is not None, if_body, else_body, get_state, set_state, ('input_shape',), 1)\n            File \"/var/tmp/__autograph_generated_file5lcjad4c.py\", line 122, in else_body\n                raise ag__.converted_call(ag__.ld(ValueError), ('You have to specify either decoder_input_ids or decoder_inputs_embeds',), None, fscope)\n        \n            ValueError: Exception encountered when calling layer 'decoder' (type TFBlenderbotDecoder).\n            \n            in user code:\n            \n                File \"/opt/conda/envs/xla-env/lib/python3.10/site-packages/transformers/modeling_tf_utils.py\", line 1079, in run_call_with_unpacked_inputs  *\n                    return func(self, **unpacked_inputs)\n                File \"/opt/conda/envs/xla-env/lib/python3.10/site-packages/transformers/models/blenderbot/modeling_tf_blenderbot.py\", line 979, in call  *\n                    raise ValueError(\"You have to specify either decoder_input_ids or decoder_inputs_embeds\")\n            \n                ValueError: You have to specify either decoder_input_ids or decoder_inputs_embeds\n            \n            \n            Call arguments received by layer 'decoder' (type TFBlenderbotDecoder):\n              \u2022 input_ids=None\n              \u2022 inputs_embeds=None\n              \u2022 attention_mask=None\n              \u2022 position_ids=None\n              \u2022 encoder_hidden_states=tf.Tensor(shape=(1, 32, 1280), dtype=float32)\n              \u2022 encoder_attention_mask=tf.Tensor(shape=(1, 32), dtype=int32)\n              \u2022 head_mask=None\n              \u2022 cross_attn_head_mask=None\n              \u2022 past_key_values=None\n              \u2022 use_cache=True\n              \u2022 output_attentions=False\n              \u2022 output_hidden_states=False\n              \u2022 return_dict=True\n              \u2022 training=False\n        \n        \n        Call arguments received by layer 'model' (type TFBlenderbotMainLayer):\n          \u2022 input_ids=tf.Tensor(shape=(1, 32), dtype=int32)\n          \u2022 attention_mask=tf.Tensor(shape=(1, 32), dtype=int32)\n          \u2022 decoder_input_ids=None\n          \u2022 decoder_attention_mask=None\n          \u2022 decoder_position_ids=None\n          \u2022 head_mask=None\n          \u2022 decoder_head_mask=None\n          \u2022 cross_attn_head_mask=None\n          \u2022 encoder_outputs=None\n          \u2022 past_key_values=None\n          \u2022 inputs_embeds=None\n          \u2022 decoder_inputs_embeds=None\n          \u2022 use_cache=True\n          \u2022 output_attentions=False\n          \u2022 output_hidden_states=False\n          \u2022 return_dict=True\n          \u2022 training=False\n          \u2022 kwargs=<class 'inspect._empty'>\n    \n    \n    Call arguments received by layer 'tf_blenderbot_model_1' (type TFBlenderbotModel):\n      \u2022 input_ids=tf.Tensor(shape=(1, 32), dtype=int32)\n      \u2022 attention_mask=tf.Tensor(shape=(1, 32), dtype=int32)\n      \u2022 decoder_input_ids=None\n      \u2022 decoder_attention_mask=None\n      \u2022 decoder_position_ids=None\n      \u2022 head_mask=None\n      \u2022 decoder_head_mask=None\n      \u2022 cross_attn_head_mask=None\n      \u2022 encoder_outputs=None\n      \u2022 past_key_values=None\n      \u2022 inputs_embeds=None\n      \u2022 decoder_inputs_embeds=None\n      \u2022 use_cache=None\n      \u2022 output_attentions=None\n      \u2022 output_hidden_states=None\n      \u2022 return_dict=None\n      \u2022 training=False\n      \u2022 kwargs=<class 'inspect._empty'>\n"
            },
            "jax_flax_eager": {
                "ttft_s": 10.94302248954773
            }
        }
    },
    {
        "BlenderbotSmallForCausalLM": {
            "pytorch_torch_compile_inductor": {
                "ttft_s": 9.610023975372314
            },
            "pytorch_torch_compile_eager": {
                "ttft_s": 1.3753547668457031
            },
            "pytorch_torch_compile_cudagraphs": {
                "ttft_s": 4.716509103775024
            },
            "pytorch_torch_compile_onnxrt": {
                "ttft_s": 7.943328142166138
            },
            "pytorch_torch_compile_openxla": {
                "error": "torch_xla/csrc/runtime/pjrt_registry.cc:214 : Check failed: client \n*** Begin stack trace ***\n\ttsl::CurrentStackTrace()\n\ttorch_xla::runtime::InitializePjRt(std::string const&)\n\ttorch_xla::runtime::PjRtComputationClient::PjRtComputationClient()\n\t\n\ttorch_xla::runtime::GetComputationClient()\n\ttorch_xla::bridge::GetDefaultDevice()\n\ttorch_xla::bridge::GetCurrentDevice()\n\ttorch_xla::bridge::GetCurrentAtenDevice()\n\t\n\t\n\t\n\t_PyObject_MakeTpCall\n\t_PyEval_EvalFrameDefault\n\t\n\t_PyFunction_Vectorcall\n\t_PyEval_EvalFrameDefault\n\t\n\t_PyFunction_Vectorcall\n\t_PyEval_EvalFrameDefault\n\t\n\t_PyFunction_Vectorcall\n\t_PyEval_EvalFrameDefault\n\t\n\t_PyFunction_Vectorcall\n\t_PyEval_EvalFrameDefault\n\t\n\t_PyFunction_Vectorcall\n\t_PyEval_EvalFrameDefault\n\t\n\t_PyFunction_Vectorcall\n\t_PyEval_EvalFrameDefault\n\t\n\t_PyFunction_Vectorcall\n\t_PyEval_EvalFrameDefault\n\t\n\t_PyFunction_Vectorcall\n\t_PyEval_EvalFrameDefault\n\t\n\t_PyFunction_Vectorcall\n\tTHPFunction_apply(_object*, _object*)\n\t\n\tPyObject_Call\n\t_PyEval_EvalFrameDefault\n\t\n\t\n\t_PyEval_EvalFrameDefault\n\t\n\t_PyFunction_Vectorcall\n\t_PyEval_EvalFrameDefault\n\t\n\t_PyFunction_Vectorcall\n\t_PyEval_EvalFrameDefault\n\t\n\t_PyFunction_Vectorcall\n\t_PyEval_EvalFrameDefault\n\t\n\t_PyFunction_Vectorcall\n\t_PyEval_EvalFrameDefault\n\t\n\t_PyFunction_Vectorcall\n\t_PyEval_EvalFrameDefault\n\t\n\t\n\t\n\tPyObject_Call\n\t_PyEval_EvalFrameDefault\n\t\n\t\n\tPyObject_Call\n\t_PyEval_EvalFrameDefault\n\t\n\t\n\tPyObject_Call\n\t_PyEval_EvalFrameDefault\n\t\n\t_PyFunction_Vectorcall\n\tPyObject_Call\n\t_PyEval_EvalFrameDefault\n\t\n\t\n\tPyObject_Call\n\t_PyEval_EvalFrameDefault\n\t\n\t_PyFunction_Vectorcall\n\t_PyObject_FastCallDictTstate\n\t_PyObject_Call_Prepend\n\t\n\tPyObject_Call\n\t_PyEval_EvalFrameDefault\n\t\n\t_PyFunction_Vectorcall\n\t_PyEval_EvalFrameDefault\n\t_PyFunction_Vectorcall\n\t_PyEval_EvalFrameDefault\n\t\n\tPyEval_EvalCode\n\t\n\t\n\t\n\t_PyRun_SimpleFileObject\n\t_PyRun_AnyFileObject\n\tPy_RunMain\n\tPy_BytesMain\n\t__libc_start_main\n\t\n*** End stack trace ***\nUnknown PJRT_DEVICE 'CUDA'"
            },
            "pytorch_torch_compile_tvm": {
                "error": "backend='tvm' raised:\nModuleNotFoundError: No module named 'tvm'\n\nSet TORCH_LOGS=\"+dynamo\" and TORCHDYNAMO_VERBOSE=1 for more information\n\n\nYou can suppress this exception and fall back to eager by setting:\n    import torch._dynamo\n    torch._dynamo.config.suppress_errors = True\n"
            },
            "jax_flax_xla": {
                "ttft_s": 14.000957250595093
            },
            "tensorflow_eager": {
                "error": "Exception encountered when calling layer 'decoder' (type TFBlenderbotSmallDecoder).\n\nYou have to specify either decoder_input_ids or decoder_inputs_embeds\n\nCall arguments received by layer 'decoder' (type TFBlenderbotSmallDecoder):\n  \u2022 input_ids=None\n  \u2022 inputs_embeds=None\n  \u2022 attention_mask=None\n  \u2022 position_ids=None\n  \u2022 encoder_hidden_states=tf.Tensor(shape=(1, 32, 512), dtype=float32)\n  \u2022 encoder_attention_mask=tf.Tensor(shape=(1, 32), dtype=int32)\n  \u2022 head_mask=None\n  \u2022 cross_attn_head_mask=None\n  \u2022 past_key_values=None\n  \u2022 use_cache=True\n  \u2022 output_attentions=False\n  \u2022 output_hidden_states=False\n  \u2022 return_dict=True\n  \u2022 training=False"
            },
            "tensorflow_xla": {
                "error": "in user code:\n\n    File \"/home/mmoffatt/jax_vs_pytorch/benchmark_ttft.py\", line 176, in fwd  *\n        return model(**inp)[0]\n    File \"/opt/conda/envs/xla-env/lib/python3.10/site-packages/tf_keras/src/utils/traceback_utils.py\", line 70, in error_handler  **\n        raise e.with_traceback(filtered_tb) from None\n    File \"/var/tmp/__autograph_generated_file138ue72o.py\", line 37, in tf__run_call_with_unpacked_inputs\n        retval_ = ag__.converted_call(ag__.ld(func), (ag__.ld(self),), dict(**ag__.ld(unpacked_inputs)), fscope)\n    File \"/var/tmp/__autograph_generated_filepjw4mvx0.py\", line 11, in tf__call\n        outputs = ag__.converted_call(ag__.ld(self).model, (), dict(input_ids=ag__.ld(input_ids), attention_mask=ag__.ld(attention_mask), decoder_input_ids=ag__.ld(decoder_input_ids), decoder_attention_mask=ag__.ld(decoder_attention_mask), decoder_position_ids=ag__.ld(decoder_position_ids), head_mask=ag__.ld(head_mask), decoder_head_mask=ag__.ld(decoder_head_mask), cross_attn_head_mask=ag__.ld(cross_attn_head_mask), encoder_outputs=ag__.ld(encoder_outputs), past_key_values=ag__.ld(past_key_values), inputs_embeds=ag__.ld(inputs_embeds), decoder_inputs_embeds=ag__.ld(decoder_inputs_embeds), use_cache=ag__.ld(use_cache), output_attentions=ag__.ld(output_attentions), output_hidden_states=ag__.ld(output_hidden_states), return_dict=ag__.ld(return_dict), training=ag__.ld(training)), fscope)\n    File \"/var/tmp/__autograph_generated_file138ue72o.py\", line 37, in tf__run_call_with_unpacked_inputs\n        retval_ = ag__.converted_call(ag__.ld(func), (ag__.ld(self),), dict(**ag__.ld(unpacked_inputs)), fscope)\n    File \"/var/tmp/__autograph_generated_filef0o968j1.py\", line 58, in tf__call\n        decoder_outputs = ag__.converted_call(ag__.ld(self).decoder, (ag__.ld(decoder_input_ids),), dict(attention_mask=ag__.ld(decoder_attention_mask), position_ids=ag__.ld(decoder_position_ids), encoder_hidden_states=ag__.ld(encoder_outputs)[0], encoder_attention_mask=ag__.ld(attention_mask), head_mask=ag__.ld(decoder_head_mask), cross_attn_head_mask=ag__.ld(cross_attn_head_mask), past_key_values=ag__.ld(past_key_values), inputs_embeds=ag__.ld(decoder_inputs_embeds), use_cache=ag__.ld(use_cache), output_attentions=ag__.ld(output_attentions), output_hidden_states=ag__.ld(output_hidden_states), return_dict=ag__.ld(return_dict), training=ag__.ld(training)), fscope)\n    File \"/var/tmp/__autograph_generated_file138ue72o.py\", line 37, in tf__run_call_with_unpacked_inputs\n        retval_ = ag__.converted_call(ag__.ld(func), (ag__.ld(self),), dict(**ag__.ld(unpacked_inputs)), fscope)\n    File \"/var/tmp/__autograph_generated_filez13q0744.py\", line 128, in tf__call\n        ag__.if_stmt(ag__.and_(lambda : ag__.ld(input_ids) is not None, lambda : ag__.ld(inputs_embeds) is not None), if_body_2, else_body_2, get_state_2, set_state_2, ('input_shape',), 1)\n    File \"/var/tmp/__autograph_generated_filez13q0744.py\", line 126, in else_body_2\n        ag__.if_stmt(ag__.ld(input_ids) is not None, if_body_1, else_body_1, get_state_1, set_state_1, ('input_shape',), 1)\n    File \"/var/tmp/__autograph_generated_filez13q0744.py\", line 124, in else_body_1\n        ag__.if_stmt(ag__.ld(inputs_embeds) is not None, if_body, else_body, get_state, set_state, ('input_shape',), 1)\n    File \"/var/tmp/__autograph_generated_filez13q0744.py\", line 122, in else_body\n        raise ag__.converted_call(ag__.ld(ValueError), ('You have to specify either decoder_input_ids or decoder_inputs_embeds',), None, fscope)\n\n    ValueError: Exception encountered when calling layer 'tf_blenderbot_small_model_1' (type TFBlenderbotSmallModel).\n    \n    in user code:\n    \n        File \"/opt/conda/envs/xla-env/lib/python3.10/site-packages/transformers/modeling_tf_utils.py\", line 1079, in run_call_with_unpacked_inputs  *\n            return func(self, **unpacked_inputs)\n        File \"/opt/conda/envs/xla-env/lib/python3.10/site-packages/transformers/models/blenderbot_small/modeling_tf_blenderbot_small.py\", line 1261, in call  *\n            outputs = self.model(\n        File \"/opt/conda/envs/xla-env/lib/python3.10/site-packages/tf_keras/src/utils/traceback_utils.py\", line 70, in error_handler  **\n            raise e.with_traceback(filtered_tb) from None\n        File \"/var/tmp/__autograph_generated_file138ue72o.py\", line 37, in tf__run_call_with_unpacked_inputs\n            retval_ = ag__.converted_call(ag__.ld(func), (ag__.ld(self),), dict(**ag__.ld(unpacked_inputs)), fscope)\n        File \"/var/tmp/__autograph_generated_filef0o968j1.py\", line 58, in tf__call\n            decoder_outputs = ag__.converted_call(ag__.ld(self).decoder, (ag__.ld(decoder_input_ids),), dict(attention_mask=ag__.ld(decoder_attention_mask), position_ids=ag__.ld(decoder_position_ids), encoder_hidden_states=ag__.ld(encoder_outputs)[0], encoder_attention_mask=ag__.ld(attention_mask), head_mask=ag__.ld(decoder_head_mask), cross_attn_head_mask=ag__.ld(cross_attn_head_mask), past_key_values=ag__.ld(past_key_values), inputs_embeds=ag__.ld(decoder_inputs_embeds), use_cache=ag__.ld(use_cache), output_attentions=ag__.ld(output_attentions), output_hidden_states=ag__.ld(output_hidden_states), return_dict=ag__.ld(return_dict), training=ag__.ld(training)), fscope)\n        File \"/var/tmp/__autograph_generated_file138ue72o.py\", line 37, in tf__run_call_with_unpacked_inputs\n            retval_ = ag__.converted_call(ag__.ld(func), (ag__.ld(self),), dict(**ag__.ld(unpacked_inputs)), fscope)\n        File \"/var/tmp/__autograph_generated_filez13q0744.py\", line 128, in tf__call\n            ag__.if_stmt(ag__.and_(lambda : ag__.ld(input_ids) is not None, lambda : ag__.ld(inputs_embeds) is not None), if_body_2, else_body_2, get_state_2, set_state_2, ('input_shape',), 1)\n        File \"/var/tmp/__autograph_generated_filez13q0744.py\", line 126, in else_body_2\n            ag__.if_stmt(ag__.ld(input_ids) is not None, if_body_1, else_body_1, get_state_1, set_state_1, ('input_shape',), 1)\n        File \"/var/tmp/__autograph_generated_filez13q0744.py\", line 124, in else_body_1\n            ag__.if_stmt(ag__.ld(inputs_embeds) is not None, if_body, else_body, get_state, set_state, ('input_shape',), 1)\n        File \"/var/tmp/__autograph_generated_filez13q0744.py\", line 122, in else_body\n            raise ag__.converted_call(ag__.ld(ValueError), ('You have to specify either decoder_input_ids or decoder_inputs_embeds',), None, fscope)\n    \n        ValueError: Exception encountered when calling layer 'model' (type TFBlenderbotSmallMainLayer).\n        \n        in user code:\n        \n            File \"/opt/conda/envs/xla-env/lib/python3.10/site-packages/transformers/modeling_tf_utils.py\", line 1079, in run_call_with_unpacked_inputs  *\n                return func(self, **unpacked_inputs)\n            File \"/opt/conda/envs/xla-env/lib/python3.10/site-packages/transformers/models/blenderbot_small/modeling_tf_blenderbot_small.py\", line 1169, in call  *\n                decoder_outputs = self.decoder(\n            File \"/opt/conda/envs/xla-env/lib/python3.10/site-packages/tf_keras/src/utils/traceback_utils.py\", line 70, in error_handler  **\n                raise e.with_traceback(filtered_tb) from None\n            File \"/var/tmp/__autograph_generated_file138ue72o.py\", line 37, in tf__run_call_with_unpacked_inputs\n                retval_ = ag__.converted_call(ag__.ld(func), (ag__.ld(self),), dict(**ag__.ld(unpacked_inputs)), fscope)\n            File \"/var/tmp/__autograph_generated_filez13q0744.py\", line 128, in tf__call\n                ag__.if_stmt(ag__.and_(lambda : ag__.ld(input_ids) is not None, lambda : ag__.ld(inputs_embeds) is not None), if_body_2, else_body_2, get_state_2, set_state_2, ('input_shape',), 1)\n            File \"/var/tmp/__autograph_generated_filez13q0744.py\", line 126, in else_body_2\n                ag__.if_stmt(ag__.ld(input_ids) is not None, if_body_1, else_body_1, get_state_1, set_state_1, ('input_shape',), 1)\n            File \"/var/tmp/__autograph_generated_filez13q0744.py\", line 124, in else_body_1\n                ag__.if_stmt(ag__.ld(inputs_embeds) is not None, if_body, else_body, get_state, set_state, ('input_shape',), 1)\n            File \"/var/tmp/__autograph_generated_filez13q0744.py\", line 122, in else_body\n                raise ag__.converted_call(ag__.ld(ValueError), ('You have to specify either decoder_input_ids or decoder_inputs_embeds',), None, fscope)\n        \n            ValueError: Exception encountered when calling layer 'decoder' (type TFBlenderbotSmallDecoder).\n            \n            in user code:\n            \n                File \"/opt/conda/envs/xla-env/lib/python3.10/site-packages/transformers/modeling_tf_utils.py\", line 1079, in run_call_with_unpacked_inputs  *\n                    return func(self, **unpacked_inputs)\n                File \"/opt/conda/envs/xla-env/lib/python3.10/site-packages/transformers/models/blenderbot_small/modeling_tf_blenderbot_small.py\", line 983, in call  *\n                    raise ValueError(\"You have to specify either decoder_input_ids or decoder_inputs_embeds\")\n            \n                ValueError: You have to specify either decoder_input_ids or decoder_inputs_embeds\n            \n            \n            Call arguments received by layer 'decoder' (type TFBlenderbotSmallDecoder):\n              \u2022 input_ids=None\n              \u2022 inputs_embeds=None\n              \u2022 attention_mask=None\n              \u2022 position_ids=None\n              \u2022 encoder_hidden_states=tf.Tensor(shape=(1, 32, 512), dtype=float32)\n              \u2022 encoder_attention_mask=tf.Tensor(shape=(1, 32), dtype=int32)\n              \u2022 head_mask=None\n              \u2022 cross_attn_head_mask=None\n              \u2022 past_key_values=None\n              \u2022 use_cache=True\n              \u2022 output_attentions=False\n              \u2022 output_hidden_states=False\n              \u2022 return_dict=True\n              \u2022 training=False\n        \n        \n        Call arguments received by layer 'model' (type TFBlenderbotSmallMainLayer):\n          \u2022 input_ids=tf.Tensor(shape=(1, 32), dtype=int32)\n          \u2022 attention_mask=tf.Tensor(shape=(1, 32), dtype=int32)\n          \u2022 decoder_input_ids=None\n          \u2022 decoder_attention_mask=None\n          \u2022 decoder_position_ids=None\n          \u2022 head_mask=None\n          \u2022 decoder_head_mask=None\n          \u2022 cross_attn_head_mask=None\n          \u2022 encoder_outputs=None\n          \u2022 past_key_values=None\n          \u2022 inputs_embeds=None\n          \u2022 decoder_inputs_embeds=None\n          \u2022 use_cache=True\n          \u2022 output_attentions=False\n          \u2022 output_hidden_states=False\n          \u2022 return_dict=True\n          \u2022 training=False\n          \u2022 kwargs=<class 'inspect._empty'>\n    \n    \n    Call arguments received by layer 'tf_blenderbot_small_model_1' (type TFBlenderbotSmallModel):\n      \u2022 input_ids=tf.Tensor(shape=(1, 32), dtype=int32)\n      \u2022 attention_mask=tf.Tensor(shape=(1, 32), dtype=int32)\n      \u2022 decoder_input_ids=None\n      \u2022 decoder_attention_mask=None\n      \u2022 decoder_position_ids=None\n      \u2022 head_mask=None\n      \u2022 decoder_head_mask=None\n      \u2022 cross_attn_head_mask=None\n      \u2022 encoder_outputs=None\n      \u2022 past_key_values=None\n      \u2022 inputs_embeds=None\n      \u2022 decoder_inputs_embeds=None\n      \u2022 use_cache=None\n      \u2022 output_attentions=None\n      \u2022 output_hidden_states=None\n      \u2022 return_dict=None\n      \u2022 training=False\n      \u2022 kwargs=<class 'inspect._empty'>\n"
            },
            "jax_flax_eager": {
                "ttft_s": 8.482727766036987
            }
        }
    },
    {
        "BlenderbotSmallForConditionalGeneration": {
            "error": "AttributeError(\"'BlenderbotSmallForConditionalGeneration' object has no attribute '_shift_right'\")"
        }
    },
    {
        "CamemBert": {
            "pytorch_torch_compile_inductor": {
                "ttft_s": 12.598877429962158
            },
            "pytorch_torch_compile_eager": {
                "ttft_s": 2.751638174057007
            },
            "pytorch_torch_compile_cudagraphs": {
                "ttft_s": 6.703953742980957
            },
            "pytorch_torch_compile_onnxrt": {
                "ttft_s": 11.896921396255493
            },
            "pytorch_torch_compile_openxla": {
                "error": "torch_xla/csrc/runtime/pjrt_registry.cc:214 : Check failed: client \n*** Begin stack trace ***\n\ttsl::CurrentStackTrace()\n\ttorch_xla::runtime::InitializePjRt(std::string const&)\n\ttorch_xla::runtime::PjRtComputationClient::PjRtComputationClient()\n\t\n\ttorch_xla::runtime::GetComputationClient()\n\ttorch_xla::bridge::GetDefaultDevice()\n\ttorch_xla::bridge::GetCurrentDevice()\n\ttorch_xla::bridge::GetCurrentAtenDevice()\n\t\n\t\n\t\n\t_PyObject_MakeTpCall\n\t_PyEval_EvalFrameDefault\n\t\n\t_PyFunction_Vectorcall\n\t_PyEval_EvalFrameDefault\n\t\n\t_PyFunction_Vectorcall\n\t_PyEval_EvalFrameDefault\n\t\n\t_PyFunction_Vectorcall\n\t_PyEval_EvalFrameDefault\n\t\n\t_PyFunction_Vectorcall\n\t_PyEval_EvalFrameDefault\n\t\n\t_PyFunction_Vectorcall\n\t_PyEval_EvalFrameDefault\n\t\n\t_PyFunction_Vectorcall\n\t_PyEval_EvalFrameDefault\n\t\n\t_PyFunction_Vectorcall\n\t_PyEval_EvalFrameDefault\n\t\n\t_PyFunction_Vectorcall\n\t_PyEval_EvalFrameDefault\n\t\n\t_PyFunction_Vectorcall\n\tTHPFunction_apply(_object*, _object*)\n\t\n\tPyObject_Call\n\t_PyEval_EvalFrameDefault\n\t\n\t\n\t_PyEval_EvalFrameDefault\n\t\n\t_PyFunction_Vectorcall\n\t_PyEval_EvalFrameDefault\n\t\n\t_PyFunction_Vectorcall\n\t_PyEval_EvalFrameDefault\n\t\n\t_PyFunction_Vectorcall\n\t_PyEval_EvalFrameDefault\n\t\n\t_PyFunction_Vectorcall\n\t_PyEval_EvalFrameDefault\n\t\n\t_PyFunction_Vectorcall\n\t_PyEval_EvalFrameDefault\n\t\n\t\n\t\n\tPyObject_Call\n\t_PyEval_EvalFrameDefault\n\t\n\t\n\tPyObject_Call\n\t_PyEval_EvalFrameDefault\n\t\n\t\n\tPyObject_Call\n\t_PyEval_EvalFrameDefault\n\t\n\t_PyFunction_Vectorcall\n\tPyObject_Call\n\t_PyEval_EvalFrameDefault\n\t\n\t\n\tPyObject_Call\n\t_PyEval_EvalFrameDefault\n\t\n\t_PyFunction_Vectorcall\n\t_PyObject_FastCallDictTstate\n\t_PyObject_Call_Prepend\n\t\n\tPyObject_Call\n\t_PyEval_EvalFrameDefault\n\t\n\t_PyFunction_Vectorcall\n\t_PyEval_EvalFrameDefault\n\t_PyFunction_Vectorcall\n\t_PyEval_EvalFrameDefault\n\t\n\tPyEval_EvalCode\n\t\n\t\n\t\n\t_PyRun_SimpleFileObject\n\t_PyRun_AnyFileObject\n\tPy_RunMain\n\tPy_BytesMain\n\t__libc_start_main\n\t\n*** End stack trace ***\nUnknown PJRT_DEVICE 'CUDA'"
            },
            "pytorch_torch_compile_tvm": {
                "error": "backend='tvm' raised:\nModuleNotFoundError: No module named 'tvm'\n\nSet TORCH_LOGS=\"+dynamo\" and TORCHDYNAMO_VERBOSE=1 for more information\n\n\nYou can suppress this exception and fall back to eager by setting:\n    import torch._dynamo\n    torch._dynamo.config.suppress_errors = True\n"
            },
            "jax_flax_xla": {
                "error": "Unrecognized configuration class <class 'transformers.models.camembert.configuration_camembert.CamembertConfig'> for this kind of AutoModel: FlaxAutoModel.\nModel type should be one of AlbertConfig, BartConfig, BeitConfig, BertConfig, BigBirdConfig, BlenderbotConfig, BlenderbotSmallConfig, BloomConfig, CLIPConfig, Dinov2Config, DistilBertConfig, ElectraConfig, GemmaConfig, GPT2Config, GPT2Config, GPTNeoConfig, GPTJConfig, LlamaConfig, LongT5Config, MarianConfig, MBartConfig, MistralConfig, MT5Config, OPTConfig, PegasusConfig, RegNetConfig, ResNetConfig, RobertaConfig, RobertaPreLayerNormConfig, RoFormerConfig, T5Config, VisionTextDualEncoderConfig, ViTConfig, Wav2Vec2Config, WhisperConfig, XGLMConfig, XLMRobertaConfig."
            },
            "tensorflow_eager": {
                "ttft_s": 0.23440766334533691
            },
            "tensorflow_xla": {
                "ttft_s": 5.543716907501221
            }
        }
    },
    {
        "DebertaForMaskedLM": {
            "pytorch_torch_compile_inductor": {
                "ttft_s": 3.0651302337646484
            },
            "pytorch_torch_compile_eager": {
                "ttft_s": 1.1806435585021973
            },
            "pytorch_torch_compile_cudagraphs": {
                "ttft_s": 2.2386059761047363
            },
            "pytorch_torch_compile_onnxrt": {
                "error": "backend='onnxrt' raised:\nAssertionError: Please convert all Tensors to FakeTensors first or instantiate FakeTensorMode with 'allow_non_fake_inputs'. Found in aten.alias.default(tensor(64., size=()))\n\nWhile executing %tensor : [num_users=1] = call_function[target=torch.tensor](args = (64,), kwargs = {dtype: torch.float32})\nOriginal traceback:\n  File \"/opt/conda/envs/xla-env/lib/python3.10/site-packages/transformers/models/deberta/modeling_deberta.py\", line 274, in forward\n    scale = scaled_size_sqrt(query_layer, scale_factor)\n  File \"/opt/conda/envs/xla-env/lib/python3.10/site-packages/torch/_dynamo/polyfills/__init__.py\", line 160, in getattr_and_trace\n    return fn(*args[2:], **kwargs)\n  File \"/opt/conda/envs/xla-env/lib/python3.10/site-packages/transformers/models/deberta/modeling_deberta.py\", line 137, in scaled_size_sqrt\n    return torch.sqrt(torch.tensor(query_layer.size(-1), dtype=torch.float) * scale_factor)\n\n\nSet TORCH_LOGS=\"+dynamo\" and TORCHDYNAMO_VERBOSE=1 for more information\n\n\nYou can suppress this exception and fall back to eager by setting:\n    import torch._dynamo\n    torch._dynamo.config.suppress_errors = True\n"
            },
            "pytorch_torch_compile_openxla": {
                "error": "torch_xla/csrc/runtime/pjrt_registry.cc:214 : Check failed: client \n*** Begin stack trace ***\n\ttsl::CurrentStackTrace()\n\ttorch_xla::runtime::InitializePjRt(std::string const&)\n\ttorch_xla::runtime::PjRtComputationClient::PjRtComputationClient()\n\t\n\ttorch_xla::runtime::GetComputationClient()\n\ttorch_xla::bridge::GetDefaultDevice()\n\ttorch_xla::bridge::GetCurrentDevice()\n\ttorch_xla::bridge::GetCurrentAtenDevice()\n\t\n\t\n\t\n\t_PyObject_MakeTpCall\n\t_PyEval_EvalFrameDefault\n\t\n\t_PyFunction_Vectorcall\n\t_PyEval_EvalFrameDefault\n\t\n\t_PyFunction_Vectorcall\n\t_PyEval_EvalFrameDefault\n\t\n\t_PyFunction_Vectorcall\n\t_PyEval_EvalFrameDefault\n\t\n\t_PyFunction_Vectorcall\n\t_PyEval_EvalFrameDefault\n\t\n\t_PyFunction_Vectorcall\n\t_PyEval_EvalFrameDefault\n\t\n\t_PyFunction_Vectorcall\n\t_PyEval_EvalFrameDefault\n\t\n\t_PyFunction_Vectorcall\n\t_PyEval_EvalFrameDefault\n\t\n\t_PyFunction_Vectorcall\n\t_PyEval_EvalFrameDefault\n\t\n\t_PyFunction_Vectorcall\n\tTHPFunction_apply(_object*, _object*)\n\t\n\tPyObject_Call\n\t_PyEval_EvalFrameDefault\n\t\n\t\n\t_PyEval_EvalFrameDefault\n\t\n\t_PyFunction_Vectorcall\n\t_PyEval_EvalFrameDefault\n\t\n\t_PyFunction_Vectorcall\n\t_PyEval_EvalFrameDefault\n\t\n\t_PyFunction_Vectorcall\n\t_PyEval_EvalFrameDefault\n\t\n\t_PyFunction_Vectorcall\n\t_PyEval_EvalFrameDefault\n\t\n\t_PyFunction_Vectorcall\n\t_PyEval_EvalFrameDefault\n\t\n\t\n\t\n\tPyObject_Call\n\t_PyEval_EvalFrameDefault\n\t\n\t\n\tPyObject_Call\n\t_PyEval_EvalFrameDefault\n\t\n\t_PyFunction_Vectorcall\n\t_PyObject_FastCallDictTstate\n\t_PyObject_Call_Prepend\n\t\n\t_PyObject_MakeTpCall\n\t_PyEval_EvalFrameDefault\n\t\n\t\n\t\n\tPyObject_Call\n\t_PyEval_EvalFrameDefault\n\t\n\t\n\tPyObject_Call\n\t_PyEval_EvalFrameDefault\n\t\n\t\n\tPyObject_Call\n\t_PyEval_EvalFrameDefault\n\t\n\t_PyFunction_Vectorcall\n\tPyObject_Call\n\t_PyEval_EvalFrameDefault\n\t\n\t\n\tPyObject_Call\n\t_PyEval_EvalFrameDefault\n\t\n\t_PyFunction_Vectorcall\n\t_PyObject_FastCallDictTstate\n\t_PyObject_Call_Prepend\n\t\n\tPyObject_Call\n\t_PyEval_EvalFrameDefault\n\t\n\t_PyFunction_Vectorcall\n\t_PyEval_EvalFrameDefault\n\t_PyFunction_Vectorcall\n\t_PyEval_EvalFrameDefault\n\t\n\tPyEval_EvalCode\n\t\n\t\n\t\n\t_PyRun_SimpleFileObject\n\t_PyRun_AnyFileObject\n\tPy_RunMain\n\tPy_BytesMain\n\t__libc_start_main\n\t\n*** End stack trace ***\nUnknown PJRT_DEVICE 'CUDA'"
            },
            "pytorch_torch_compile_tvm": {
                "error": "backend='tvm' raised:\nModuleNotFoundError: No module named 'tvm'\n\nSet TORCH_LOGS=\"+dynamo\" and TORCHDYNAMO_VERBOSE=1 for more information\n\n\nYou can suppress this exception and fall back to eager by setting:\n    import torch._dynamo\n    torch._dynamo.config.suppress_errors = True\n"
            },
            "jax_flax_xla": {
                "error": "Unrecognized configuration class <class 'transformers.models.deberta.configuration_deberta.DebertaConfig'> for this kind of AutoModel: FlaxAutoModel.\nModel type should be one of AlbertConfig, BartConfig, BeitConfig, BertConfig, BigBirdConfig, BlenderbotConfig, BlenderbotSmallConfig, BloomConfig, CLIPConfig, Dinov2Config, DistilBertConfig, ElectraConfig, GemmaConfig, GPT2Config, GPT2Config, GPTNeoConfig, GPTJConfig, LlamaConfig, LongT5Config, MarianConfig, MBartConfig, MistralConfig, MT5Config, OPTConfig, PegasusConfig, RegNetConfig, ResNetConfig, RobertaConfig, RobertaPreLayerNormConfig, RoFormerConfig, T5Config, VisionTextDualEncoderConfig, ViTConfig, Wav2Vec2Config, WhisperConfig, XGLMConfig, XLMRobertaConfig."
            },
            "tensorflow_eager": {
                "ttft_s": 1.2513551712036133
            },
            "tensorflow_xla": {
                "ttft_s": 14.623409509658813
            }
        }
    },
    {
        "DebertaForQuestionAnswering": {
            "pytorch_torch_compile_inductor": {
                "ttft_s": 0.39226722717285156
            },
            "pytorch_torch_compile_eager": {
                "ttft_s": 0.2614712715148926
            },
            "pytorch_torch_compile_cudagraphs": {
                "ttft_s": 0.40032267570495605
            },
            "pytorch_torch_compile_onnxrt": {
                "error": "backend='onnxrt' raised:\nAssertionError: Please convert all Tensors to FakeTensors first or instantiate FakeTensorMode with 'allow_non_fake_inputs'. Found in aten.alias.default(tensor(64., size=()))\n\nWhile executing %tensor : [num_users=1] = call_function[target=torch.tensor](args = (64,), kwargs = {dtype: torch.float32})\nOriginal traceback:\n  File \"/opt/conda/envs/xla-env/lib/python3.10/site-packages/transformers/models/deberta/modeling_deberta.py\", line 274, in forward\n    scale = scaled_size_sqrt(query_layer, scale_factor)\n  File \"/opt/conda/envs/xla-env/lib/python3.10/site-packages/torch/_dynamo/polyfills/__init__.py\", line 160, in getattr_and_trace\n    return fn(*args[2:], **kwargs)\n  File \"/opt/conda/envs/xla-env/lib/python3.10/site-packages/transformers/models/deberta/modeling_deberta.py\", line 137, in scaled_size_sqrt\n    return torch.sqrt(torch.tensor(query_layer.size(-1), dtype=torch.float) * scale_factor)\n\n\nSet TORCH_LOGS=\"+dynamo\" and TORCHDYNAMO_VERBOSE=1 for more information\n\n\nYou can suppress this exception and fall back to eager by setting:\n    import torch._dynamo\n    torch._dynamo.config.suppress_errors = True\n"
            },
            "pytorch_torch_compile_openxla": {
                "error": "torch_xla/csrc/runtime/pjrt_registry.cc:214 : Check failed: client \n*** Begin stack trace ***\n\ttsl::CurrentStackTrace()\n\ttorch_xla::runtime::InitializePjRt(std::string const&)\n\ttorch_xla::runtime::PjRtComputationClient::PjRtComputationClient()\n\t\n\ttorch_xla::runtime::GetComputationClient()\n\ttorch_xla::bridge::GetDefaultDevice()\n\ttorch_xla::bridge::GetCurrentDevice()\n\ttorch_xla::bridge::GetCurrentAtenDevice()\n\t\n\t\n\t\n\t_PyObject_MakeTpCall\n\t_PyEval_EvalFrameDefault\n\t\n\t_PyFunction_Vectorcall\n\t_PyEval_EvalFrameDefault\n\t\n\t_PyFunction_Vectorcall\n\t_PyEval_EvalFrameDefault\n\t\n\t_PyFunction_Vectorcall\n\t_PyEval_EvalFrameDefault\n\t\n\t_PyFunction_Vectorcall\n\t_PyEval_EvalFrameDefault\n\t\n\t_PyFunction_Vectorcall\n\t_PyEval_EvalFrameDefault\n\t\n\t_PyFunction_Vectorcall\n\t_PyEval_EvalFrameDefault\n\t\n\t_PyFunction_Vectorcall\n\t_PyEval_EvalFrameDefault\n\t\n\t_PyFunction_Vectorcall\n\t_PyEval_EvalFrameDefault\n\t\n\t_PyFunction_Vectorcall\n\tTHPFunction_apply(_object*, _object*)\n\t\n\tPyObject_Call\n\t_PyEval_EvalFrameDefault\n\t\n\t\n\t_PyEval_EvalFrameDefault\n\t\n\t_PyFunction_Vectorcall\n\t_PyEval_EvalFrameDefault\n\t\n\t_PyFunction_Vectorcall\n\t_PyEval_EvalFrameDefault\n\t\n\t_PyFunction_Vectorcall\n\t_PyEval_EvalFrameDefault\n\t\n\t_PyFunction_Vectorcall\n\t_PyEval_EvalFrameDefault\n\t\n\t_PyFunction_Vectorcall\n\t_PyEval_EvalFrameDefault\n\t\n\t\n\t\n\tPyObject_Call\n\t_PyEval_EvalFrameDefault\n\t\n\t\n\tPyObject_Call\n\t_PyEval_EvalFrameDefault\n\t\n\t_PyFunction_Vectorcall\n\t_PyObject_FastCallDictTstate\n\t_PyObject_Call_Prepend\n\t\n\t_PyObject_MakeTpCall\n\t_PyEval_EvalFrameDefault\n\t\n\t\n\t\n\tPyObject_Call\n\t_PyEval_EvalFrameDefault\n\t\n\t\n\tPyObject_Call\n\t_PyEval_EvalFrameDefault\n\t\n\t\n\tPyObject_Call\n\t_PyEval_EvalFrameDefault\n\t\n\t_PyFunction_Vectorcall\n\tPyObject_Call\n\t_PyEval_EvalFrameDefault\n\t\n\t\n\tPyObject_Call\n\t_PyEval_EvalFrameDefault\n\t\n\t_PyFunction_Vectorcall\n\t_PyObject_FastCallDictTstate\n\t_PyObject_Call_Prepend\n\t\n\tPyObject_Call\n\t_PyEval_EvalFrameDefault\n\t\n\t_PyFunction_Vectorcall\n\t_PyEval_EvalFrameDefault\n\t_PyFunction_Vectorcall\n\t_PyEval_EvalFrameDefault\n\t\n\tPyEval_EvalCode\n\t\n\t\n\t\n\t_PyRun_SimpleFileObject\n\t_PyRun_AnyFileObject\n\tPy_RunMain\n\tPy_BytesMain\n\t__libc_start_main\n\t\n*** End stack trace ***\nUnknown PJRT_DEVICE 'CUDA'"
            },
            "pytorch_torch_compile_tvm": {
                "error": "backend='tvm' raised:\nModuleNotFoundError: No module named 'tvm'\n\nSet TORCH_LOGS=\"+dynamo\" and TORCHDYNAMO_VERBOSE=1 for more information\n\n\nYou can suppress this exception and fall back to eager by setting:\n    import torch._dynamo\n    torch._dynamo.config.suppress_errors = True\n"
            },
            "jax_flax_xla": {
                "error": "Unrecognized configuration class <class 'transformers.models.deberta.configuration_deberta.DebertaConfig'> for this kind of AutoModel: FlaxAutoModel.\nModel type should be one of AlbertConfig, BartConfig, BeitConfig, BertConfig, BigBirdConfig, BlenderbotConfig, BlenderbotSmallConfig, BloomConfig, CLIPConfig, Dinov2Config, DistilBertConfig, ElectraConfig, GemmaConfig, GPT2Config, GPT2Config, GPTNeoConfig, GPTJConfig, LlamaConfig, LongT5Config, MarianConfig, MBartConfig, MistralConfig, MT5Config, OPTConfig, PegasusConfig, RegNetConfig, ResNetConfig, RobertaConfig, RobertaPreLayerNormConfig, RoFormerConfig, T5Config, VisionTextDualEncoderConfig, ViTConfig, Wav2Vec2Config, WhisperConfig, XGLMConfig, XLMRobertaConfig."
            },
            "tensorflow_eager": {
                "ttft_s": 0.3950328826904297
            },
            "tensorflow_xla": {
                "ttft_s": 9.017994403839111
            }
        }
    },
    {
        "DebertaV2ForMaskedLM": {
            "pytorch_torch_compile_inductor": {
                "ttft_s": 33.06136703491211
            },
            "pytorch_torch_compile_eager": {
                "ttft_s": 6.840337753295898
            },
            "pytorch_torch_compile_cudagraphs": {
                "ttft_s": 25.11499547958374
            },
            "pytorch_torch_compile_onnxrt": {
                "error": "backend='onnxrt' raised:\nAssertionError: Please convert all Tensors to FakeTensors first or instantiate FakeTensorMode with 'allow_non_fake_inputs'. Found in aten.alias.default(tensor(127, size=()))\n\nWhile executing %tensor : [num_users=1] = call_function[target=torch.tensor](args = (127,), kwargs = {})\nOriginal traceback:\n  File \"/opt/conda/envs/xla-env/lib/python3.10/site-packages/transformers/models/deberta_v2/modeling_deberta_v2.py\", line 1059, in forward\n    outputs = self.deberta(\n  File \"/opt/conda/envs/xla-env/lib/python3.10/site-packages/transformers/models/deberta_v2/modeling_deberta_v2.py\", line 870, in forward\n    encoder_outputs = self.encoder(\n  File \"/opt/conda/envs/xla-env/lib/python3.10/site-packages/transformers/models/deberta_v2/modeling_deberta_v2.py\", line 655, in forward\n    relative_pos = self.get_rel_pos(hidden_states, query_states, relative_pos)\n  File \"/opt/conda/envs/xla-env/lib/python3.10/site-packages/transformers/models/deberta_v2/modeling_deberta_v2.py\", line 632, in get_rel_pos\n    relative_pos = build_relative_position(\n  File \"/opt/conda/envs/xla-env/lib/python3.10/site-packages/transformers/models/deberta_v2/modeling_deberta_v2.py\", line 103, in build_relative_position\n    rel_pos_ids = make_log_bucket_position(rel_pos_ids, bucket_size, max_position)\n  File \"/opt/conda/envs/xla-env/lib/python3.10/site-packages/torch/_dynamo/polyfills/__init__.py\", line 160, in getattr_and_trace\n    return fn(*args[2:], **kwargs)\n  File \"/opt/conda/envs/xla-env/lib/python3.10/site-packages/transformers/models/deberta_v2/modeling_deberta_v2.py\", line 68, in make_log_bucket_position\n    torch.tensor(mid - 1).type_as(relative_pos),\n\n\nSet TORCH_LOGS=\"+dynamo\" and TORCHDYNAMO_VERBOSE=1 for more information\n\n\nYou can suppress this exception and fall back to eager by setting:\n    import torch._dynamo\n    torch._dynamo.config.suppress_errors = True\n"
            },
            "pytorch_torch_compile_openxla": {
                "error": "torch_xla/csrc/runtime/pjrt_registry.cc:214 : Check failed: client \n*** Begin stack trace ***\n\ttsl::CurrentStackTrace()\n\ttorch_xla::runtime::InitializePjRt(std::string const&)\n\ttorch_xla::runtime::PjRtComputationClient::PjRtComputationClient()\n\t\n\ttorch_xla::runtime::GetComputationClient()\n\ttorch_xla::bridge::GetDefaultDevice()\n\ttorch_xla::bridge::GetCurrentDevice()\n\ttorch_xla::bridge::GetCurrentAtenDevice()\n\t\n\t\n\t\n\t_PyObject_MakeTpCall\n\t_PyEval_EvalFrameDefault\n\t\n\t_PyFunction_Vectorcall\n\t_PyEval_EvalFrameDefault\n\t\n\t_PyFunction_Vectorcall\n\t_PyEval_EvalFrameDefault\n\t\n\t_PyFunction_Vectorcall\n\t_PyEval_EvalFrameDefault\n\t\n\t_PyFunction_Vectorcall\n\t_PyEval_EvalFrameDefault\n\t\n\t_PyFunction_Vectorcall\n\t_PyEval_EvalFrameDefault\n\t\n\t_PyFunction_Vectorcall\n\t_PyEval_EvalFrameDefault\n\t\n\t_PyFunction_Vectorcall\n\t_PyEval_EvalFrameDefault\n\t\n\t_PyFunction_Vectorcall\n\t_PyEval_EvalFrameDefault\n\t\n\t_PyFunction_Vectorcall\n\tTHPFunction_apply(_object*, _object*)\n\t\n\tPyObject_Call\n\t_PyEval_EvalFrameDefault\n\t\n\t\n\t_PyEval_EvalFrameDefault\n\t\n\t_PyFunction_Vectorcall\n\t_PyEval_EvalFrameDefault\n\t\n\t_PyFunction_Vectorcall\n\t_PyEval_EvalFrameDefault\n\t\n\t_PyFunction_Vectorcall\n\t_PyEval_EvalFrameDefault\n\t\n\t_PyFunction_Vectorcall\n\t_PyEval_EvalFrameDefault\n\t\n\t_PyFunction_Vectorcall\n\t_PyEval_EvalFrameDefault\n\t\n\t\n\t\n\tPyObject_Call\n\t_PyEval_EvalFrameDefault\n\t\n\t\n\tPyObject_Call\n\t_PyEval_EvalFrameDefault\n\t\n\t\n\tPyObject_Call\n\t_PyEval_EvalFrameDefault\n\t\n\t_PyFunction_Vectorcall\n\tPyObject_Call\n\t_PyEval_EvalFrameDefault\n\t\n\t\n\tPyObject_Call\n\t_PyEval_EvalFrameDefault\n\t\n\t_PyFunction_Vectorcall\n\t_PyObject_FastCallDictTstate\n\t_PyObject_Call_Prepend\n\t\n\tPyObject_Call\n\t_PyEval_EvalFrameDefault\n\t\n\t_PyFunction_Vectorcall\n\t_PyEval_EvalFrameDefault\n\t_PyFunction_Vectorcall\n\t_PyEval_EvalFrameDefault\n\t\n\tPyEval_EvalCode\n\t\n\t\n\t\n\t_PyRun_SimpleFileObject\n\t_PyRun_AnyFileObject\n\tPy_RunMain\n\tPy_BytesMain\n\t__libc_start_main\n\t\n*** End stack trace ***\nUnknown PJRT_DEVICE 'CUDA'"
            },
            "pytorch_torch_compile_tvm": {
                "error": "backend='tvm' raised:\nModuleNotFoundError: No module named 'tvm'\n\nSet TORCH_LOGS=\"+dynamo\" and TORCHDYNAMO_VERBOSE=1 for more information\n\n\nYou can suppress this exception and fall back to eager by setting:\n    import torch._dynamo\n    torch._dynamo.config.suppress_errors = True\n"
            },
            "jax_flax_xla": {
                "error": "Unrecognized configuration class <class 'transformers.models.deberta_v2.configuration_deberta_v2.DebertaV2Config'> for this kind of AutoModel: FlaxAutoModel.\nModel type should be one of AlbertConfig, BartConfig, BeitConfig, BertConfig, BigBirdConfig, BlenderbotConfig, BlenderbotSmallConfig, BloomConfig, CLIPConfig, Dinov2Config, DistilBertConfig, ElectraConfig, GemmaConfig, GPT2Config, GPT2Config, GPTNeoConfig, GPTJConfig, LlamaConfig, LongT5Config, MarianConfig, MBartConfig, MistralConfig, MT5Config, OPTConfig, PegasusConfig, RegNetConfig, ResNetConfig, RobertaConfig, RobertaPreLayerNormConfig, RoFormerConfig, T5Config, VisionTextDualEncoderConfig, ViTConfig, Wav2Vec2Config, WhisperConfig, XGLMConfig, XLMRobertaConfig."
            },
            "tensorflow_eager": {
                "ttft_s": 2.5975067615509033
            },
            "tensorflow_xla": {
                "error": "{{function_node __wrapped__Mul_device_/job:localhost/replica:0/task:0/device:GPU:0}} failed to allocate memory [Op:Mul] name: "
            }
        }
    },
    {
        "DebertaV2ForQuestionAnswering": {
            "pytorch_torch_compile_inductor": {
                "ttft_s": 33.20711588859558
            },
            "pytorch_torch_compile_eager": {
                "ttft_s": 6.860586643218994
            },
            "pytorch_torch_compile_cudagraphs": {
                "ttft_s": 22.58266019821167
            },
            "pytorch_torch_compile_onnxrt": {
                "error": "backend='onnxrt' raised:\nAssertionError: Please convert all Tensors to FakeTensors first or instantiate FakeTensorMode with 'allow_non_fake_inputs'. Found in aten.alias.default(tensor(127, size=()))\n\nWhile executing %tensor : [num_users=1] = call_function[target=torch.tensor](args = (127,), kwargs = {})\nOriginal traceback:\n  File \"/opt/conda/envs/xla-env/lib/python3.10/site-packages/transformers/models/deberta_v2/modeling_deberta_v2.py\", line 1361, in forward\n    outputs = self.deberta(\n  File \"/opt/conda/envs/xla-env/lib/python3.10/site-packages/transformers/models/deberta_v2/modeling_deberta_v2.py\", line 870, in forward\n    encoder_outputs = self.encoder(\n  File \"/opt/conda/envs/xla-env/lib/python3.10/site-packages/transformers/models/deberta_v2/modeling_deberta_v2.py\", line 655, in forward\n    relative_pos = self.get_rel_pos(hidden_states, query_states, relative_pos)\n  File \"/opt/conda/envs/xla-env/lib/python3.10/site-packages/transformers/models/deberta_v2/modeling_deberta_v2.py\", line 632, in get_rel_pos\n    relative_pos = build_relative_position(\n  File \"/opt/conda/envs/xla-env/lib/python3.10/site-packages/transformers/models/deberta_v2/modeling_deberta_v2.py\", line 103, in build_relative_position\n    rel_pos_ids = make_log_bucket_position(rel_pos_ids, bucket_size, max_position)\n  File \"/opt/conda/envs/xla-env/lib/python3.10/site-packages/torch/_dynamo/polyfills/__init__.py\", line 160, in getattr_and_trace\n    return fn(*args[2:], **kwargs)\n  File \"/opt/conda/envs/xla-env/lib/python3.10/site-packages/transformers/models/deberta_v2/modeling_deberta_v2.py\", line 68, in make_log_bucket_position\n    torch.tensor(mid - 1).type_as(relative_pos),\n\n\nSet TORCH_LOGS=\"+dynamo\" and TORCHDYNAMO_VERBOSE=1 for more information\n\n\nYou can suppress this exception and fall back to eager by setting:\n    import torch._dynamo\n    torch._dynamo.config.suppress_errors = True\n"
            },
            "pytorch_torch_compile_openxla": {
                "error": "torch_xla/csrc/runtime/pjrt_registry.cc:214 : Check failed: client \n*** Begin stack trace ***\n\ttsl::CurrentStackTrace()\n\ttorch_xla::runtime::InitializePjRt(std::string const&)\n\ttorch_xla::runtime::PjRtComputationClient::PjRtComputationClient()\n\t\n\ttorch_xla::runtime::GetComputationClient()\n\ttorch_xla::bridge::GetDefaultDevice()\n\ttorch_xla::bridge::GetCurrentDevice()\n\ttorch_xla::bridge::GetCurrentAtenDevice()\n\t\n\t\n\t\n\t_PyObject_MakeTpCall\n\t_PyEval_EvalFrameDefault\n\t\n\t_PyFunction_Vectorcall\n\t_PyEval_EvalFrameDefault\n\t\n\t_PyFunction_Vectorcall\n\t_PyEval_EvalFrameDefault\n\t\n\t_PyFunction_Vectorcall\n\t_PyEval_EvalFrameDefault\n\t\n\t_PyFunction_Vectorcall\n\t_PyEval_EvalFrameDefault\n\t\n\t_PyFunction_Vectorcall\n\t_PyEval_EvalFrameDefault\n\t\n\t_PyFunction_Vectorcall\n\t_PyEval_EvalFrameDefault\n\t\n\t_PyFunction_Vectorcall\n\t_PyEval_EvalFrameDefault\n\t\n\t_PyFunction_Vectorcall\n\t_PyEval_EvalFrameDefault\n\t\n\t_PyFunction_Vectorcall\n\tTHPFunction_apply(_object*, _object*)\n\t\n\tPyObject_Call\n\t_PyEval_EvalFrameDefault\n\t\n\t\n\t_PyEval_EvalFrameDefault\n\t\n\t_PyFunction_Vectorcall\n\t_PyEval_EvalFrameDefault\n\t\n\t_PyFunction_Vectorcall\n\t_PyEval_EvalFrameDefault\n\t\n\t_PyFunction_Vectorcall\n\t_PyEval_EvalFrameDefault\n\t\n\t_PyFunction_Vectorcall\n\t_PyEval_EvalFrameDefault\n\t\n\t_PyFunction_Vectorcall\n\t_PyEval_EvalFrameDefault\n\t\n\t\n\t\n\tPyObject_Call\n\t_PyEval_EvalFrameDefault\n\t\n\t\n\tPyObject_Call\n\t_PyEval_EvalFrameDefault\n\t\n\t\n\tPyObject_Call\n\t_PyEval_EvalFrameDefault\n\t\n\t_PyFunction_Vectorcall\n\tPyObject_Call\n\t_PyEval_EvalFrameDefault\n\t\n\t\n\tPyObject_Call\n\t_PyEval_EvalFrameDefault\n\t\n\t_PyFunction_Vectorcall\n\t_PyObject_FastCallDictTstate\n\t_PyObject_Call_Prepend\n\t\n\tPyObject_Call\n\t_PyEval_EvalFrameDefault\n\t\n\t_PyFunction_Vectorcall\n\t_PyEval_EvalFrameDefault\n\t_PyFunction_Vectorcall\n\t_PyEval_EvalFrameDefault\n\t\n\tPyEval_EvalCode\n\t\n\t\n\t\n\t_PyRun_SimpleFileObject\n\t_PyRun_AnyFileObject\n\tPy_RunMain\n\tPy_BytesMain\n\t__libc_start_main\n\t\n*** End stack trace ***\nUnknown PJRT_DEVICE 'CUDA'"
            },
            "pytorch_torch_compile_tvm": {
                "error": "backend='tvm' raised:\nModuleNotFoundError: No module named 'tvm'\n\nSet TORCH_LOGS=\"+dynamo\" and TORCHDYNAMO_VERBOSE=1 for more information\n\n\nYou can suppress this exception and fall back to eager by setting:\n    import torch._dynamo\n    torch._dynamo.config.suppress_errors = True\n"
            },
            "jax_flax_xla": {
                "error": "Unrecognized configuration class <class 'transformers.models.deberta_v2.configuration_deberta_v2.DebertaV2Config'> for this kind of AutoModel: FlaxAutoModel.\nModel type should be one of AlbertConfig, BartConfig, BeitConfig, BertConfig, BigBirdConfig, BlenderbotConfig, BlenderbotSmallConfig, BloomConfig, CLIPConfig, Dinov2Config, DistilBertConfig, ElectraConfig, GemmaConfig, GPT2Config, GPT2Config, GPTNeoConfig, GPTJConfig, LlamaConfig, LongT5Config, MarianConfig, MBartConfig, MistralConfig, MT5Config, OPTConfig, PegasusConfig, RegNetConfig, ResNetConfig, RobertaConfig, RobertaPreLayerNormConfig, RoFormerConfig, T5Config, VisionTextDualEncoderConfig, ViTConfig, Wav2Vec2Config, WhisperConfig, XGLMConfig, XLMRobertaConfig."
            },
            "tensorflow_eager": {
                "error": "{{function_node __wrapped__Mul_device_/job:localhost/replica:0/task:0/device:GPU:0}} failed to allocate memory [Op:Mul] name: "
            },
            "tensorflow_xla": {
                "error": "{{function_node __wrapped__Mul_device_/job:localhost/replica:0/task:0/device:GPU:0}} failed to allocate memory [Op:Mul] name: "
            }
        }
    },
    {
        "DistilBertForMaskedLM": {
            "pytorch_torch_compile_inductor": {
                "ttft_s": 4.646743535995483
            },
            "pytorch_torch_compile_eager": {
                "ttft_s": 0.9778742790222168
            },
            "pytorch_torch_compile_cudagraphs": {
                "ttft_s": 3.0413339138031006
            },
            "pytorch_torch_compile_onnxrt": {
                "error": "backend='onnxrt' raised:\nAssertionError: Please convert all Tensors to FakeTensors first or instantiate FakeTensorMode with 'allow_non_fake_inputs'. Found in aten.alias.default(tensor(-3.4028e+38, size=()))\n\nWhile executing %tensor : [num_users=1] = call_function[target=torch.tensor](args = (-3.4028234663852886e+38,), kwargs = {})\nOriginal traceback:\n  File \"/opt/conda/envs/xla-env/lib/python3.10/site-packages/transformers/models/distilbert/modeling_distilbert.py\", line 880, in forward\n    dlbrt_output = self.distilbert(\n  File \"/opt/conda/envs/xla-env/lib/python3.10/site-packages/transformers/models/distilbert/modeling_distilbert.py\", line 797, in forward\n    return self.transformer(\n  File \"/opt/conda/envs/xla-env/lib/python3.10/site-packages/transformers/models/distilbert/modeling_distilbert.py\", line 550, in forward\n    layer_outputs = layer_module(\n  File \"/opt/conda/envs/xla-env/lib/python3.10/site-packages/transformers/models/distilbert/modeling_distilbert.py\", line 476, in forward\n    sa_output = self.attention(\n  File \"/opt/conda/envs/xla-env/lib/python3.10/site-packages/transformers/models/distilbert/modeling_distilbert.py\", line 221, in forward\n    mask, torch.tensor(torch.finfo(scores.dtype).min)\n\n\nSet TORCH_LOGS=\"+dynamo\" and TORCHDYNAMO_VERBOSE=1 for more information\n\n\nYou can suppress this exception and fall back to eager by setting:\n    import torch._dynamo\n    torch._dynamo.config.suppress_errors = True\n"
            },
            "pytorch_torch_compile_openxla": {
                "error": "torch_xla/csrc/runtime/pjrt_registry.cc:214 : Check failed: client \n*** Begin stack trace ***\n\ttsl::CurrentStackTrace()\n\ttorch_xla::runtime::InitializePjRt(std::string const&)\n\ttorch_xla::runtime::PjRtComputationClient::PjRtComputationClient()\n\t\n\ttorch_xla::runtime::GetComputationClient()\n\ttorch_xla::bridge::GetDefaultDevice()\n\ttorch_xla::bridge::GetCurrentDevice()\n\ttorch_xla::bridge::GetCurrentAtenDevice()\n\t\n\t\n\t\n\t_PyObject_MakeTpCall\n\t_PyEval_EvalFrameDefault\n\t\n\t_PyFunction_Vectorcall\n\t_PyEval_EvalFrameDefault\n\t\n\t_PyFunction_Vectorcall\n\t_PyEval_EvalFrameDefault\n\t\n\t_PyFunction_Vectorcall\n\t_PyEval_EvalFrameDefault\n\t\n\t_PyFunction_Vectorcall\n\t_PyEval_EvalFrameDefault\n\t\n\t_PyFunction_Vectorcall\n\t_PyEval_EvalFrameDefault\n\t\n\t_PyFunction_Vectorcall\n\t_PyEval_EvalFrameDefault\n\t\n\t_PyFunction_Vectorcall\n\t_PyEval_EvalFrameDefault\n\t\n\t_PyFunction_Vectorcall\n\t_PyEval_EvalFrameDefault\n\t\n\t_PyFunction_Vectorcall\n\tTHPFunction_apply(_object*, _object*)\n\t\n\tPyObject_Call\n\t_PyEval_EvalFrameDefault\n\t\n\t\n\t_PyEval_EvalFrameDefault\n\t\n\t_PyFunction_Vectorcall\n\t_PyEval_EvalFrameDefault\n\t\n\t_PyFunction_Vectorcall\n\t_PyEval_EvalFrameDefault\n\t\n\t_PyFunction_Vectorcall\n\t_PyEval_EvalFrameDefault\n\t\n\t_PyFunction_Vectorcall\n\t_PyEval_EvalFrameDefault\n\t\n\t_PyFunction_Vectorcall\n\t_PyEval_EvalFrameDefault\n\t\n\t\n\t\n\tPyObject_Call\n\t_PyEval_EvalFrameDefault\n\t\n\t\n\tPyObject_Call\n\t_PyEval_EvalFrameDefault\n\t\n\t\n\tPyObject_Call\n\t_PyEval_EvalFrameDefault\n\t\n\t_PyFunction_Vectorcall\n\tPyObject_Call\n\t_PyEval_EvalFrameDefault\n\t\n\t\n\tPyObject_Call\n\t_PyEval_EvalFrameDefault\n\t\n\t_PyFunction_Vectorcall\n\t_PyObject_FastCallDictTstate\n\t_PyObject_Call_Prepend\n\t\n\tPyObject_Call\n\t_PyEval_EvalFrameDefault\n\t\n\t_PyFunction_Vectorcall\n\t_PyEval_EvalFrameDefault\n\t_PyFunction_Vectorcall\n\t_PyEval_EvalFrameDefault\n\t\n\tPyEval_EvalCode\n\t\n\t\n\t\n\t_PyRun_SimpleFileObject\n\t_PyRun_AnyFileObject\n\tPy_RunMain\n\tPy_BytesMain\n\t__libc_start_main\n\t\n*** End stack trace ***\nUnknown PJRT_DEVICE 'CUDA'"
            },
            "pytorch_torch_compile_tvm": {
                "error": "backend='tvm' raised:\nModuleNotFoundError: No module named 'tvm'\n\nSet TORCH_LOGS=\"+dynamo\" and TORCHDYNAMO_VERBOSE=1 for more information\n\n\nYou can suppress this exception and fall back to eager by setting:\n    import torch._dynamo\n    torch._dynamo.config.suppress_errors = True\n"
            },
            "jax_flax_xla": {
                "ttft_s": 6.290394067764282
            },
            "tensorflow_eager": {
                "ttft_s": 0.11983084678649902
            },
            "tensorflow_xla": {
                "ttft_s": 3.599954128265381
            },
            "jax_flax_eager": {
                "ttft_s": 0.9715824127197266
            }
        }
    },
    {
        "DistilBertForQuestionAnswering": {
            "pytorch_torch_compile_inductor": {
                "ttft_s": 4.503129720687866
            },
            "pytorch_torch_compile_eager": {
                "ttft_s": 0.9448833465576172
            },
            "pytorch_torch_compile_cudagraphs": {
                "ttft_s": 2.9707109928131104
            },
            "pytorch_torch_compile_onnxrt": {
                "error": "backend='onnxrt' raised:\nAssertionError: Please convert all Tensors to FakeTensors first or instantiate FakeTensorMode with 'allow_non_fake_inputs'. Found in aten.alias.default(tensor(-3.4028e+38, size=()))\n\nWhile executing %tensor : [num_users=1] = call_function[target=torch.tensor](args = (-3.4028234663852886e+38,), kwargs = {})\nOriginal traceback:\n  File \"/opt/conda/envs/xla-env/lib/python3.10/site-packages/transformers/models/distilbert/modeling_distilbert.py\", line 1099, in forward\n    distilbert_output = self.distilbert(\n  File \"/opt/conda/envs/xla-env/lib/python3.10/site-packages/transformers/models/distilbert/modeling_distilbert.py\", line 797, in forward\n    return self.transformer(\n  File \"/opt/conda/envs/xla-env/lib/python3.10/site-packages/transformers/models/distilbert/modeling_distilbert.py\", line 550, in forward\n    layer_outputs = layer_module(\n  File \"/opt/conda/envs/xla-env/lib/python3.10/site-packages/transformers/models/distilbert/modeling_distilbert.py\", line 476, in forward\n    sa_output = self.attention(\n  File \"/opt/conda/envs/xla-env/lib/python3.10/site-packages/transformers/models/distilbert/modeling_distilbert.py\", line 221, in forward\n    mask, torch.tensor(torch.finfo(scores.dtype).min)\n\n\nSet TORCH_LOGS=\"+dynamo\" and TORCHDYNAMO_VERBOSE=1 for more information\n\n\nYou can suppress this exception and fall back to eager by setting:\n    import torch._dynamo\n    torch._dynamo.config.suppress_errors = True\n"
            },
            "pytorch_torch_compile_openxla": {
                "error": "torch_xla/csrc/runtime/pjrt_registry.cc:214 : Check failed: client \n*** Begin stack trace ***\n\ttsl::CurrentStackTrace()\n\ttorch_xla::runtime::InitializePjRt(std::string const&)\n\ttorch_xla::runtime::PjRtComputationClient::PjRtComputationClient()\n\t\n\ttorch_xla::runtime::GetComputationClient()\n\ttorch_xla::bridge::GetDefaultDevice()\n\ttorch_xla::bridge::GetCurrentDevice()\n\ttorch_xla::bridge::GetCurrentAtenDevice()\n\t\n\t\n\t\n\t_PyObject_MakeTpCall\n\t_PyEval_EvalFrameDefault\n\t\n\t_PyFunction_Vectorcall\n\t_PyEval_EvalFrameDefault\n\t\n\t_PyFunction_Vectorcall\n\t_PyEval_EvalFrameDefault\n\t\n\t_PyFunction_Vectorcall\n\t_PyEval_EvalFrameDefault\n\t\n\t_PyFunction_Vectorcall\n\t_PyEval_EvalFrameDefault\n\t\n\t_PyFunction_Vectorcall\n\t_PyEval_EvalFrameDefault\n\t\n\t_PyFunction_Vectorcall\n\t_PyEval_EvalFrameDefault\n\t\n\t_PyFunction_Vectorcall\n\t_PyEval_EvalFrameDefault\n\t\n\t_PyFunction_Vectorcall\n\t_PyEval_EvalFrameDefault\n\t\n\t_PyFunction_Vectorcall\n\tTHPFunction_apply(_object*, _object*)\n\t\n\tPyObject_Call\n\t_PyEval_EvalFrameDefault\n\t\n\t\n\t_PyEval_EvalFrameDefault\n\t\n\t_PyFunction_Vectorcall\n\t_PyEval_EvalFrameDefault\n\t\n\t_PyFunction_Vectorcall\n\t_PyEval_EvalFrameDefault\n\t\n\t_PyFunction_Vectorcall\n\t_PyEval_EvalFrameDefault\n\t\n\t_PyFunction_Vectorcall\n\t_PyEval_EvalFrameDefault\n\t\n\t_PyFunction_Vectorcall\n\t_PyEval_EvalFrameDefault\n\t\n\t\n\t\n\tPyObject_Call\n\t_PyEval_EvalFrameDefault\n\t\n\t\n\tPyObject_Call\n\t_PyEval_EvalFrameDefault\n\t\n\t\n\tPyObject_Call\n\t_PyEval_EvalFrameDefault\n\t\n\t_PyFunction_Vectorcall\n\tPyObject_Call\n\t_PyEval_EvalFrameDefault\n\t\n\t\n\tPyObject_Call\n\t_PyEval_EvalFrameDefault\n\t\n\t_PyFunction_Vectorcall\n\t_PyObject_FastCallDictTstate\n\t_PyObject_Call_Prepend\n\t\n\tPyObject_Call\n\t_PyEval_EvalFrameDefault\n\t\n\t_PyFunction_Vectorcall\n\t_PyEval_EvalFrameDefault\n\t_PyFunction_Vectorcall\n\t_PyEval_EvalFrameDefault\n\t\n\tPyEval_EvalCode\n\t\n\t\n\t\n\t_PyRun_SimpleFileObject\n\t_PyRun_AnyFileObject\n\tPy_RunMain\n\tPy_BytesMain\n\t__libc_start_main\n\t\n*** End stack trace ***\nUnknown PJRT_DEVICE 'CUDA'"
            },
            "pytorch_torch_compile_tvm": {
                "error": "backend='tvm' raised:\nModuleNotFoundError: No module named 'tvm'\n\nSet TORCH_LOGS=\"+dynamo\" and TORCHDYNAMO_VERBOSE=1 for more information\n\n\nYou can suppress this exception and fall back to eager by setting:\n    import torch._dynamo\n    torch._dynamo.config.suppress_errors = True\n"
            },
            "jax_flax_xla": {
                "error": "'jaxlib.xla_extension.ArrayImpl' object has no attribute 'split'"
            },
            "tensorflow_eager": {
                "ttft_s": 0.11722707748413086
            },
            "tensorflow_xla": {
                "ttft_s": 2.1959445476531982
            }
        }
    },
    {
        "DistillGPT2": {
            "pytorch_torch_compile_inductor": {
                "ttft_s": 8.987540006637573
            },
            "pytorch_torch_compile_eager": {
                "ttft_s": 1.4437000751495361
            },
            "pytorch_torch_compile_cudagraphs": {
                "ttft_s": 3.4638848304748535
            },
            "pytorch_torch_compile_onnxrt": {
                "ttft_s": 6.30178427696228
            },
            "pytorch_torch_compile_openxla": {
                "error": "torch_xla/csrc/runtime/pjrt_registry.cc:214 : Check failed: client \n*** Begin stack trace ***\n\ttsl::CurrentStackTrace()\n\ttorch_xla::runtime::InitializePjRt(std::string const&)\n\ttorch_xla::runtime::PjRtComputationClient::PjRtComputationClient()\n\t\n\ttorch_xla::runtime::GetComputationClient()\n\ttorch_xla::bridge::GetDefaultDevice()\n\ttorch_xla::bridge::GetCurrentDevice()\n\ttorch_xla::bridge::GetCurrentAtenDevice()\n\t\n\t\n\t\n\t_PyObject_MakeTpCall\n\t_PyEval_EvalFrameDefault\n\t\n\t_PyFunction_Vectorcall\n\t_PyEval_EvalFrameDefault\n\t\n\t_PyFunction_Vectorcall\n\t_PyEval_EvalFrameDefault\n\t\n\t_PyFunction_Vectorcall\n\t_PyEval_EvalFrameDefault\n\t\n\t_PyFunction_Vectorcall\n\t_PyEval_EvalFrameDefault\n\t\n\t_PyFunction_Vectorcall\n\t_PyEval_EvalFrameDefault\n\t\n\t_PyFunction_Vectorcall\n\t_PyEval_EvalFrameDefault\n\t\n\t_PyFunction_Vectorcall\n\t_PyEval_EvalFrameDefault\n\t\n\t_PyFunction_Vectorcall\n\t_PyEval_EvalFrameDefault\n\t\n\t_PyFunction_Vectorcall\n\tTHPFunction_apply(_object*, _object*)\n\t\n\tPyObject_Call\n\t_PyEval_EvalFrameDefault\n\t\n\t\n\t_PyEval_EvalFrameDefault\n\t\n\t_PyFunction_Vectorcall\n\t_PyEval_EvalFrameDefault\n\t\n\t_PyFunction_Vectorcall\n\t_PyEval_EvalFrameDefault\n\t\n\t_PyFunction_Vectorcall\n\t_PyEval_EvalFrameDefault\n\t\n\t_PyFunction_Vectorcall\n\t_PyEval_EvalFrameDefault\n\t\n\t_PyFunction_Vectorcall\n\t_PyEval_EvalFrameDefault\n\t\n\t\n\t\n\tPyObject_Call\n\t_PyEval_EvalFrameDefault\n\t\n\t\n\tPyObject_Call\n\t_PyEval_EvalFrameDefault\n\t\n\t\n\tPyObject_Call\n\t_PyEval_EvalFrameDefault\n\t\n\t_PyFunction_Vectorcall\n\tPyObject_Call\n\t_PyEval_EvalFrameDefault\n\t\n\t\n\tPyObject_Call\n\t_PyEval_EvalFrameDefault\n\t\n\t_PyFunction_Vectorcall\n\t_PyObject_FastCallDictTstate\n\t_PyObject_Call_Prepend\n\t\n\tPyObject_Call\n\t_PyEval_EvalFrameDefault\n\t\n\t_PyFunction_Vectorcall\n\t_PyEval_EvalFrameDefault\n\t_PyFunction_Vectorcall\n\t_PyEval_EvalFrameDefault\n\t\n\tPyEval_EvalCode\n\t\n\t\n\t\n\t_PyRun_SimpleFileObject\n\t_PyRun_AnyFileObject\n\tPy_RunMain\n\tPy_BytesMain\n\t__libc_start_main\n\t\n*** End stack trace ***\nUnknown PJRT_DEVICE 'CUDA'"
            },
            "pytorch_torch_compile_tvm": {
                "error": "backend='tvm' raised:\nModuleNotFoundError: No module named 'tvm'\n\nSet TORCH_LOGS=\"+dynamo\" and TORCHDYNAMO_VERBOSE=1 for more information\n\n\nYou can suppress this exception and fall back to eager by setting:\n    import torch._dynamo\n    torch._dynamo.config.suppress_errors = True\n"
            },
            "jax_flax_xla": {
                "ttft_s": 6.339476823806763
            },
            "tensorflow_eager": {
                "error": "Asking to pad but the tokenizer does not have a padding token. Please select a token to use as `pad_token` `(tokenizer.pad_token = tokenizer.eos_token e.g.)` or add a new pad token via `tokenizer.add_special_tokens({'pad_token': '[PAD]'})`."
            },
            "tensorflow_xla": {
                "error": "Asking to pad but the tokenizer does not have a padding token. Please select a token to use as `pad_token` `(tokenizer.pad_token = tokenizer.eos_token e.g.)` or add a new pad token via `tokenizer.add_special_tokens({'pad_token': '[PAD]'})`."
            },
            "jax_flax_eager": {
                "ttft_s": 10.734085083007812
            }
        }
    },
    {
        "ElectraForCausalLM": {
            "pytorch_torch_compile_inductor": {
                "ttft_s": 10.20144009590149
            },
            "pytorch_torch_compile_eager": {
                "ttft_s": 2.793844699859619
            },
            "pytorch_torch_compile_cudagraphs": {
                "ttft_s": 6.67264723777771
            },
            "pytorch_torch_compile_onnxrt": {
                "ttft_s": 16.19869351387024
            },
            "pytorch_torch_compile_openxla": {
                "error": "torch_xla/csrc/runtime/pjrt_registry.cc:214 : Check failed: client \n*** Begin stack trace ***\n\ttsl::CurrentStackTrace()\n\ttorch_xla::runtime::InitializePjRt(std::string const&)\n\ttorch_xla::runtime::PjRtComputationClient::PjRtComputationClient()\n\t\n\ttorch_xla::runtime::GetComputationClient()\n\ttorch_xla::bridge::GetDefaultDevice()\n\ttorch_xla::bridge::GetCurrentDevice()\n\ttorch_xla::bridge::GetCurrentAtenDevice()\n\t\n\t\n\t\n\t_PyObject_MakeTpCall\n\t_PyEval_EvalFrameDefault\n\t\n\t_PyFunction_Vectorcall\n\t_PyEval_EvalFrameDefault\n\t\n\t_PyFunction_Vectorcall\n\t_PyEval_EvalFrameDefault\n\t\n\t_PyFunction_Vectorcall\n\t_PyEval_EvalFrameDefault\n\t\n\t_PyFunction_Vectorcall\n\t_PyEval_EvalFrameDefault\n\t\n\t_PyFunction_Vectorcall\n\t_PyEval_EvalFrameDefault\n\t\n\t_PyFunction_Vectorcall\n\t_PyEval_EvalFrameDefault\n\t\n\t_PyFunction_Vectorcall\n\t_PyEval_EvalFrameDefault\n\t\n\t_PyFunction_Vectorcall\n\t_PyEval_EvalFrameDefault\n\t\n\t_PyFunction_Vectorcall\n\tTHPFunction_apply(_object*, _object*)\n\t\n\tPyObject_Call\n\t_PyEval_EvalFrameDefault\n\t\n\t\n\t_PyEval_EvalFrameDefault\n\t\n\t_PyFunction_Vectorcall\n\t_PyEval_EvalFrameDefault\n\t\n\t_PyFunction_Vectorcall\n\t_PyEval_EvalFrameDefault\n\t\n\t_PyFunction_Vectorcall\n\t_PyEval_EvalFrameDefault\n\t\n\t_PyFunction_Vectorcall\n\t_PyEval_EvalFrameDefault\n\t\n\t_PyFunction_Vectorcall\n\t_PyEval_EvalFrameDefault\n\t\n\t\n\t\n\tPyObject_Call\n\t_PyEval_EvalFrameDefault\n\t\n\t\n\tPyObject_Call\n\t_PyEval_EvalFrameDefault\n\t\n\t\n\tPyObject_Call\n\t_PyEval_EvalFrameDefault\n\t\n\t_PyFunction_Vectorcall\n\tPyObject_Call\n\t_PyEval_EvalFrameDefault\n\t\n\t\n\tPyObject_Call\n\t_PyEval_EvalFrameDefault\n\t\n\t_PyFunction_Vectorcall\n\t_PyObject_FastCallDictTstate\n\t_PyObject_Call_Prepend\n\t\n\tPyObject_Call\n\t_PyEval_EvalFrameDefault\n\t\n\t_PyFunction_Vectorcall\n\t_PyEval_EvalFrameDefault\n\t_PyFunction_Vectorcall\n\t_PyEval_EvalFrameDefault\n\t\n\tPyEval_EvalCode\n\t\n\t\n\t\n\t_PyRun_SimpleFileObject\n\t_PyRun_AnyFileObject\n\tPy_RunMain\n\tPy_BytesMain\n\t__libc_start_main\n\t\n*** End stack trace ***\nUnknown PJRT_DEVICE 'CUDA'"
            },
            "pytorch_torch_compile_tvm": {
                "error": "backend='tvm' raised:\nModuleNotFoundError: No module named 'tvm'\n\nSet TORCH_LOGS=\"+dynamo\" and TORCHDYNAMO_VERBOSE=1 for more information\n\n\nYou can suppress this exception and fall back to eager by setting:\n    import torch._dynamo\n    torch._dynamo.config.suppress_errors = True\n"
            },
            "jax_flax_xla": {
                "ttft_s": 10.888854265213013
            },
            "tensorflow_eager": {
                "error": "Unrecognized configuration class <class 'transformers.models.electra.configuration_electra.ElectraConfig'> for this kind of AutoModel: TFAutoModelForCausalLM.\nModel type should be one of BertConfig, CamembertConfig, CTRLConfig, GPT2Config, GPT2Config, GPTJConfig, MistralConfig, OpenAIGPTConfig, OPTConfig, RemBertConfig, RobertaConfig, RobertaPreLayerNormConfig, RoFormerConfig, TransfoXLConfig, XGLMConfig, XLMConfig, XLMRobertaConfig, XLNetConfig."
            },
            "tensorflow_xla": {
                "error": "Unrecognized configuration class <class 'transformers.models.electra.configuration_electra.ElectraConfig'> for this kind of AutoModel: TFAutoModelForCausalLM.\nModel type should be one of BertConfig, CamembertConfig, CTRLConfig, GPT2Config, GPT2Config, GPTJConfig, MistralConfig, OpenAIGPTConfig, OPTConfig, RemBertConfig, RobertaConfig, RobertaPreLayerNormConfig, RoFormerConfig, TransfoXLConfig, XGLMConfig, XLMConfig, XLMRobertaConfig, XLNetConfig."
            },
            "jax_flax_eager": {
                "ttft_s": 10.6463623046875
            }
        }
    },
    {
        "ElectraForQuestionAnswering": {
            "pytorch_torch_compile_inductor": {
                "ttft_s": 9.929898977279663
            },
            "pytorch_torch_compile_eager": {
                "ttft_s": 2.784576177597046
            },
            "pytorch_torch_compile_cudagraphs": {
                "ttft_s": 6.560392618179321
            },
            "pytorch_torch_compile_onnxrt": {
                "ttft_s": 11.485021114349365
            },
            "pytorch_torch_compile_openxla": {
                "error": "torch_xla/csrc/runtime/pjrt_registry.cc:214 : Check failed: client \n*** Begin stack trace ***\n\ttsl::CurrentStackTrace()\n\ttorch_xla::runtime::InitializePjRt(std::string const&)\n\ttorch_xla::runtime::PjRtComputationClient::PjRtComputationClient()\n\t\n\ttorch_xla::runtime::GetComputationClient()\n\ttorch_xla::bridge::GetDefaultDevice()\n\ttorch_xla::bridge::GetCurrentDevice()\n\ttorch_xla::bridge::GetCurrentAtenDevice()\n\t\n\t\n\t\n\t_PyObject_MakeTpCall\n\t_PyEval_EvalFrameDefault\n\t\n\t_PyFunction_Vectorcall\n\t_PyEval_EvalFrameDefault\n\t\n\t_PyFunction_Vectorcall\n\t_PyEval_EvalFrameDefault\n\t\n\t_PyFunction_Vectorcall\n\t_PyEval_EvalFrameDefault\n\t\n\t_PyFunction_Vectorcall\n\t_PyEval_EvalFrameDefault\n\t\n\t_PyFunction_Vectorcall\n\t_PyEval_EvalFrameDefault\n\t\n\t_PyFunction_Vectorcall\n\t_PyEval_EvalFrameDefault\n\t\n\t_PyFunction_Vectorcall\n\t_PyEval_EvalFrameDefault\n\t\n\t_PyFunction_Vectorcall\n\t_PyEval_EvalFrameDefault\n\t\n\t_PyFunction_Vectorcall\n\tTHPFunction_apply(_object*, _object*)\n\t\n\tPyObject_Call\n\t_PyEval_EvalFrameDefault\n\t\n\t\n\t_PyEval_EvalFrameDefault\n\t\n\t_PyFunction_Vectorcall\n\t_PyEval_EvalFrameDefault\n\t\n\t_PyFunction_Vectorcall\n\t_PyEval_EvalFrameDefault\n\t\n\t_PyFunction_Vectorcall\n\t_PyEval_EvalFrameDefault\n\t\n\t_PyFunction_Vectorcall\n\t_PyEval_EvalFrameDefault\n\t\n\t_PyFunction_Vectorcall\n\t_PyEval_EvalFrameDefault\n\t\n\t\n\t\n\tPyObject_Call\n\t_PyEval_EvalFrameDefault\n\t\n\t\n\tPyObject_Call\n\t_PyEval_EvalFrameDefault\n\t\n\t\n\tPyObject_Call\n\t_PyEval_EvalFrameDefault\n\t\n\t_PyFunction_Vectorcall\n\tPyObject_Call\n\t_PyEval_EvalFrameDefault\n\t\n\t\n\tPyObject_Call\n\t_PyEval_EvalFrameDefault\n\t\n\t_PyFunction_Vectorcall\n\t_PyObject_FastCallDictTstate\n\t_PyObject_Call_Prepend\n\t\n\tPyObject_Call\n\t_PyEval_EvalFrameDefault\n\t\n\t_PyFunction_Vectorcall\n\t_PyEval_EvalFrameDefault\n\t_PyFunction_Vectorcall\n\t_PyEval_EvalFrameDefault\n\t\n\tPyEval_EvalCode\n\t\n\t\n\t\n\t_PyRun_SimpleFileObject\n\t_PyRun_AnyFileObject\n\tPy_RunMain\n\tPy_BytesMain\n\t__libc_start_main\n\t\n*** End stack trace ***\nUnknown PJRT_DEVICE 'CUDA'"
            },
            "pytorch_torch_compile_tvm": {
                "error": "backend='tvm' raised:\nModuleNotFoundError: No module named 'tvm'\n\nSet TORCH_LOGS=\"+dynamo\" and TORCHDYNAMO_VERBOSE=1 for more information\n\n\nYou can suppress this exception and fall back to eager by setting:\n    import torch._dynamo\n    torch._dynamo.config.suppress_errors = True\n"
            },
            "jax_flax_xla": {
                "error": "'jaxlib.xla_extension.ArrayImpl' object has no attribute 'split'"
            },
            "tensorflow_eager": {
                "ttft_s": 0.2368302345275879
            },
            "tensorflow_xla": {
                "ttft_s": 8.66569471359253
            }
        }
    },
    {
        "GoogleFnet": {
            "pytorch_torch_compile_inductor": {
                "ttft_s": 5.271390438079834
            },
            "pytorch_torch_compile_eager": {
                "ttft_s": 1.3178772926330566
            },
            "pytorch_torch_compile_cudagraphs": {
                "ttft_s": 4.006875514984131
            },
            "pytorch_torch_compile_onnxrt": {
                "ttft_s": 6.596152305603027
            },
            "pytorch_torch_compile_openxla": {
                "error": "torch_xla/csrc/runtime/pjrt_registry.cc:214 : Check failed: client \n*** Begin stack trace ***\n\ttsl::CurrentStackTrace()\n\ttorch_xla::runtime::InitializePjRt(std::string const&)\n\ttorch_xla::runtime::PjRtComputationClient::PjRtComputationClient()\n\t\n\ttorch_xla::runtime::GetComputationClient()\n\ttorch_xla::bridge::GetDefaultDevice()\n\ttorch_xla::bridge::GetCurrentDevice()\n\ttorch_xla::bridge::GetCurrentAtenDevice()\n\t\n\t\n\t\n\t_PyObject_MakeTpCall\n\t_PyEval_EvalFrameDefault\n\t\n\t_PyFunction_Vectorcall\n\t_PyEval_EvalFrameDefault\n\t\n\t_PyFunction_Vectorcall\n\t_PyEval_EvalFrameDefault\n\t\n\t_PyFunction_Vectorcall\n\t_PyEval_EvalFrameDefault\n\t\n\t_PyFunction_Vectorcall\n\t_PyEval_EvalFrameDefault\n\t\n\t_PyFunction_Vectorcall\n\t_PyEval_EvalFrameDefault\n\t\n\t_PyFunction_Vectorcall\n\t_PyEval_EvalFrameDefault\n\t\n\t_PyFunction_Vectorcall\n\t_PyEval_EvalFrameDefault\n\t\n\t_PyFunction_Vectorcall\n\t_PyEval_EvalFrameDefault\n\t\n\t_PyFunction_Vectorcall\n\tTHPFunction_apply(_object*, _object*)\n\t\n\tPyObject_Call\n\t_PyEval_EvalFrameDefault\n\t\n\t\n\t_PyEval_EvalFrameDefault\n\t\n\t_PyFunction_Vectorcall\n\t_PyEval_EvalFrameDefault\n\t\n\t_PyFunction_Vectorcall\n\t_PyEval_EvalFrameDefault\n\t\n\t_PyFunction_Vectorcall\n\t_PyEval_EvalFrameDefault\n\t\n\t_PyFunction_Vectorcall\n\t_PyEval_EvalFrameDefault\n\t\n\t_PyFunction_Vectorcall\n\t_PyEval_EvalFrameDefault\n\t\n\t\n\t\n\tPyObject_Call\n\t_PyEval_EvalFrameDefault\n\t\n\t\n\tPyObject_Call\n\t_PyEval_EvalFrameDefault\n\t\n\t\n\tPyObject_Call\n\t_PyEval_EvalFrameDefault\n\t\n\t_PyFunction_Vectorcall\n\tPyObject_Call\n\t_PyEval_EvalFrameDefault\n\t\n\t\n\tPyObject_Call\n\t_PyEval_EvalFrameDefault\n\t\n\t_PyFunction_Vectorcall\n\t_PyObject_FastCallDictTstate\n\t_PyObject_Call_Prepend\n\t\n\tPyObject_Call\n\t_PyEval_EvalFrameDefault\n\t\n\t_PyFunction_Vectorcall\n\t_PyEval_EvalFrameDefault\n\t_PyFunction_Vectorcall\n\t_PyEval_EvalFrameDefault\n\t\n\tPyEval_EvalCode\n\t\n\t\n\t\n\t_PyRun_SimpleFileObject\n\t_PyRun_AnyFileObject\n\tPy_RunMain\n\tPy_BytesMain\n\t__libc_start_main\n\t\n*** End stack trace ***\nUnknown PJRT_DEVICE 'CUDA'"
            },
            "pytorch_torch_compile_tvm": {
                "error": "backend='tvm' raised:\nModuleNotFoundError: No module named 'tvm'\n\nSet TORCH_LOGS=\"+dynamo\" and TORCHDYNAMO_VERBOSE=1 for more information\n\n\nYou can suppress this exception and fall back to eager by setting:\n    import torch._dynamo\n    torch._dynamo.config.suppress_errors = True\n"
            },
            "jax_flax_xla": {
                "error": "Unrecognized configuration class <class 'transformers.models.fnet.configuration_fnet.FNetConfig'> for this kind of AutoModel: FlaxAutoModel.\nModel type should be one of AlbertConfig, BartConfig, BeitConfig, BertConfig, BigBirdConfig, BlenderbotConfig, BlenderbotSmallConfig, BloomConfig, CLIPConfig, Dinov2Config, DistilBertConfig, ElectraConfig, GemmaConfig, GPT2Config, GPT2Config, GPTNeoConfig, GPTJConfig, LlamaConfig, LongT5Config, MarianConfig, MBartConfig, MistralConfig, MT5Config, OPTConfig, PegasusConfig, RegNetConfig, ResNetConfig, RobertaConfig, RobertaPreLayerNormConfig, RoFormerConfig, T5Config, VisionTextDualEncoderConfig, ViTConfig, Wav2Vec2Config, WhisperConfig, XGLMConfig, XLMRobertaConfig."
            },
            "tensorflow_eager": {
                "error": "Unrecognized configuration class <class 'transformers.models.fnet.configuration_fnet.FNetConfig'> for this kind of AutoModel: TFAutoModel.\nModel type should be one of AlbertConfig, BartConfig, BertConfig, BlenderbotConfig, BlenderbotSmallConfig, BlipConfig, CamembertConfig, CLIPConfig, ConvBertConfig, ConvNextConfig, ConvNextV2Config, CTRLConfig, CvtConfig, Data2VecVisionConfig, DebertaConfig, DebertaV2Config, DeiTConfig, DistilBertConfig, DPRConfig, EfficientFormerConfig, ElectraConfig, EsmConfig, FlaubertConfig, FunnelConfig, GPT2Config, GPT2Config, GPTJConfig, GroupViTConfig, HubertConfig, IdeficsConfig, LayoutLMConfig, LayoutLMv3Config, LEDConfig, LongformerConfig, LxmertConfig, MarianConfig, MBartConfig, MistralConfig, MobileBertConfig, MobileViTConfig, MPNetConfig, MT5Config, OpenAIGPTConfig, OPTConfig, PegasusConfig, RegNetConfig, RemBertConfig, ResNetConfig, RobertaConfig, RobertaPreLayerNormConfig, RoFormerConfig, SamConfig, SegformerConfig, Speech2TextConfig, SwiftFormerConfig, SwinConfig, T5Config, TapasConfig, TransfoXLConfig, VisionTextDualEncoderConfig, ViTConfig, ViTMAEConfig, Wav2Vec2Config, WhisperConfig, XGLMConfig, XLMConfig, XLMRobertaConfig, XLNetConfig."
            },
            "tensorflow_xla": {
                "error": "Unrecognized configuration class <class 'transformers.models.fnet.configuration_fnet.FNetConfig'> for this kind of AutoModel: TFAutoModel.\nModel type should be one of AlbertConfig, BartConfig, BertConfig, BlenderbotConfig, BlenderbotSmallConfig, BlipConfig, CamembertConfig, CLIPConfig, ConvBertConfig, ConvNextConfig, ConvNextV2Config, CTRLConfig, CvtConfig, Data2VecVisionConfig, DebertaConfig, DebertaV2Config, DeiTConfig, DistilBertConfig, DPRConfig, EfficientFormerConfig, ElectraConfig, EsmConfig, FlaubertConfig, FunnelConfig, GPT2Config, GPT2Config, GPTJConfig, GroupViTConfig, HubertConfig, IdeficsConfig, LayoutLMConfig, LayoutLMv3Config, LEDConfig, LongformerConfig, LxmertConfig, MarianConfig, MBartConfig, MistralConfig, MobileBertConfig, MobileViTConfig, MPNetConfig, MT5Config, OpenAIGPTConfig, OPTConfig, PegasusConfig, RegNetConfig, RemBertConfig, ResNetConfig, RobertaConfig, RobertaPreLayerNormConfig, RoFormerConfig, SamConfig, SegformerConfig, Speech2TextConfig, SwiftFormerConfig, SwinConfig, T5Config, TapasConfig, TransfoXLConfig, VisionTextDualEncoderConfig, ViTConfig, ViTMAEConfig, Wav2Vec2Config, WhisperConfig, XGLMConfig, XLMConfig, XLMRobertaConfig, XLNetConfig."
            }
        }
    },
    {
        "GPT2ForSequenceClassification": {
            "pytorch_torch_compile_inductor": {
                "ttft_s": 9.028907537460327
            },
            "pytorch_torch_compile_eager": {
                "ttft_s": 2.8551604747772217
            },
            "pytorch_torch_compile_cudagraphs": {
                "ttft_s": 6.803621292114258
            },
            "pytorch_torch_compile_onnxrt": {
                "ttft_s": 17.39772319793701
            },
            "pytorch_torch_compile_openxla": {
                "error": "torch_xla/csrc/runtime/pjrt_registry.cc:214 : Check failed: client \n*** Begin stack trace ***\n\ttsl::CurrentStackTrace()\n\ttorch_xla::runtime::InitializePjRt(std::string const&)\n\ttorch_xla::runtime::PjRtComputationClient::PjRtComputationClient()\n\t\n\ttorch_xla::runtime::GetComputationClient()\n\ttorch_xla::bridge::GetDefaultDevice()\n\ttorch_xla::bridge::GetCurrentDevice()\n\ttorch_xla::bridge::GetCurrentAtenDevice()\n\t\n\t\n\t\n\t_PyObject_MakeTpCall\n\t_PyEval_EvalFrameDefault\n\t\n\t_PyFunction_Vectorcall\n\t_PyEval_EvalFrameDefault\n\t\n\t_PyFunction_Vectorcall\n\t_PyEval_EvalFrameDefault\n\t\n\t_PyFunction_Vectorcall\n\t_PyEval_EvalFrameDefault\n\t\n\t_PyFunction_Vectorcall\n\t_PyEval_EvalFrameDefault\n\t\n\t_PyFunction_Vectorcall\n\t_PyEval_EvalFrameDefault\n\t\n\t_PyFunction_Vectorcall\n\t_PyEval_EvalFrameDefault\n\t\n\t_PyFunction_Vectorcall\n\t_PyEval_EvalFrameDefault\n\t\n\t_PyFunction_Vectorcall\n\t_PyEval_EvalFrameDefault\n\t\n\t_PyFunction_Vectorcall\n\tTHPFunction_apply(_object*, _object*)\n\t\n\tPyObject_Call\n\t_PyEval_EvalFrameDefault\n\t\n\t\n\t_PyEval_EvalFrameDefault\n\t\n\t_PyFunction_Vectorcall\n\t_PyEval_EvalFrameDefault\n\t\n\t_PyFunction_Vectorcall\n\t_PyEval_EvalFrameDefault\n\t\n\t_PyFunction_Vectorcall\n\t_PyEval_EvalFrameDefault\n\t\n\t_PyFunction_Vectorcall\n\t_PyEval_EvalFrameDefault\n\t\n\t_PyFunction_Vectorcall\n\t_PyEval_EvalFrameDefault\n\t\n\t\n\t\n\tPyObject_Call\n\t_PyEval_EvalFrameDefault\n\t\n\t\n\tPyObject_Call\n\t_PyEval_EvalFrameDefault\n\t\n\t\n\tPyObject_Call\n\t_PyEval_EvalFrameDefault\n\t\n\t_PyFunction_Vectorcall\n\tPyObject_Call\n\t_PyEval_EvalFrameDefault\n\t\n\t\n\tPyObject_Call\n\t_PyEval_EvalFrameDefault\n\t\n\t_PyFunction_Vectorcall\n\t_PyObject_FastCallDictTstate\n\t_PyObject_Call_Prepend\n\t\n\tPyObject_Call\n\t_PyEval_EvalFrameDefault\n\t\n\t_PyFunction_Vectorcall\n\t_PyEval_EvalFrameDefault\n\t_PyFunction_Vectorcall\n\t_PyEval_EvalFrameDefault\n\t\n\tPyEval_EvalCode\n\t\n\t\n\t\n\t_PyRun_SimpleFileObject\n\t_PyRun_AnyFileObject\n\tPy_RunMain\n\tPy_BytesMain\n\t__libc_start_main\n\t\n*** End stack trace ***\nUnknown PJRT_DEVICE 'CUDA'"
            },
            "pytorch_torch_compile_tvm": {
                "error": "backend='tvm' raised:\nModuleNotFoundError: No module named 'tvm'\n\nSet TORCH_LOGS=\"+dynamo\" and TORCHDYNAMO_VERBOSE=1 for more information\n\n\nYou can suppress this exception and fall back to eager by setting:\n    import torch._dynamo\n    torch._dynamo.config.suppress_errors = True\n"
            },
            "jax_flax_xla": {
                "ttft_s": 2.5559678077697754
            },
            "tensorflow_eager": {
                "error": "Asking to pad but the tokenizer does not have a padding token. Please select a token to use as `pad_token` `(tokenizer.pad_token = tokenizer.eos_token e.g.)` or add a new pad token via `tokenizer.add_special_tokens({'pad_token': '[PAD]'})`."
            },
            "tensorflow_xla": {
                "error": "Asking to pad but the tokenizer does not have a padding token. Please select a token to use as `pad_token` `(tokenizer.pad_token = tokenizer.eos_token e.g.)` or add a new pad token via `tokenizer.add_special_tokens({'pad_token': '[PAD]'})`."
            },
            "jax_flax_eager": {
                "ttft_s": 0.42253780364990234
            }
        }
    },
    {
        "LayoutLMForMaskedLM": {
            "pytorch_torch_compile_inductor": {
                "ttft_s": 10.699179649353027
            },
            "pytorch_torch_compile_eager": {
                "ttft_s": 3.4719295501708984
            },
            "pytorch_torch_compile_cudagraphs": {
                "ttft_s": 7.4037346839904785
            },
            "pytorch_torch_compile_onnxrt": {
                "ttft_s": 17.847732305526733
            },
            "pytorch_torch_compile_openxla": {
                "error": "torch_xla/csrc/runtime/pjrt_registry.cc:214 : Check failed: client \n*** Begin stack trace ***\n\ttsl::CurrentStackTrace()\n\ttorch_xla::runtime::InitializePjRt(std::string const&)\n\ttorch_xla::runtime::PjRtComputationClient::PjRtComputationClient()\n\t\n\ttorch_xla::runtime::GetComputationClient()\n\ttorch_xla::bridge::GetDefaultDevice()\n\ttorch_xla::bridge::GetCurrentDevice()\n\ttorch_xla::bridge::GetCurrentAtenDevice()\n\t\n\t\n\t\n\t_PyObject_MakeTpCall\n\t_PyEval_EvalFrameDefault\n\t\n\t_PyFunction_Vectorcall\n\t_PyEval_EvalFrameDefault\n\t\n\t_PyFunction_Vectorcall\n\t_PyEval_EvalFrameDefault\n\t\n\t_PyFunction_Vectorcall\n\t_PyEval_EvalFrameDefault\n\t\n\t_PyFunction_Vectorcall\n\t_PyEval_EvalFrameDefault\n\t\n\t_PyFunction_Vectorcall\n\t_PyEval_EvalFrameDefault\n\t\n\t_PyFunction_Vectorcall\n\t_PyEval_EvalFrameDefault\n\t\n\t_PyFunction_Vectorcall\n\t_PyEval_EvalFrameDefault\n\t\n\t_PyFunction_Vectorcall\n\t_PyEval_EvalFrameDefault\n\t\n\t_PyFunction_Vectorcall\n\tTHPFunction_apply(_object*, _object*)\n\t\n\tPyObject_Call\n\t_PyEval_EvalFrameDefault\n\t\n\t\n\t_PyEval_EvalFrameDefault\n\t\n\t_PyFunction_Vectorcall\n\t_PyEval_EvalFrameDefault\n\t\n\t_PyFunction_Vectorcall\n\t_PyEval_EvalFrameDefault\n\t\n\t_PyFunction_Vectorcall\n\t_PyEval_EvalFrameDefault\n\t\n\t_PyFunction_Vectorcall\n\t_PyEval_EvalFrameDefault\n\t\n\t_PyFunction_Vectorcall\n\t_PyEval_EvalFrameDefault\n\t\n\t\n\t\n\tPyObject_Call\n\t_PyEval_EvalFrameDefault\n\t\n\t\n\tPyObject_Call\n\t_PyEval_EvalFrameDefault\n\t\n\t\n\tPyObject_Call\n\t_PyEval_EvalFrameDefault\n\t\n\t_PyFunction_Vectorcall\n\tPyObject_Call\n\t_PyEval_EvalFrameDefault\n\t\n\t\n\tPyObject_Call\n\t_PyEval_EvalFrameDefault\n\t\n\t_PyFunction_Vectorcall\n\t_PyObject_FastCallDictTstate\n\t_PyObject_Call_Prepend\n\t\n\tPyObject_Call\n\t_PyEval_EvalFrameDefault\n\t\n\t_PyFunction_Vectorcall\n\t_PyEval_EvalFrameDefault\n\t_PyFunction_Vectorcall\n\t_PyEval_EvalFrameDefault\n\t\n\tPyEval_EvalCode\n\t\n\t\n\t\n\t_PyRun_SimpleFileObject\n\t_PyRun_AnyFileObject\n\tPy_RunMain\n\tPy_BytesMain\n\t__libc_start_main\n\t\n*** End stack trace ***\nUnknown PJRT_DEVICE 'CUDA'"
            },
            "pytorch_torch_compile_tvm": {
                "error": "backend='tvm' raised:\nModuleNotFoundError: No module named 'tvm'\n\nSet TORCH_LOGS=\"+dynamo\" and TORCHDYNAMO_VERBOSE=1 for more information\n\n\nYou can suppress this exception and fall back to eager by setting:\n    import torch._dynamo\n    torch._dynamo.config.suppress_errors = True\n"
            },
            "jax_flax_xla": {
                "error": "Unrecognized configuration class <class 'transformers.models.layoutlm.configuration_layoutlm.LayoutLMConfig'> for this kind of AutoModel: FlaxAutoModel.\nModel type should be one of AlbertConfig, BartConfig, BeitConfig, BertConfig, BigBirdConfig, BlenderbotConfig, BlenderbotSmallConfig, BloomConfig, CLIPConfig, Dinov2Config, DistilBertConfig, ElectraConfig, GemmaConfig, GPT2Config, GPT2Config, GPTNeoConfig, GPTJConfig, LlamaConfig, LongT5Config, MarianConfig, MBartConfig, MistralConfig, MT5Config, OPTConfig, PegasusConfig, RegNetConfig, ResNetConfig, RobertaConfig, RobertaPreLayerNormConfig, RoFormerConfig, T5Config, VisionTextDualEncoderConfig, ViTConfig, Wav2Vec2Config, WhisperConfig, XGLMConfig, XLMRobertaConfig."
            },
            "tensorflow_eager": {
                "ttft_s": 0.2306368350982666
            },
            "tensorflow_xla": {
                "ttft_s": 4.881600379943848
            }
        }
    },
    {
        "LayoutLMForSequenceClassification": {
            "pytorch_torch_compile_inductor": {
                "ttft_s": 10.769028902053833
            },
            "pytorch_torch_compile_eager": {
                "ttft_s": 3.4622790813446045
            },
            "pytorch_torch_compile_cudagraphs": {
                "ttft_s": 7.401792287826538
            },
            "pytorch_torch_compile_onnxrt": {
                "ttft_s": 12.355679273605347
            },
            "pytorch_torch_compile_openxla": {
                "error": "torch_xla/csrc/runtime/pjrt_registry.cc:214 : Check failed: client \n*** Begin stack trace ***\n\ttsl::CurrentStackTrace()\n\ttorch_xla::runtime::InitializePjRt(std::string const&)\n\ttorch_xla::runtime::PjRtComputationClient::PjRtComputationClient()\n\t\n\ttorch_xla::runtime::GetComputationClient()\n\ttorch_xla::bridge::GetDefaultDevice()\n\ttorch_xla::bridge::GetCurrentDevice()\n\ttorch_xla::bridge::GetCurrentAtenDevice()\n\t\n\t\n\t\n\t_PyObject_MakeTpCall\n\t_PyEval_EvalFrameDefault\n\t\n\t_PyFunction_Vectorcall\n\t_PyEval_EvalFrameDefault\n\t\n\t_PyFunction_Vectorcall\n\t_PyEval_EvalFrameDefault\n\t\n\t_PyFunction_Vectorcall\n\t_PyEval_EvalFrameDefault\n\t\n\t_PyFunction_Vectorcall\n\t_PyEval_EvalFrameDefault\n\t\n\t_PyFunction_Vectorcall\n\t_PyEval_EvalFrameDefault\n\t\n\t_PyFunction_Vectorcall\n\t_PyEval_EvalFrameDefault\n\t\n\t_PyFunction_Vectorcall\n\t_PyEval_EvalFrameDefault\n\t\n\t_PyFunction_Vectorcall\n\t_PyEval_EvalFrameDefault\n\t\n\t_PyFunction_Vectorcall\n\tTHPFunction_apply(_object*, _object*)\n\t\n\tPyObject_Call\n\t_PyEval_EvalFrameDefault\n\t\n\t\n\t_PyEval_EvalFrameDefault\n\t\n\t_PyFunction_Vectorcall\n\t_PyEval_EvalFrameDefault\n\t\n\t_PyFunction_Vectorcall\n\t_PyEval_EvalFrameDefault\n\t\n\t_PyFunction_Vectorcall\n\t_PyEval_EvalFrameDefault\n\t\n\t_PyFunction_Vectorcall\n\t_PyEval_EvalFrameDefault\n\t\n\t_PyFunction_Vectorcall\n\t_PyEval_EvalFrameDefault\n\t\n\t\n\t\n\tPyObject_Call\n\t_PyEval_EvalFrameDefault\n\t\n\t\n\tPyObject_Call\n\t_PyEval_EvalFrameDefault\n\t\n\t\n\tPyObject_Call\n\t_PyEval_EvalFrameDefault\n\t\n\t_PyFunction_Vectorcall\n\tPyObject_Call\n\t_PyEval_EvalFrameDefault\n\t\n\t\n\tPyObject_Call\n\t_PyEval_EvalFrameDefault\n\t\n\t_PyFunction_Vectorcall\n\t_PyObject_FastCallDictTstate\n\t_PyObject_Call_Prepend\n\t\n\tPyObject_Call\n\t_PyEval_EvalFrameDefault\n\t\n\t_PyFunction_Vectorcall\n\t_PyEval_EvalFrameDefault\n\t_PyFunction_Vectorcall\n\t_PyEval_EvalFrameDefault\n\t\n\tPyEval_EvalCode\n\t\n\t\n\t\n\t_PyRun_SimpleFileObject\n\t_PyRun_AnyFileObject\n\tPy_RunMain\n\tPy_BytesMain\n\t__libc_start_main\n\t\n*** End stack trace ***\nUnknown PJRT_DEVICE 'CUDA'"
            },
            "pytorch_torch_compile_tvm": {
                "error": "backend='tvm' raised:\nModuleNotFoundError: No module named 'tvm'\n\nSet TORCH_LOGS=\"+dynamo\" and TORCHDYNAMO_VERBOSE=1 for more information\n\n\nYou can suppress this exception and fall back to eager by setting:\n    import torch._dynamo\n    torch._dynamo.config.suppress_errors = True\n"
            },
            "jax_flax_xla": {
                "error": "Unrecognized configuration class <class 'transformers.models.layoutlm.configuration_layoutlm.LayoutLMConfig'> for this kind of AutoModel: FlaxAutoModel.\nModel type should be one of AlbertConfig, BartConfig, BeitConfig, BertConfig, BigBirdConfig, BlenderbotConfig, BlenderbotSmallConfig, BloomConfig, CLIPConfig, Dinov2Config, DistilBertConfig, ElectraConfig, GemmaConfig, GPT2Config, GPT2Config, GPTNeoConfig, GPTJConfig, LlamaConfig, LongT5Config, MarianConfig, MBartConfig, MistralConfig, MT5Config, OPTConfig, PegasusConfig, RegNetConfig, ResNetConfig, RobertaConfig, RobertaPreLayerNormConfig, RoFormerConfig, T5Config, VisionTextDualEncoderConfig, ViTConfig, Wav2Vec2Config, WhisperConfig, XGLMConfig, XLMRobertaConfig."
            },
            "tensorflow_eager": {
                "ttft_s": 0.23071599006652832
            },
            "tensorflow_xla": {
                "ttft_s": 3.3197035789489746
            }
        }
    },
    {
        "M2M100ForConditionalGeneration": {
            "error": "AttributeError(\"'M2M100ForConditionalGeneration' object has no attribute '_shift_right'\")"
        }
    },
    {
        "MBartForConditionalGeneration": {
            "error": "AttributeError(\"'MBartForConditionalGeneration' object has no attribute '_shift_right'\")"
        }
    },
    {
        "MegatronBertForCausalLM": {
            "pytorch_torch_compile_inductor": {
                "ttft_s": 22.132827758789062
            },
            "pytorch_torch_compile_eager": {
                "ttft_s": 5.754762411117554
            },
            "pytorch_torch_compile_cudagraphs": {
                "ttft_s": 14.585710525512695
            },
            "pytorch_torch_compile_onnxrt": {
                "ttft_s": 32.08271050453186
            },
            "pytorch_torch_compile_openxla": {
                "error": "torch_xla/csrc/runtime/pjrt_registry.cc:214 : Check failed: client \n*** Begin stack trace ***\n\ttsl::CurrentStackTrace()\n\ttorch_xla::runtime::InitializePjRt(std::string const&)\n\ttorch_xla::runtime::PjRtComputationClient::PjRtComputationClient()\n\t\n\ttorch_xla::runtime::GetComputationClient()\n\ttorch_xla::bridge::GetDefaultDevice()\n\ttorch_xla::bridge::GetCurrentDevice()\n\ttorch_xla::bridge::GetCurrentAtenDevice()\n\t\n\t\n\t\n\t_PyObject_MakeTpCall\n\t_PyEval_EvalFrameDefault\n\t\n\t_PyFunction_Vectorcall\n\t_PyEval_EvalFrameDefault\n\t\n\t_PyFunction_Vectorcall\n\t_PyEval_EvalFrameDefault\n\t\n\t_PyFunction_Vectorcall\n\t_PyEval_EvalFrameDefault\n\t\n\t_PyFunction_Vectorcall\n\t_PyEval_EvalFrameDefault\n\t\n\t_PyFunction_Vectorcall\n\t_PyEval_EvalFrameDefault\n\t\n\t_PyFunction_Vectorcall\n\t_PyEval_EvalFrameDefault\n\t\n\t_PyFunction_Vectorcall\n\t_PyEval_EvalFrameDefault\n\t\n\t_PyFunction_Vectorcall\n\t_PyEval_EvalFrameDefault\n\t\n\t_PyFunction_Vectorcall\n\tTHPFunction_apply(_object*, _object*)\n\t\n\tPyObject_Call\n\t_PyEval_EvalFrameDefault\n\t\n\t\n\t_PyEval_EvalFrameDefault\n\t\n\t_PyFunction_Vectorcall\n\t_PyEval_EvalFrameDefault\n\t\n\t_PyFunction_Vectorcall\n\t_PyEval_EvalFrameDefault\n\t\n\t_PyFunction_Vectorcall\n\t_PyEval_EvalFrameDefault\n\t\n\t_PyFunction_Vectorcall\n\t_PyEval_EvalFrameDefault\n\t\n\t_PyFunction_Vectorcall\n\t_PyEval_EvalFrameDefault\n\t\n\t\n\t\n\tPyObject_Call\n\t_PyEval_EvalFrameDefault\n\t\n\t\n\tPyObject_Call\n\t_PyEval_EvalFrameDefault\n\t\n\t\n\tPyObject_Call\n\t_PyEval_EvalFrameDefault\n\t\n\t_PyFunction_Vectorcall\n\tPyObject_Call\n\t_PyEval_EvalFrameDefault\n\t\n\t\n\tPyObject_Call\n\t_PyEval_EvalFrameDefault\n\t\n\t_PyFunction_Vectorcall\n\t_PyObject_FastCallDictTstate\n\t_PyObject_Call_Prepend\n\t\n\tPyObject_Call\n\t_PyEval_EvalFrameDefault\n\t\n\t_PyFunction_Vectorcall\n\t_PyEval_EvalFrameDefault\n\t_PyFunction_Vectorcall\n\t_PyEval_EvalFrameDefault\n\t\n\tPyEval_EvalCode\n\t\n\t\n\t\n\t_PyRun_SimpleFileObject\n\t_PyRun_AnyFileObject\n\tPy_RunMain\n\tPy_BytesMain\n\t__libc_start_main\n\t\n*** End stack trace ***\nUnknown PJRT_DEVICE 'CUDA'"
            },
            "pytorch_torch_compile_tvm": {
                "error": "backend='tvm' raised:\nModuleNotFoundError: No module named 'tvm'\n\nSet TORCH_LOGS=\"+dynamo\" and TORCHDYNAMO_VERBOSE=1 for more information\n\n\nYou can suppress this exception and fall back to eager by setting:\n    import torch._dynamo\n    torch._dynamo.config.suppress_errors = True\n"
            },
            "jax_flax_xla": {
                "error": "Unrecognized configuration class <class 'transformers.models.megatron_bert.configuration_megatron_bert.MegatronBertConfig'> for this kind of AutoModel: FlaxAutoModel.\nModel type should be one of AlbertConfig, BartConfig, BeitConfig, BertConfig, BigBirdConfig, BlenderbotConfig, BlenderbotSmallConfig, BloomConfig, CLIPConfig, Dinov2Config, DistilBertConfig, ElectraConfig, GemmaConfig, GPT2Config, GPT2Config, GPTNeoConfig, GPTJConfig, LlamaConfig, LongT5Config, MarianConfig, MBartConfig, MistralConfig, MT5Config, OPTConfig, PegasusConfig, RegNetConfig, ResNetConfig, RobertaConfig, RobertaPreLayerNormConfig, RoFormerConfig, T5Config, VisionTextDualEncoderConfig, ViTConfig, Wav2Vec2Config, WhisperConfig, XGLMConfig, XLMRobertaConfig."
            },
            "tensorflow_eager": {
                "error": "Unrecognized configuration class <class 'transformers.models.megatron_bert.configuration_megatron_bert.MegatronBertConfig'> for this kind of AutoModel: TFAutoModel.\nModel type should be one of AlbertConfig, BartConfig, BertConfig, BlenderbotConfig, BlenderbotSmallConfig, BlipConfig, CamembertConfig, CLIPConfig, ConvBertConfig, ConvNextConfig, ConvNextV2Config, CTRLConfig, CvtConfig, Data2VecVisionConfig, DebertaConfig, DebertaV2Config, DeiTConfig, DistilBertConfig, DPRConfig, EfficientFormerConfig, ElectraConfig, EsmConfig, FlaubertConfig, FunnelConfig, GPT2Config, GPT2Config, GPTJConfig, GroupViTConfig, HubertConfig, IdeficsConfig, LayoutLMConfig, LayoutLMv3Config, LEDConfig, LongformerConfig, LxmertConfig, MarianConfig, MBartConfig, MistralConfig, MobileBertConfig, MobileViTConfig, MPNetConfig, MT5Config, OpenAIGPTConfig, OPTConfig, PegasusConfig, RegNetConfig, RemBertConfig, ResNetConfig, RobertaConfig, RobertaPreLayerNormConfig, RoFormerConfig, SamConfig, SegformerConfig, Speech2TextConfig, SwiftFormerConfig, SwinConfig, T5Config, TapasConfig, TransfoXLConfig, VisionTextDualEncoderConfig, ViTConfig, ViTMAEConfig, Wav2Vec2Config, WhisperConfig, XGLMConfig, XLMConfig, XLMRobertaConfig, XLNetConfig."
            },
            "tensorflow_xla": {
                "error": "Unrecognized configuration class <class 'transformers.models.megatron_bert.configuration_megatron_bert.MegatronBertConfig'> for this kind of AutoModel: TFAutoModel.\nModel type should be one of AlbertConfig, BartConfig, BertConfig, BlenderbotConfig, BlenderbotSmallConfig, BlipConfig, CamembertConfig, CLIPConfig, ConvBertConfig, ConvNextConfig, ConvNextV2Config, CTRLConfig, CvtConfig, Data2VecVisionConfig, DebertaConfig, DebertaV2Config, DeiTConfig, DistilBertConfig, DPRConfig, EfficientFormerConfig, ElectraConfig, EsmConfig, FlaubertConfig, FunnelConfig, GPT2Config, GPT2Config, GPTJConfig, GroupViTConfig, HubertConfig, IdeficsConfig, LayoutLMConfig, LayoutLMv3Config, LEDConfig, LongformerConfig, LxmertConfig, MarianConfig, MBartConfig, MistralConfig, MobileBertConfig, MobileViTConfig, MPNetConfig, MT5Config, OpenAIGPTConfig, OPTConfig, PegasusConfig, RegNetConfig, RemBertConfig, ResNetConfig, RobertaConfig, RobertaPreLayerNormConfig, RoFormerConfig, SamConfig, SegformerConfig, Speech2TextConfig, SwiftFormerConfig, SwinConfig, T5Config, TapasConfig, TransfoXLConfig, VisionTextDualEncoderConfig, ViTConfig, ViTMAEConfig, Wav2Vec2Config, WhisperConfig, XGLMConfig, XLMConfig, XLMRobertaConfig, XLNetConfig."
            }
        }
    },
    {
        "MegatronBertForQuestionAnswering": {
            "pytorch_torch_compile_inductor": {
                "ttft_s": 19.139528274536133
            },
            "pytorch_torch_compile_eager": {
                "ttft_s": 5.437692403793335
            },
            "pytorch_torch_compile_cudagraphs": {
                "ttft_s": 13.016801834106445
            },
            "pytorch_torch_compile_onnxrt": {
                "ttft_s": 30.763964653015137
            },
            "pytorch_torch_compile_openxla": {
                "error": "torch_xla/csrc/runtime/pjrt_registry.cc:214 : Check failed: client \n*** Begin stack trace ***\n\ttsl::CurrentStackTrace()\n\ttorch_xla::runtime::InitializePjRt(std::string const&)\n\ttorch_xla::runtime::PjRtComputationClient::PjRtComputationClient()\n\t\n\ttorch_xla::runtime::GetComputationClient()\n\ttorch_xla::bridge::GetDefaultDevice()\n\ttorch_xla::bridge::GetCurrentDevice()\n\ttorch_xla::bridge::GetCurrentAtenDevice()\n\t\n\t\n\t\n\t_PyObject_MakeTpCall\n\t_PyEval_EvalFrameDefault\n\t\n\t_PyFunction_Vectorcall\n\t_PyEval_EvalFrameDefault\n\t\n\t_PyFunction_Vectorcall\n\t_PyEval_EvalFrameDefault\n\t\n\t_PyFunction_Vectorcall\n\t_PyEval_EvalFrameDefault\n\t\n\t_PyFunction_Vectorcall\n\t_PyEval_EvalFrameDefault\n\t\n\t_PyFunction_Vectorcall\n\t_PyEval_EvalFrameDefault\n\t\n\t_PyFunction_Vectorcall\n\t_PyEval_EvalFrameDefault\n\t\n\t_PyFunction_Vectorcall\n\t_PyEval_EvalFrameDefault\n\t\n\t_PyFunction_Vectorcall\n\t_PyEval_EvalFrameDefault\n\t\n\t_PyFunction_Vectorcall\n\tTHPFunction_apply(_object*, _object*)\n\t\n\tPyObject_Call\n\t_PyEval_EvalFrameDefault\n\t\n\t\n\t_PyEval_EvalFrameDefault\n\t\n\t_PyFunction_Vectorcall\n\t_PyEval_EvalFrameDefault\n\t\n\t_PyFunction_Vectorcall\n\t_PyEval_EvalFrameDefault\n\t\n\t_PyFunction_Vectorcall\n\t_PyEval_EvalFrameDefault\n\t\n\t_PyFunction_Vectorcall\n\t_PyEval_EvalFrameDefault\n\t\n\t_PyFunction_Vectorcall\n\t_PyEval_EvalFrameDefault\n\t\n\t\n\t\n\tPyObject_Call\n\t_PyEval_EvalFrameDefault\n\t\n\t\n\tPyObject_Call\n\t_PyEval_EvalFrameDefault\n\t\n\t\n\tPyObject_Call\n\t_PyEval_EvalFrameDefault\n\t\n\t_PyFunction_Vectorcall\n\tPyObject_Call\n\t_PyEval_EvalFrameDefault\n\t\n\t\n\tPyObject_Call\n\t_PyEval_EvalFrameDefault\n\t\n\t_PyFunction_Vectorcall\n\t_PyObject_FastCallDictTstate\n\t_PyObject_Call_Prepend\n\t\n\tPyObject_Call\n\t_PyEval_EvalFrameDefault\n\t\n\t_PyFunction_Vectorcall\n\t_PyEval_EvalFrameDefault\n\t_PyFunction_Vectorcall\n\t_PyEval_EvalFrameDefault\n\t\n\tPyEval_EvalCode\n\t\n\t\n\t\n\t_PyRun_SimpleFileObject\n\t_PyRun_AnyFileObject\n\tPy_RunMain\n\tPy_BytesMain\n\t__libc_start_main\n\t\n*** End stack trace ***\nUnknown PJRT_DEVICE 'CUDA'"
            },
            "pytorch_torch_compile_tvm": {
                "error": "backend='tvm' raised:\nModuleNotFoundError: No module named 'tvm'\n\nSet TORCH_LOGS=\"+dynamo\" and TORCHDYNAMO_VERBOSE=1 for more information\n\n\nYou can suppress this exception and fall back to eager by setting:\n    import torch._dynamo\n    torch._dynamo.config.suppress_errors = True\n"
            },
            "jax_flax_xla": {
                "error": "Unrecognized configuration class <class 'transformers.models.megatron_bert.configuration_megatron_bert.MegatronBertConfig'> for this kind of AutoModel: FlaxAutoModel.\nModel type should be one of AlbertConfig, BartConfig, BeitConfig, BertConfig, BigBirdConfig, BlenderbotConfig, BlenderbotSmallConfig, BloomConfig, CLIPConfig, Dinov2Config, DistilBertConfig, ElectraConfig, GemmaConfig, GPT2Config, GPT2Config, GPTNeoConfig, GPTJConfig, LlamaConfig, LongT5Config, MarianConfig, MBartConfig, MistralConfig, MT5Config, OPTConfig, PegasusConfig, RegNetConfig, ResNetConfig, RobertaConfig, RobertaPreLayerNormConfig, RoFormerConfig, T5Config, VisionTextDualEncoderConfig, ViTConfig, Wav2Vec2Config, WhisperConfig, XGLMConfig, XLMRobertaConfig."
            },
            "tensorflow_eager": {
                "error": "Unrecognized configuration class <class 'transformers.models.megatron_bert.configuration_megatron_bert.MegatronBertConfig'> for this kind of AutoModel: TFAutoModel.\nModel type should be one of AlbertConfig, BartConfig, BertConfig, BlenderbotConfig, BlenderbotSmallConfig, BlipConfig, CamembertConfig, CLIPConfig, ConvBertConfig, ConvNextConfig, ConvNextV2Config, CTRLConfig, CvtConfig, Data2VecVisionConfig, DebertaConfig, DebertaV2Config, DeiTConfig, DistilBertConfig, DPRConfig, EfficientFormerConfig, ElectraConfig, EsmConfig, FlaubertConfig, FunnelConfig, GPT2Config, GPT2Config, GPTJConfig, GroupViTConfig, HubertConfig, IdeficsConfig, LayoutLMConfig, LayoutLMv3Config, LEDConfig, LongformerConfig, LxmertConfig, MarianConfig, MBartConfig, MistralConfig, MobileBertConfig, MobileViTConfig, MPNetConfig, MT5Config, OpenAIGPTConfig, OPTConfig, PegasusConfig, RegNetConfig, RemBertConfig, ResNetConfig, RobertaConfig, RobertaPreLayerNormConfig, RoFormerConfig, SamConfig, SegformerConfig, Speech2TextConfig, SwiftFormerConfig, SwinConfig, T5Config, TapasConfig, TransfoXLConfig, VisionTextDualEncoderConfig, ViTConfig, ViTMAEConfig, Wav2Vec2Config, WhisperConfig, XGLMConfig, XLMConfig, XLMRobertaConfig, XLNetConfig."
            },
            "tensorflow_xla": {
                "error": "Unrecognized configuration class <class 'transformers.models.megatron_bert.configuration_megatron_bert.MegatronBertConfig'> for this kind of AutoModel: TFAutoModel.\nModel type should be one of AlbertConfig, BartConfig, BertConfig, BlenderbotConfig, BlenderbotSmallConfig, BlipConfig, CamembertConfig, CLIPConfig, ConvBertConfig, ConvNextConfig, ConvNextV2Config, CTRLConfig, CvtConfig, Data2VecVisionConfig, DebertaConfig, DebertaV2Config, DeiTConfig, DistilBertConfig, DPRConfig, EfficientFormerConfig, ElectraConfig, EsmConfig, FlaubertConfig, FunnelConfig, GPT2Config, GPT2Config, GPTJConfig, GroupViTConfig, HubertConfig, IdeficsConfig, LayoutLMConfig, LayoutLMv3Config, LEDConfig, LongformerConfig, LxmertConfig, MarianConfig, MBartConfig, MistralConfig, MobileBertConfig, MobileViTConfig, MPNetConfig, MT5Config, OpenAIGPTConfig, OPTConfig, PegasusConfig, RegNetConfig, RemBertConfig, ResNetConfig, RobertaConfig, RobertaPreLayerNormConfig, RoFormerConfig, SamConfig, SegformerConfig, Speech2TextConfig, SwiftFormerConfig, SwinConfig, T5Config, TapasConfig, TransfoXLConfig, VisionTextDualEncoderConfig, ViTConfig, ViTMAEConfig, Wav2Vec2Config, WhisperConfig, XGLMConfig, XLMConfig, XLMRobertaConfig, XLNetConfig."
            }
        }
    },
    {
        "MobileBertForMaskedLM": {
            "pytorch_torch_compile_inductor": {
                "ttft_s": 48.49378442764282
            },
            "pytorch_torch_compile_eager": {
                "ttft_s": 12.959543466567993
            },
            "pytorch_torch_compile_cudagraphs": {
                "ttft_s": 30.68888545036316
            },
            "pytorch_torch_compile_onnxrt": {
                "error": "backend='onnxrt' raised:\nAssertionError: Please convert all Tensors to FakeTensors first or instantiate FakeTensorMode with 'allow_non_fake_inputs'. Found in aten.alias.default(tensor(1000, size=()))\n\nWhile executing %tensor : [num_users=0] = call_function[target=torch.tensor](args = (1000,), kwargs = {})\nOriginal traceback:\n  File \"/opt/conda/envs/xla-env/lib/python3.10/site-packages/transformers/models/mobilebert/modeling_mobilebert.py\", line 1091, in forward\n    outputs = self.mobilebert(\n  File \"/opt/conda/envs/xla-env/lib/python3.10/site-packages/transformers/models/mobilebert/modeling_mobilebert.py\", line 898, in forward\n    encoder_outputs = self.encoder(\n  File \"/opt/conda/envs/xla-env/lib/python3.10/site-packages/transformers/models/mobilebert/modeling_mobilebert.py\", line 580, in forward\n    layer_outputs = layer_module(\n  File \"/opt/conda/envs/xla-env/lib/python3.10/site-packages/transformers/models/mobilebert/modeling_mobilebert.py\", line 547, in forward\n    torch.tensor(1000),\n\n\nSet TORCH_LOGS=\"+dynamo\" and TORCHDYNAMO_VERBOSE=1 for more information\n\n\nYou can suppress this exception and fall back to eager by setting:\n    import torch._dynamo\n    torch._dynamo.config.suppress_errors = True\n"
            },
            "pytorch_torch_compile_openxla": {
                "error": "torch_xla/csrc/runtime/pjrt_registry.cc:214 : Check failed: client \n*** Begin stack trace ***\n\ttsl::CurrentStackTrace()\n\ttorch_xla::runtime::InitializePjRt(std::string const&)\n\ttorch_xla::runtime::PjRtComputationClient::PjRtComputationClient()\n\t\n\ttorch_xla::runtime::GetComputationClient()\n\ttorch_xla::bridge::GetDefaultDevice()\n\ttorch_xla::bridge::GetCurrentDevice()\n\ttorch_xla::bridge::GetCurrentAtenDevice()\n\t\n\t\n\t\n\t_PyObject_MakeTpCall\n\t_PyEval_EvalFrameDefault\n\t\n\t_PyFunction_Vectorcall\n\t_PyEval_EvalFrameDefault\n\t\n\t_PyFunction_Vectorcall\n\t_PyEval_EvalFrameDefault\n\t\n\t_PyFunction_Vectorcall\n\t_PyEval_EvalFrameDefault\n\t\n\t_PyFunction_Vectorcall\n\t_PyEval_EvalFrameDefault\n\t\n\t_PyFunction_Vectorcall\n\t_PyEval_EvalFrameDefault\n\t\n\t_PyFunction_Vectorcall\n\t_PyEval_EvalFrameDefault\n\t\n\t_PyFunction_Vectorcall\n\t_PyEval_EvalFrameDefault\n\t\n\t_PyFunction_Vectorcall\n\t_PyEval_EvalFrameDefault\n\t\n\t_PyFunction_Vectorcall\n\tTHPFunction_apply(_object*, _object*)\n\t\n\tPyObject_Call\n\t_PyEval_EvalFrameDefault\n\t\n\t\n\t_PyEval_EvalFrameDefault\n\t\n\t_PyFunction_Vectorcall\n\t_PyEval_EvalFrameDefault\n\t\n\t_PyFunction_Vectorcall\n\t_PyEval_EvalFrameDefault\n\t\n\t_PyFunction_Vectorcall\n\t_PyEval_EvalFrameDefault\n\t\n\t_PyFunction_Vectorcall\n\t_PyEval_EvalFrameDefault\n\t\n\t_PyFunction_Vectorcall\n\t_PyEval_EvalFrameDefault\n\t\n\t\n\t\n\tPyObject_Call\n\t_PyEval_EvalFrameDefault\n\t\n\t\n\tPyObject_Call\n\t_PyEval_EvalFrameDefault\n\t\n\t\n\tPyObject_Call\n\t_PyEval_EvalFrameDefault\n\t\n\t_PyFunction_Vectorcall\n\tPyObject_Call\n\t_PyEval_EvalFrameDefault\n\t\n\t\n\tPyObject_Call\n\t_PyEval_EvalFrameDefault\n\t\n\t_PyFunction_Vectorcall\n\t_PyObject_FastCallDictTstate\n\t_PyObject_Call_Prepend\n\t\n\tPyObject_Call\n\t_PyEval_EvalFrameDefault\n\t\n\t_PyFunction_Vectorcall\n\t_PyEval_EvalFrameDefault\n\t_PyFunction_Vectorcall\n\t_PyEval_EvalFrameDefault\n\t\n\tPyEval_EvalCode\n\t\n\t\n\t\n\t_PyRun_SimpleFileObject\n\t_PyRun_AnyFileObject\n\tPy_RunMain\n\tPy_BytesMain\n\t__libc_start_main\n\t\n*** End stack trace ***\nUnknown PJRT_DEVICE 'CUDA'"
            },
            "pytorch_torch_compile_tvm": {
                "error": "backend='tvm' raised:\nModuleNotFoundError: No module named 'tvm'\n\nSet TORCH_LOGS=\"+dynamo\" and TORCHDYNAMO_VERBOSE=1 for more information\n\n\nYou can suppress this exception and fall back to eager by setting:\n    import torch._dynamo\n    torch._dynamo.config.suppress_errors = True\n"
            },
            "jax_flax_xla": {
                "error": "Unrecognized configuration class <class 'transformers.models.mobilebert.configuration_mobilebert.MobileBertConfig'> for this kind of AutoModel: FlaxAutoModel.\nModel type should be one of AlbertConfig, BartConfig, BeitConfig, BertConfig, BigBirdConfig, BlenderbotConfig, BlenderbotSmallConfig, BloomConfig, CLIPConfig, Dinov2Config, DistilBertConfig, ElectraConfig, GemmaConfig, GPT2Config, GPT2Config, GPTNeoConfig, GPTJConfig, LlamaConfig, LongT5Config, MarianConfig, MBartConfig, MistralConfig, MT5Config, OPTConfig, PegasusConfig, RegNetConfig, ResNetConfig, RobertaConfig, RobertaPreLayerNormConfig, RoFormerConfig, T5Config, VisionTextDualEncoderConfig, ViTConfig, Wav2Vec2Config, WhisperConfig, XGLMConfig, XLMRobertaConfig."
            },
            "tensorflow_eager": {
                "ttft_s": 0.9559192657470703
            },
            "tensorflow_xla": {
                "ttft_s": 17.339622974395752
            }
        }
    },
    {
        "MobileBertForQuestionAnswering": {
            "pytorch_torch_compile_inductor": {
                "ttft_s": 42.00508236885071
            },
            "pytorch_torch_compile_eager": {
                "ttft_s": 12.82788372039795
            },
            "pytorch_torch_compile_cudagraphs": {
                "ttft_s": 38.12371277809143
            },
            "pytorch_torch_compile_onnxrt": {
                "error": "backend='onnxrt' raised:\nAssertionError: Please convert all Tensors to FakeTensors first or instantiate FakeTensorMode with 'allow_non_fake_inputs'. Found in aten.alias.default(tensor(1000, size=()))\n\nWhile executing %tensor : [num_users=0] = call_function[target=torch.tensor](args = (1000,), kwargs = {})\nOriginal traceback:\n  File \"/opt/conda/envs/xla-env/lib/python3.10/site-packages/transformers/models/mobilebert/modeling_mobilebert.py\", line 1392, in forward\n    outputs = self.mobilebert(\n  File \"/opt/conda/envs/xla-env/lib/python3.10/site-packages/transformers/models/mobilebert/modeling_mobilebert.py\", line 898, in forward\n    encoder_outputs = self.encoder(\n  File \"/opt/conda/envs/xla-env/lib/python3.10/site-packages/transformers/models/mobilebert/modeling_mobilebert.py\", line 580, in forward\n    layer_outputs = layer_module(\n  File \"/opt/conda/envs/xla-env/lib/python3.10/site-packages/transformers/models/mobilebert/modeling_mobilebert.py\", line 547, in forward\n    torch.tensor(1000),\n\n\nSet TORCH_LOGS=\"+dynamo\" and TORCHDYNAMO_VERBOSE=1 for more information\n\n\nYou can suppress this exception and fall back to eager by setting:\n    import torch._dynamo\n    torch._dynamo.config.suppress_errors = True\n"
            },
            "pytorch_torch_compile_openxla": {
                "error": "torch_xla/csrc/runtime/pjrt_registry.cc:214 : Check failed: client \n*** Begin stack trace ***\n\ttsl::CurrentStackTrace()\n\ttorch_xla::runtime::InitializePjRt(std::string const&)\n\ttorch_xla::runtime::PjRtComputationClient::PjRtComputationClient()\n\t\n\ttorch_xla::runtime::GetComputationClient()\n\ttorch_xla::bridge::GetDefaultDevice()\n\ttorch_xla::bridge::GetCurrentDevice()\n\ttorch_xla::bridge::GetCurrentAtenDevice()\n\t\n\t\n\t\n\t_PyObject_MakeTpCall\n\t_PyEval_EvalFrameDefault\n\t\n\t_PyFunction_Vectorcall\n\t_PyEval_EvalFrameDefault\n\t\n\t_PyFunction_Vectorcall\n\t_PyEval_EvalFrameDefault\n\t\n\t_PyFunction_Vectorcall\n\t_PyEval_EvalFrameDefault\n\t\n\t_PyFunction_Vectorcall\n\t_PyEval_EvalFrameDefault\n\t\n\t_PyFunction_Vectorcall\n\t_PyEval_EvalFrameDefault\n\t\n\t_PyFunction_Vectorcall\n\t_PyEval_EvalFrameDefault\n\t\n\t_PyFunction_Vectorcall\n\t_PyEval_EvalFrameDefault\n\t\n\t_PyFunction_Vectorcall\n\t_PyEval_EvalFrameDefault\n\t\n\t_PyFunction_Vectorcall\n\tTHPFunction_apply(_object*, _object*)\n\t\n\tPyObject_Call\n\t_PyEval_EvalFrameDefault\n\t\n\t\n\t_PyEval_EvalFrameDefault\n\t\n\t_PyFunction_Vectorcall\n\t_PyEval_EvalFrameDefault\n\t\n\t_PyFunction_Vectorcall\n\t_PyEval_EvalFrameDefault\n\t\n\t_PyFunction_Vectorcall\n\t_PyEval_EvalFrameDefault\n\t\n\t_PyFunction_Vectorcall\n\t_PyEval_EvalFrameDefault\n\t\n\t_PyFunction_Vectorcall\n\t_PyEval_EvalFrameDefault\n\t\n\t\n\t\n\tPyObject_Call\n\t_PyEval_EvalFrameDefault\n\t\n\t\n\tPyObject_Call\n\t_PyEval_EvalFrameDefault\n\t\n\t\n\tPyObject_Call\n\t_PyEval_EvalFrameDefault\n\t\n\t_PyFunction_Vectorcall\n\tPyObject_Call\n\t_PyEval_EvalFrameDefault\n\t\n\t\n\tPyObject_Call\n\t_PyEval_EvalFrameDefault\n\t\n\t_PyFunction_Vectorcall\n\t_PyObject_FastCallDictTstate\n\t_PyObject_Call_Prepend\n\t\n\tPyObject_Call\n\t_PyEval_EvalFrameDefault\n\t\n\t_PyFunction_Vectorcall\n\t_PyEval_EvalFrameDefault\n\t_PyFunction_Vectorcall\n\t_PyEval_EvalFrameDefault\n\t\n\tPyEval_EvalCode\n\t\n\t\n\t\n\t_PyRun_SimpleFileObject\n\t_PyRun_AnyFileObject\n\tPy_RunMain\n\tPy_BytesMain\n\t__libc_start_main\n\t\n*** End stack trace ***\nUnknown PJRT_DEVICE 'CUDA'"
            },
            "pytorch_torch_compile_tvm": {
                "error": "backend='tvm' raised:\nModuleNotFoundError: No module named 'tvm'\n\nSet TORCH_LOGS=\"+dynamo\" and TORCHDYNAMO_VERBOSE=1 for more information\n\n\nYou can suppress this exception and fall back to eager by setting:\n    import torch._dynamo\n    torch._dynamo.config.suppress_errors = True\n"
            },
            "jax_flax_xla": {
                "error": "Unrecognized configuration class <class 'transformers.models.mobilebert.configuration_mobilebert.MobileBertConfig'> for this kind of AutoModel: FlaxAutoModel.\nModel type should be one of AlbertConfig, BartConfig, BeitConfig, BertConfig, BigBirdConfig, BlenderbotConfig, BlenderbotSmallConfig, BloomConfig, CLIPConfig, Dinov2Config, DistilBertConfig, ElectraConfig, GemmaConfig, GPT2Config, GPT2Config, GPTNeoConfig, GPTJConfig, LlamaConfig, LongT5Config, MarianConfig, MBartConfig, MistralConfig, MT5Config, OPTConfig, PegasusConfig, RegNetConfig, ResNetConfig, RobertaConfig, RobertaPreLayerNormConfig, RoFormerConfig, T5Config, VisionTextDualEncoderConfig, ViTConfig, Wav2Vec2Config, WhisperConfig, XGLMConfig, XLMRobertaConfig."
            },
            "tensorflow_eager": {
                "ttft_s": 0.8323957920074463
            },
            "tensorflow_xla": {
                "ttft_s": 9.445225715637207
            }
        }
    },
    {
        "MT5ForConditionalGeneration": {
            "pytorch_torch_compile_inductor": {
                "ttft_s": 40.094958782196045
            },
            "pytorch_torch_compile_eager": {
                "ttft_s": 9.516787767410278
            },
            "pytorch_torch_compile_cudagraphs": {
                "ttft_s": 25.803033590316772
            },
            "pytorch_torch_compile_onnxrt": {
                "ttft_s": 43.554811000823975
            },
            "pytorch_torch_compile_openxla": {
                "error": "torch_xla/csrc/runtime/pjrt_registry.cc:214 : Check failed: client \n*** Begin stack trace ***\n\ttsl::CurrentStackTrace()\n\ttorch_xla::runtime::InitializePjRt(std::string const&)\n\ttorch_xla::runtime::PjRtComputationClient::PjRtComputationClient()\n\t\n\ttorch_xla::runtime::GetComputationClient()\n\ttorch_xla::bridge::GetDefaultDevice()\n\ttorch_xla::bridge::GetCurrentDevice()\n\ttorch_xla::bridge::GetCurrentAtenDevice()\n\t\n\t\n\t\n\t_PyObject_MakeTpCall\n\t_PyEval_EvalFrameDefault\n\t\n\t_PyFunction_Vectorcall\n\t_PyEval_EvalFrameDefault\n\t\n\t_PyFunction_Vectorcall\n\t_PyEval_EvalFrameDefault\n\t\n\t_PyFunction_Vectorcall\n\t_PyEval_EvalFrameDefault\n\t\n\t_PyFunction_Vectorcall\n\t_PyEval_EvalFrameDefault\n\t\n\t_PyFunction_Vectorcall\n\t_PyEval_EvalFrameDefault\n\t\n\t_PyFunction_Vectorcall\n\t_PyEval_EvalFrameDefault\n\t\n\t_PyFunction_Vectorcall\n\t_PyEval_EvalFrameDefault\n\t\n\t_PyFunction_Vectorcall\n\t_PyEval_EvalFrameDefault\n\t\n\t_PyFunction_Vectorcall\n\tTHPFunction_apply(_object*, _object*)\n\t\n\tPyObject_Call\n\t_PyEval_EvalFrameDefault\n\t\n\t\n\t_PyEval_EvalFrameDefault\n\t\n\t_PyFunction_Vectorcall\n\t_PyEval_EvalFrameDefault\n\t\n\t_PyFunction_Vectorcall\n\t_PyEval_EvalFrameDefault\n\t\n\t_PyFunction_Vectorcall\n\t_PyEval_EvalFrameDefault\n\t\n\t_PyFunction_Vectorcall\n\t_PyEval_EvalFrameDefault\n\t\n\t_PyFunction_Vectorcall\n\t_PyEval_EvalFrameDefault\n\t\n\t\n\t\n\tPyObject_Call\n\t_PyEval_EvalFrameDefault\n\t\n\t\n\tPyObject_Call\n\t_PyEval_EvalFrameDefault\n\t\n\t\n\tPyObject_Call\n\t_PyEval_EvalFrameDefault\n\t\n\t_PyFunction_Vectorcall\n\tPyObject_Call\n\t_PyEval_EvalFrameDefault\n\t\n\t\n\tPyObject_Call\n\t_PyEval_EvalFrameDefault\n\t\n\t_PyFunction_Vectorcall\n\t_PyObject_FastCallDictTstate\n\t_PyObject_Call_Prepend\n\t\n\tPyObject_Call\n\t_PyEval_EvalFrameDefault\n\t\n\t_PyFunction_Vectorcall\n\t_PyEval_EvalFrameDefault\n\t_PyFunction_Vectorcall\n\t_PyEval_EvalFrameDefault\n\t\n\tPyEval_EvalCode\n\t\n\t\n\t\n\t_PyRun_SimpleFileObject\n\t_PyRun_AnyFileObject\n\tPy_RunMain\n\tPy_BytesMain\n\t__libc_start_main\n\t\n*** End stack trace ***\nUnknown PJRT_DEVICE 'CUDA'"
            },
            "pytorch_torch_compile_tvm": {
                "error": "backend='tvm' raised:\nModuleNotFoundError: No module named 'tvm'\n\nSet TORCH_LOGS=\"+dynamo\" and TORCHDYNAMO_VERBOSE=1 for more information\n\n\nYou can suppress this exception and fall back to eager by setting:\n    import torch._dynamo\n    torch._dynamo.config.suppress_errors = True\n"
            },
            "jax_flax_xla": {
                "ttft_s": 16.93352174758911
            },
            "tensorflow_eager": {
                "error": "Exception encountered when calling layer 'decoder' (type TFT5MainLayer).\n\nYou have to specify either decoder_input_ids or decoder_inputs_embeds\n\nCall arguments received by layer 'decoder' (type TFT5MainLayer):\n  \u2022 input_ids=None\n  \u2022 attention_mask=None\n  \u2022 encoder_hidden_states=tf.Tensor(shape=(1, 32, 768), dtype=float32)\n  \u2022 encoder_attention_mask=tf.Tensor(shape=(1, 32), dtype=int32)\n  \u2022 inputs_embeds=None\n  \u2022 head_mask=None\n  \u2022 encoder_head_mask=None\n  \u2022 past_key_values=None\n  \u2022 use_cache=True\n  \u2022 output_attentions=False\n  \u2022 output_hidden_states=False\n  \u2022 return_dict=True\n  \u2022 training=False"
            },
            "tensorflow_xla": {
                "error": "in user code:\n\n    File \"/home/mmoffatt/jax_vs_pytorch/benchmark_ttft.py\", line 176, in fwd  *\n        return model(**inp)[0]\n    File \"/opt/conda/envs/xla-env/lib/python3.10/site-packages/tf_keras/src/utils/traceback_utils.py\", line 70, in error_handler  **\n        raise e.with_traceback(filtered_tb) from None\n    File \"/var/tmp/__autograph_generated_file138ue72o.py\", line 37, in tf__run_call_with_unpacked_inputs\n        retval_ = ag__.converted_call(ag__.ld(func), (ag__.ld(self),), dict(**ag__.ld(unpacked_inputs)), fscope)\n    File \"/var/tmp/__autograph_generated_filesn5endok.py\", line 91, in tf__call\n        decoder_outputs = ag__.converted_call(ag__.ld(self).decoder, (ag__.ld(decoder_input_ids),), dict(attention_mask=ag__.ld(decoder_attention_mask), encoder_hidden_states=ag__.ld(hidden_states), encoder_attention_mask=ag__.ld(attention_mask), inputs_embeds=ag__.ld(decoder_inputs_embeds), head_mask=ag__.ld(decoder_head_mask), past_key_values=ag__.ld(past_key_values), use_cache=ag__.ld(use_cache), output_attentions=ag__.ld(output_attentions), output_hidden_states=ag__.ld(output_hidden_states), return_dict=ag__.ld(return_dict), training=ag__.ld(training)), fscope)\n    File \"/var/tmp/__autograph_generated_file138ue72o.py\", line 37, in tf__run_call_with_unpacked_inputs\n        retval_ = ag__.converted_call(ag__.ld(func), (ag__.ld(self),), dict(**ag__.ld(unpacked_inputs)), fscope)\n    File \"/var/tmp/__autograph_generated_filehcwpld1c.py\", line 65, in tf__call\n        ag__.if_stmt(ag__.and_(lambda : ag__.ld(input_ids) is not None, lambda : ag__.ld(inputs_embeds) is not None), if_body_2, else_body_2, get_state_2, set_state_2, ('input_ids', 'input_shape'), 2)\n    File \"/var/tmp/__autograph_generated_filehcwpld1c.py\", line 62, in else_body_2\n        ag__.if_stmt(ag__.ld(input_ids) is not None, if_body_1, else_body_1, get_state_1, set_state_1, ('input_ids', 'input_shape'), 2)\n    File \"/var/tmp/__autograph_generated_filehcwpld1c.py\", line 59, in else_body_1\n        ag__.if_stmt(ag__.ld(inputs_embeds) is not None, if_body, else_body, get_state, set_state, ('input_shape',), 1)\n    File \"/var/tmp/__autograph_generated_filehcwpld1c.py\", line 56, in else_body\n        raise ag__.converted_call(ag__.ld(ValueError), (f'You have to specify either {ag__.ld(err_msg_prefix)}input_ids or {ag__.ld(err_msg_prefix)}inputs_embeds',), None, fscope)\n\n    ValueError: Exception encountered when calling layer 'tfmt5_for_conditional_generation_1' (type TFMT5ForConditionalGeneration).\n    \n    in user code:\n    \n        File \"/opt/conda/envs/xla-env/lib/python3.10/site-packages/transformers/modeling_tf_utils.py\", line 1079, in run_call_with_unpacked_inputs  *\n            return func(self, **unpacked_inputs)\n        File \"/opt/conda/envs/xla-env/lib/python3.10/site-packages/transformers/models/t5/modeling_tf_t5.py\", line 1455, in call  *\n            decoder_outputs = self.decoder(\n        File \"/opt/conda/envs/xla-env/lib/python3.10/site-packages/tf_keras/src/utils/traceback_utils.py\", line 70, in error_handler  **\n            raise e.with_traceback(filtered_tb) from None\n        File \"/var/tmp/__autograph_generated_file138ue72o.py\", line 37, in tf__run_call_with_unpacked_inputs\n            retval_ = ag__.converted_call(ag__.ld(func), (ag__.ld(self),), dict(**ag__.ld(unpacked_inputs)), fscope)\n        File \"/var/tmp/__autograph_generated_filehcwpld1c.py\", line 65, in tf__call\n            ag__.if_stmt(ag__.and_(lambda : ag__.ld(input_ids) is not None, lambda : ag__.ld(inputs_embeds) is not None), if_body_2, else_body_2, get_state_2, set_state_2, ('input_ids', 'input_shape'), 2)\n        File \"/var/tmp/__autograph_generated_filehcwpld1c.py\", line 62, in else_body_2\n            ag__.if_stmt(ag__.ld(input_ids) is not None, if_body_1, else_body_1, get_state_1, set_state_1, ('input_ids', 'input_shape'), 2)\n        File \"/var/tmp/__autograph_generated_filehcwpld1c.py\", line 59, in else_body_1\n            ag__.if_stmt(ag__.ld(inputs_embeds) is not None, if_body, else_body, get_state, set_state, ('input_shape',), 1)\n        File \"/var/tmp/__autograph_generated_filehcwpld1c.py\", line 56, in else_body\n            raise ag__.converted_call(ag__.ld(ValueError), (f'You have to specify either {ag__.ld(err_msg_prefix)}input_ids or {ag__.ld(err_msg_prefix)}inputs_embeds',), None, fscope)\n    \n        ValueError: Exception encountered when calling layer 'decoder' (type TFT5MainLayer).\n        \n        in user code:\n        \n            File \"/opt/conda/envs/xla-env/lib/python3.10/site-packages/transformers/modeling_tf_utils.py\", line 1079, in run_call_with_unpacked_inputs  *\n                return func(self, **unpacked_inputs)\n            File \"/opt/conda/envs/xla-env/lib/python3.10/site-packages/transformers/models/t5/modeling_tf_t5.py\", line 754, in call  *\n                raise ValueError(f\"You have to specify either {err_msg_prefix}input_ids or {err_msg_prefix}inputs_embeds\")\n        \n            ValueError: You have to specify either decoder_input_ids or decoder_inputs_embeds\n        \n        \n        Call arguments received by layer 'decoder' (type TFT5MainLayer):\n          \u2022 input_ids=None\n          \u2022 attention_mask=None\n          \u2022 encoder_hidden_states=tf.Tensor(shape=(1, 32, 768), dtype=float32)\n          \u2022 encoder_attention_mask=tf.Tensor(shape=(1, 32), dtype=int32)\n          \u2022 inputs_embeds=None\n          \u2022 head_mask=None\n          \u2022 encoder_head_mask=None\n          \u2022 past_key_values=None\n          \u2022 use_cache=True\n          \u2022 output_attentions=False\n          \u2022 output_hidden_states=False\n          \u2022 return_dict=True\n          \u2022 training=False\n    \n    \n    Call arguments received by layer 'tfmt5_for_conditional_generation_1' (type TFMT5ForConditionalGeneration):\n      \u2022 input_ids=tf.Tensor(shape=(1, 32), dtype=int32)\n      \u2022 attention_mask=tf.Tensor(shape=(1, 32), dtype=int32)\n      \u2022 decoder_input_ids=None\n      \u2022 decoder_attention_mask=None\n      \u2022 head_mask=None\n      \u2022 decoder_head_mask=None\n      \u2022 encoder_outputs=None\n      \u2022 past_key_values=None\n      \u2022 inputs_embeds=None\n      \u2022 decoder_inputs_embeds=None\n      \u2022 labels=None\n      \u2022 use_cache=None\n      \u2022 output_attentions=None\n      \u2022 output_hidden_states=None\n      \u2022 return_dict=None\n      \u2022 training=False\n"
            },
            "jax_flax_eager": {
                "ttft_s": 12.009096145629883
            }
        }
    },
    {
        "OPTForCausalLM": {
            "pytorch_torch_compile_inductor": {
                "ttft_s": 19.90337300300598
            },
            "pytorch_torch_compile_eager": {
                "ttft_s": 1.7403676509857178
            },
            "pytorch_torch_compile_cudagraphs": {
                "ttft_s": 5.945383071899414
            },
            "pytorch_torch_compile_onnxrt": {
                "ttft_s": 10.484352350234985
            },
            "pytorch_torch_compile_openxla": {
                "error": "torch_xla/csrc/runtime/pjrt_registry.cc:214 : Check failed: client \n*** Begin stack trace ***\n\ttsl::CurrentStackTrace()\n\ttorch_xla::runtime::InitializePjRt(std::string const&)\n\ttorch_xla::runtime::PjRtComputationClient::PjRtComputationClient()\n\t\n\ttorch_xla::runtime::GetComputationClient()\n\ttorch_xla::bridge::GetDefaultDevice()\n\ttorch_xla::bridge::GetCurrentDevice()\n\ttorch_xla::bridge::GetCurrentAtenDevice()\n\t\n\t\n\t\n\t_PyObject_MakeTpCall\n\t_PyEval_EvalFrameDefault\n\t\n\t_PyFunction_Vectorcall\n\t_PyEval_EvalFrameDefault\n\t\n\t_PyFunction_Vectorcall\n\t_PyEval_EvalFrameDefault\n\t\n\t_PyFunction_Vectorcall\n\t_PyEval_EvalFrameDefault\n\t\n\t_PyFunction_Vectorcall\n\t_PyEval_EvalFrameDefault\n\t\n\t_PyFunction_Vectorcall\n\t_PyEval_EvalFrameDefault\n\t\n\t_PyFunction_Vectorcall\n\t_PyEval_EvalFrameDefault\n\t\n\t_PyFunction_Vectorcall\n\t_PyEval_EvalFrameDefault\n\t\n\t_PyFunction_Vectorcall\n\t_PyEval_EvalFrameDefault\n\t\n\t_PyFunction_Vectorcall\n\tTHPFunction_apply(_object*, _object*)\n\t\n\tPyObject_Call\n\t_PyEval_EvalFrameDefault\n\t\n\t\n\t_PyEval_EvalFrameDefault\n\t\n\t_PyFunction_Vectorcall\n\t_PyEval_EvalFrameDefault\n\t\n\t_PyFunction_Vectorcall\n\t_PyEval_EvalFrameDefault\n\t\n\t_PyFunction_Vectorcall\n\t_PyEval_EvalFrameDefault\n\t\n\t_PyFunction_Vectorcall\n\t_PyEval_EvalFrameDefault\n\t\n\t_PyFunction_Vectorcall\n\t_PyEval_EvalFrameDefault\n\t\n\t\n\t\n\tPyObject_Call\n\t_PyEval_EvalFrameDefault\n\t\n\t\n\tPyObject_Call\n\t_PyEval_EvalFrameDefault\n\t\n\t\n\tPyObject_Call\n\t_PyEval_EvalFrameDefault\n\t\n\t_PyFunction_Vectorcall\n\tPyObject_Call\n\t_PyEval_EvalFrameDefault\n\t\n\t\n\tPyObject_Call\n\t_PyEval_EvalFrameDefault\n\t\n\t_PyFunction_Vectorcall\n\t_PyObject_FastCallDictTstate\n\t_PyObject_Call_Prepend\n\t\n\tPyObject_Call\n\t_PyEval_EvalFrameDefault\n\t\n\t_PyFunction_Vectorcall\n\t_PyEval_EvalFrameDefault\n\t_PyFunction_Vectorcall\n\t_PyEval_EvalFrameDefault\n\t\n\tPyEval_EvalCode\n\t\n\t\n\t\n\t_PyRun_SimpleFileObject\n\t_PyRun_AnyFileObject\n\tPy_RunMain\n\tPy_BytesMain\n\t__libc_start_main\n\t\n*** End stack trace ***\nUnknown PJRT_DEVICE 'CUDA'"
            },
            "pytorch_torch_compile_tvm": {
                "error": "backend='tvm' raised:\nModuleNotFoundError: No module named 'tvm'\n\nSet TORCH_LOGS=\"+dynamo\" and TORCHDYNAMO_VERBOSE=1 for more information\n\n\nYou can suppress this exception and fall back to eager by setting:\n    import torch._dynamo\n    torch._dynamo.config.suppress_errors = True\n"
            },
            "jax_flax_xla": {
                "error": "FlaxOPTPreTrainedModel.__call__() got an unexpected keyword argument 'train'"
            },
            "tensorflow_eager": {
                "ttft_s": 0.24822568893432617
            },
            "tensorflow_xla": {
                "ttft_s": 8.140085697174072
            }
        }
    },
    {
        "PegasusForCausalLM": {
            "pytorch_torch_compile_inductor": {
                "ttft_s": 12.269262790679932
            },
            "pytorch_torch_compile_eager": {
                "ttft_s": 2.4798479080200195
            },
            "pytorch_torch_compile_cudagraphs": {
                "ttft_s": 8.709046602249146
            },
            "pytorch_torch_compile_onnxrt": {
                "ttft_s": 25.733108282089233
            },
            "pytorch_torch_compile_openxla": {
                "error": "torch_xla/csrc/runtime/pjrt_registry.cc:214 : Check failed: client \n*** Begin stack trace ***\n\ttsl::CurrentStackTrace()\n\ttorch_xla::runtime::InitializePjRt(std::string const&)\n\ttorch_xla::runtime::PjRtComputationClient::PjRtComputationClient()\n\t\n\ttorch_xla::runtime::GetComputationClient()\n\ttorch_xla::bridge::GetDefaultDevice()\n\ttorch_xla::bridge::GetCurrentDevice()\n\ttorch_xla::bridge::GetCurrentAtenDevice()\n\t\n\t\n\t\n\t_PyObject_MakeTpCall\n\t_PyEval_EvalFrameDefault\n\t\n\t_PyFunction_Vectorcall\n\t_PyEval_EvalFrameDefault\n\t\n\t_PyFunction_Vectorcall\n\t_PyEval_EvalFrameDefault\n\t\n\t_PyFunction_Vectorcall\n\t_PyEval_EvalFrameDefault\n\t\n\t_PyFunction_Vectorcall\n\t_PyEval_EvalFrameDefault\n\t\n\t_PyFunction_Vectorcall\n\t_PyEval_EvalFrameDefault\n\t\n\t_PyFunction_Vectorcall\n\t_PyEval_EvalFrameDefault\n\t\n\t_PyFunction_Vectorcall\n\t_PyEval_EvalFrameDefault\n\t\n\t_PyFunction_Vectorcall\n\t_PyEval_EvalFrameDefault\n\t\n\t_PyFunction_Vectorcall\n\tTHPFunction_apply(_object*, _object*)\n\t\n\tPyObject_Call\n\t_PyEval_EvalFrameDefault\n\t\n\t\n\t_PyEval_EvalFrameDefault\n\t\n\t_PyFunction_Vectorcall\n\t_PyEval_EvalFrameDefault\n\t\n\t_PyFunction_Vectorcall\n\t_PyEval_EvalFrameDefault\n\t\n\t_PyFunction_Vectorcall\n\t_PyEval_EvalFrameDefault\n\t\n\t_PyFunction_Vectorcall\n\t_PyEval_EvalFrameDefault\n\t\n\t_PyFunction_Vectorcall\n\t_PyEval_EvalFrameDefault\n\t\n\t\n\t\n\tPyObject_Call\n\t_PyEval_EvalFrameDefault\n\t\n\t\n\tPyObject_Call\n\t_PyEval_EvalFrameDefault\n\t\n\t\n\tPyObject_Call\n\t_PyEval_EvalFrameDefault\n\t\n\t_PyFunction_Vectorcall\n\tPyObject_Call\n\t_PyEval_EvalFrameDefault\n\t\n\t\n\tPyObject_Call\n\t_PyEval_EvalFrameDefault\n\t\n\t_PyFunction_Vectorcall\n\t_PyObject_FastCallDictTstate\n\t_PyObject_Call_Prepend\n\t\n\tPyObject_Call\n\t_PyEval_EvalFrameDefault\n\t\n\t_PyFunction_Vectorcall\n\t_PyEval_EvalFrameDefault\n\t_PyFunction_Vectorcall\n\t_PyEval_EvalFrameDefault\n\t\n\tPyEval_EvalCode\n\t\n\t\n\t\n\t_PyRun_SimpleFileObject\n\t_PyRun_AnyFileObject\n\tPy_RunMain\n\tPy_BytesMain\n\t__libc_start_main\n\t\n*** End stack trace ***\nUnknown PJRT_DEVICE 'CUDA'"
            },
            "pytorch_torch_compile_tvm": {
                "error": "backend='tvm' raised:\nModuleNotFoundError: No module named 'tvm'\n\nSet TORCH_LOGS=\"+dynamo\" and TORCHDYNAMO_VERBOSE=1 for more information\n\n\nYou can suppress this exception and fall back to eager by setting:\n    import torch._dynamo\n    torch._dynamo.config.suppress_errors = True\n"
            },
            "jax_flax_xla": {
                "ttft_s": 19.80593228340149
            },
            "tensorflow_eager": {
                "error": "Exception encountered when calling layer 'decoder' (type TFPegasusDecoder).\n\nYou have to specify either decoder_input_ids or decoder_inputs_embeds\n\nCall arguments received by layer 'decoder' (type TFPegasusDecoder):\n  \u2022 input_ids=None\n  \u2022 inputs_embeds=None\n  \u2022 attention_mask=None\n  \u2022 position_ids=None\n  \u2022 encoder_hidden_states=tf.Tensor(shape=(1, 32, 1024), dtype=float32)\n  \u2022 encoder_attention_mask=tf.Tensor(shape=(1, 32), dtype=int32)\n  \u2022 head_mask=None\n  \u2022 cross_attn_head_mask=None\n  \u2022 past_key_values=None\n  \u2022 use_cache=False\n  \u2022 output_attentions=False\n  \u2022 output_hidden_states=False\n  \u2022 return_dict=True\n  \u2022 training=False"
            },
            "tensorflow_xla": {
                "error": "in user code:\n\n    File \"/home/mmoffatt/jax_vs_pytorch/benchmark_ttft.py\", line 176, in fwd  *\n        return model(**inp)[0]\n    File \"/opt/conda/envs/xla-env/lib/python3.10/site-packages/tf_keras/src/utils/traceback_utils.py\", line 70, in error_handler  **\n        raise e.with_traceback(filtered_tb) from None\n    File \"/var/tmp/__autograph_generated_file138ue72o.py\", line 37, in tf__run_call_with_unpacked_inputs\n        retval_ = ag__.converted_call(ag__.ld(func), (ag__.ld(self),), dict(**ag__.ld(unpacked_inputs)), fscope)\n    File \"/var/tmp/__autograph_generated_filekyi6sar6.py\", line 11, in tf__call\n        outputs = ag__.converted_call(ag__.ld(self).model, (), dict(input_ids=ag__.ld(input_ids), attention_mask=ag__.ld(attention_mask), decoder_input_ids=ag__.ld(decoder_input_ids), decoder_attention_mask=ag__.ld(decoder_attention_mask), decoder_position_ids=ag__.ld(decoder_position_ids), head_mask=ag__.ld(head_mask), decoder_head_mask=ag__.ld(decoder_head_mask), cross_attn_head_mask=ag__.ld(cross_attn_head_mask), encoder_outputs=ag__.ld(encoder_outputs), past_key_values=ag__.ld(past_key_values), inputs_embeds=ag__.ld(inputs_embeds), decoder_inputs_embeds=ag__.ld(decoder_inputs_embeds), use_cache=ag__.ld(use_cache), output_attentions=ag__.ld(output_attentions), output_hidden_states=ag__.ld(output_hidden_states), return_dict=ag__.ld(return_dict), training=ag__.ld(training)), fscope)\n    File \"/var/tmp/__autograph_generated_file138ue72o.py\", line 37, in tf__run_call_with_unpacked_inputs\n        retval_ = ag__.converted_call(ag__.ld(func), (ag__.ld(self),), dict(**ag__.ld(unpacked_inputs)), fscope)\n    File \"/var/tmp/__autograph_generated_file678hxr2p.py\", line 74, in tf__call\n        decoder_outputs = ag__.converted_call(ag__.ld(self).decoder, (ag__.ld(decoder_input_ids),), dict(attention_mask=ag__.ld(decoder_attention_mask), position_ids=ag__.ld(decoder_position_ids), encoder_hidden_states=ag__.ld(encoder_outputs)[0], encoder_attention_mask=ag__.ld(attention_mask), head_mask=ag__.ld(decoder_head_mask), cross_attn_head_mask=ag__.ld(cross_attn_head_mask), past_key_values=ag__.ld(past_key_values), inputs_embeds=ag__.ld(decoder_inputs_embeds), use_cache=ag__.ld(use_cache), output_attentions=ag__.ld(output_attentions), output_hidden_states=ag__.ld(output_hidden_states), return_dict=ag__.ld(return_dict), training=ag__.ld(training)), fscope)\n    File \"/var/tmp/__autograph_generated_file138ue72o.py\", line 37, in tf__run_call_with_unpacked_inputs\n        retval_ = ag__.converted_call(ag__.ld(func), (ag__.ld(self),), dict(**ag__.ld(unpacked_inputs)), fscope)\n    File \"/var/tmp/__autograph_generated_fileo809d_t3.py\", line 128, in tf__call\n        ag__.if_stmt(ag__.and_(lambda : ag__.ld(input_ids) is not None, lambda : ag__.ld(inputs_embeds) is not None), if_body_2, else_body_2, get_state_2, set_state_2, ('input_shape',), 1)\n    File \"/var/tmp/__autograph_generated_fileo809d_t3.py\", line 126, in else_body_2\n        ag__.if_stmt(ag__.ld(input_ids) is not None, if_body_1, else_body_1, get_state_1, set_state_1, ('input_shape',), 1)\n    File \"/var/tmp/__autograph_generated_fileo809d_t3.py\", line 124, in else_body_1\n        ag__.if_stmt(ag__.ld(inputs_embeds) is not None, if_body, else_body, get_state, set_state, ('input_shape',), 1)\n    File \"/var/tmp/__autograph_generated_fileo809d_t3.py\", line 122, in else_body\n        raise ag__.converted_call(ag__.ld(ValueError), ('You have to specify either decoder_input_ids or decoder_inputs_embeds',), None, fscope)\n\n    ValueError: Exception encountered when calling layer 'tf_pegasus_model_1' (type TFPegasusModel).\n    \n    in user code:\n    \n        File \"/opt/conda/envs/xla-env/lib/python3.10/site-packages/transformers/modeling_tf_utils.py\", line 1079, in run_call_with_unpacked_inputs  *\n            return func(self, **unpacked_inputs)\n        File \"/opt/conda/envs/xla-env/lib/python3.10/site-packages/transformers/models/pegasus/modeling_tf_pegasus.py\", line 1304, in call  *\n            outputs = self.model(\n        File \"/opt/conda/envs/xla-env/lib/python3.10/site-packages/tf_keras/src/utils/traceback_utils.py\", line 70, in error_handler  **\n            raise e.with_traceback(filtered_tb) from None\n        File \"/var/tmp/__autograph_generated_file138ue72o.py\", line 37, in tf__run_call_with_unpacked_inputs\n            retval_ = ag__.converted_call(ag__.ld(func), (ag__.ld(self),), dict(**ag__.ld(unpacked_inputs)), fscope)\n        File \"/var/tmp/__autograph_generated_file678hxr2p.py\", line 74, in tf__call\n            decoder_outputs = ag__.converted_call(ag__.ld(self).decoder, (ag__.ld(decoder_input_ids),), dict(attention_mask=ag__.ld(decoder_attention_mask), position_ids=ag__.ld(decoder_position_ids), encoder_hidden_states=ag__.ld(encoder_outputs)[0], encoder_attention_mask=ag__.ld(attention_mask), head_mask=ag__.ld(decoder_head_mask), cross_attn_head_mask=ag__.ld(cross_attn_head_mask), past_key_values=ag__.ld(past_key_values), inputs_embeds=ag__.ld(decoder_inputs_embeds), use_cache=ag__.ld(use_cache), output_attentions=ag__.ld(output_attentions), output_hidden_states=ag__.ld(output_hidden_states), return_dict=ag__.ld(return_dict), training=ag__.ld(training)), fscope)\n        File \"/var/tmp/__autograph_generated_file138ue72o.py\", line 37, in tf__run_call_with_unpacked_inputs\n            retval_ = ag__.converted_call(ag__.ld(func), (ag__.ld(self),), dict(**ag__.ld(unpacked_inputs)), fscope)\n        File \"/var/tmp/__autograph_generated_fileo809d_t3.py\", line 128, in tf__call\n            ag__.if_stmt(ag__.and_(lambda : ag__.ld(input_ids) is not None, lambda : ag__.ld(inputs_embeds) is not None), if_body_2, else_body_2, get_state_2, set_state_2, ('input_shape',), 1)\n        File \"/var/tmp/__autograph_generated_fileo809d_t3.py\", line 126, in else_body_2\n            ag__.if_stmt(ag__.ld(input_ids) is not None, if_body_1, else_body_1, get_state_1, set_state_1, ('input_shape',), 1)\n        File \"/var/tmp/__autograph_generated_fileo809d_t3.py\", line 124, in else_body_1\n            ag__.if_stmt(ag__.ld(inputs_embeds) is not None, if_body, else_body, get_state, set_state, ('input_shape',), 1)\n        File \"/var/tmp/__autograph_generated_fileo809d_t3.py\", line 122, in else_body\n            raise ag__.converted_call(ag__.ld(ValueError), ('You have to specify either decoder_input_ids or decoder_inputs_embeds',), None, fscope)\n    \n        ValueError: Exception encountered when calling layer 'model' (type TFPegasusMainLayer).\n        \n        in user code:\n        \n            File \"/opt/conda/envs/xla-env/lib/python3.10/site-packages/transformers/modeling_tf_utils.py\", line 1079, in run_call_with_unpacked_inputs  *\n                return func(self, **unpacked_inputs)\n            File \"/opt/conda/envs/xla-env/lib/python3.10/site-packages/transformers/models/pegasus/modeling_tf_pegasus.py\", line 1212, in call  *\n                decoder_outputs = self.decoder(\n            File \"/opt/conda/envs/xla-env/lib/python3.10/site-packages/tf_keras/src/utils/traceback_utils.py\", line 70, in error_handler  **\n                raise e.with_traceback(filtered_tb) from None\n            File \"/var/tmp/__autograph_generated_file138ue72o.py\", line 37, in tf__run_call_with_unpacked_inputs\n                retval_ = ag__.converted_call(ag__.ld(func), (ag__.ld(self),), dict(**ag__.ld(unpacked_inputs)), fscope)\n            File \"/var/tmp/__autograph_generated_fileo809d_t3.py\", line 128, in tf__call\n                ag__.if_stmt(ag__.and_(lambda : ag__.ld(input_ids) is not None, lambda : ag__.ld(inputs_embeds) is not None), if_body_2, else_body_2, get_state_2, set_state_2, ('input_shape',), 1)\n            File \"/var/tmp/__autograph_generated_fileo809d_t3.py\", line 126, in else_body_2\n                ag__.if_stmt(ag__.ld(input_ids) is not None, if_body_1, else_body_1, get_state_1, set_state_1, ('input_shape',), 1)\n            File \"/var/tmp/__autograph_generated_fileo809d_t3.py\", line 124, in else_body_1\n                ag__.if_stmt(ag__.ld(inputs_embeds) is not None, if_body, else_body, get_state, set_state, ('input_shape',), 1)\n            File \"/var/tmp/__autograph_generated_fileo809d_t3.py\", line 122, in else_body\n                raise ag__.converted_call(ag__.ld(ValueError), ('You have to specify either decoder_input_ids or decoder_inputs_embeds',), None, fscope)\n        \n            ValueError: Exception encountered when calling layer 'decoder' (type TFPegasusDecoder).\n            \n            in user code:\n            \n                File \"/opt/conda/envs/xla-env/lib/python3.10/site-packages/transformers/modeling_tf_utils.py\", line 1079, in run_call_with_unpacked_inputs  *\n                    return func(self, **unpacked_inputs)\n                File \"/opt/conda/envs/xla-env/lib/python3.10/site-packages/transformers/models/pegasus/modeling_tf_pegasus.py\", line 1020, in call  *\n                    raise ValueError(\"You have to specify either decoder_input_ids or decoder_inputs_embeds\")\n            \n                ValueError: You have to specify either decoder_input_ids or decoder_inputs_embeds\n            \n            \n            Call arguments received by layer 'decoder' (type TFPegasusDecoder):\n              \u2022 input_ids=None\n              \u2022 inputs_embeds=None\n              \u2022 attention_mask=None\n              \u2022 position_ids=None\n              \u2022 encoder_hidden_states=tf.Tensor(shape=(1, 32, 1024), dtype=float32)\n              \u2022 encoder_attention_mask=tf.Tensor(shape=(1, 32), dtype=int32)\n              \u2022 head_mask=None\n              \u2022 cross_attn_head_mask=None\n              \u2022 past_key_values=None\n              \u2022 use_cache=False\n              \u2022 output_attentions=False\n              \u2022 output_hidden_states=False\n              \u2022 return_dict=True\n              \u2022 training=False\n        \n        \n        Call arguments received by layer 'model' (type TFPegasusMainLayer):\n          \u2022 input_ids=tf.Tensor(shape=(1, 32), dtype=int32)\n          \u2022 attention_mask=tf.Tensor(shape=(1, 32), dtype=int32)\n          \u2022 decoder_input_ids=None\n          \u2022 decoder_attention_mask=None\n          \u2022 decoder_position_ids=None\n          \u2022 head_mask=None\n          \u2022 decoder_head_mask=None\n          \u2022 cross_attn_head_mask=None\n          \u2022 encoder_outputs=None\n          \u2022 past_key_values=None\n          \u2022 inputs_embeds=None\n          \u2022 decoder_inputs_embeds=None\n          \u2022 use_cache=True\n          \u2022 output_attentions=False\n          \u2022 output_hidden_states=False\n          \u2022 return_dict=True\n          \u2022 training=False\n          \u2022 kwargs=<class 'inspect._empty'>\n    \n    \n    Call arguments received by layer 'tf_pegasus_model_1' (type TFPegasusModel):\n      \u2022 input_ids=tf.Tensor(shape=(1, 32), dtype=int32)\n      \u2022 attention_mask=tf.Tensor(shape=(1, 32), dtype=int32)\n      \u2022 decoder_input_ids=None\n      \u2022 decoder_attention_mask=None\n      \u2022 decoder_position_ids=None\n      \u2022 head_mask=None\n      \u2022 decoder_head_mask=None\n      \u2022 cross_attn_head_mask=None\n      \u2022 encoder_outputs=None\n      \u2022 past_key_values=None\n      \u2022 inputs_embeds=None\n      \u2022 decoder_inputs_embeds=None\n      \u2022 use_cache=None\n      \u2022 output_attentions=None\n      \u2022 output_hidden_states=None\n      \u2022 return_dict=None\n      \u2022 training=False\n      \u2022 kwargs=<class 'inspect._empty'>\n"
            },
            "jax_flax_eager": {
                "ttft_s": 13.117932796478271
            }
        }
    },
    {
        "PegasusForConditionalGeneration": {
            "error": "AttributeError(\"'PegasusForConditionalGeneration' object has no attribute '_shift_right'\")"
        }
    },
    {
        "PLBartForConditionalGeneration": {
            "error": "AttributeError(\"'PLBartForConditionalGeneration' object has no attribute '_shift_right'\")"
        }
    },
    {
        "RobertaForCausalLM": {
            "pytorch_torch_compile_inductor": {
                "ttft_s": 9.969597339630127
            },
            "pytorch_torch_compile_eager": {
                "ttft_s": 2.817627191543579
            },
            "pytorch_torch_compile_cudagraphs": {
                "ttft_s": 6.746962308883667
            },
            "pytorch_torch_compile_onnxrt": {
                "ttft_s": 11.948337316513062
            },
            "pytorch_torch_compile_openxla": {
                "error": "torch_xla/csrc/runtime/pjrt_registry.cc:214 : Check failed: client \n*** Begin stack trace ***\n\ttsl::CurrentStackTrace()\n\ttorch_xla::runtime::InitializePjRt(std::string const&)\n\ttorch_xla::runtime::PjRtComputationClient::PjRtComputationClient()\n\t\n\ttorch_xla::runtime::GetComputationClient()\n\ttorch_xla::bridge::GetDefaultDevice()\n\ttorch_xla::bridge::GetCurrentDevice()\n\ttorch_xla::bridge::GetCurrentAtenDevice()\n\t\n\t\n\t\n\t_PyObject_MakeTpCall\n\t_PyEval_EvalFrameDefault\n\t\n\t_PyFunction_Vectorcall\n\t_PyEval_EvalFrameDefault\n\t\n\t_PyFunction_Vectorcall\n\t_PyEval_EvalFrameDefault\n\t\n\t_PyFunction_Vectorcall\n\t_PyEval_EvalFrameDefault\n\t\n\t_PyFunction_Vectorcall\n\t_PyEval_EvalFrameDefault\n\t\n\t_PyFunction_Vectorcall\n\t_PyEval_EvalFrameDefault\n\t\n\t_PyFunction_Vectorcall\n\t_PyEval_EvalFrameDefault\n\t\n\t_PyFunction_Vectorcall\n\t_PyEval_EvalFrameDefault\n\t\n\t_PyFunction_Vectorcall\n\t_PyEval_EvalFrameDefault\n\t\n\t_PyFunction_Vectorcall\n\tTHPFunction_apply(_object*, _object*)\n\t\n\tPyObject_Call\n\t_PyEval_EvalFrameDefault\n\t\n\t\n\t_PyEval_EvalFrameDefault\n\t\n\t_PyFunction_Vectorcall\n\t_PyEval_EvalFrameDefault\n\t\n\t_PyFunction_Vectorcall\n\t_PyEval_EvalFrameDefault\n\t\n\t_PyFunction_Vectorcall\n\t_PyEval_EvalFrameDefault\n\t\n\t_PyFunction_Vectorcall\n\t_PyEval_EvalFrameDefault\n\t\n\t_PyFunction_Vectorcall\n\t_PyEval_EvalFrameDefault\n\t\n\t\n\t\n\tPyObject_Call\n\t_PyEval_EvalFrameDefault\n\t\n\t\n\tPyObject_Call\n\t_PyEval_EvalFrameDefault\n\t\n\t\n\tPyObject_Call\n\t_PyEval_EvalFrameDefault\n\t\n\t_PyFunction_Vectorcall\n\tPyObject_Call\n\t_PyEval_EvalFrameDefault\n\t\n\t\n\tPyObject_Call\n\t_PyEval_EvalFrameDefault\n\t\n\t_PyFunction_Vectorcall\n\t_PyObject_FastCallDictTstate\n\t_PyObject_Call_Prepend\n\t\n\tPyObject_Call\n\t_PyEval_EvalFrameDefault\n\t\n\t_PyFunction_Vectorcall\n\t_PyEval_EvalFrameDefault\n\t_PyFunction_Vectorcall\n\t_PyEval_EvalFrameDefault\n\t\n\tPyEval_EvalCode\n\t\n\t\n\t\n\t_PyRun_SimpleFileObject\n\t_PyRun_AnyFileObject\n\tPy_RunMain\n\tPy_BytesMain\n\t__libc_start_main\n\t\n*** End stack trace ***\nUnknown PJRT_DEVICE 'CUDA'"
            },
            "pytorch_torch_compile_tvm": {
                "error": "backend='tvm' raised:\nModuleNotFoundError: No module named 'tvm'\n\nSet TORCH_LOGS=\"+dynamo\" and TORCHDYNAMO_VERBOSE=1 for more information\n\n\nYou can suppress this exception and fall back to eager by setting:\n    import torch._dynamo\n    torch._dynamo.config.suppress_errors = True\n"
            },
            "jax_flax_xla": {
                "ttft_s": 6.395744323730469
            },
            "tensorflow_eager": {
                "ttft_s": 0.22659087181091309
            },
            "tensorflow_xla": {
                "ttft_s": 4.99056339263916
            },
            "jax_flax_eager": {
                "ttft_s": 4.439101219177246
            }
        }
    },
    {
        "RobertaForQuestionAnswering": {
            "pytorch_torch_compile_inductor": {
                "ttft_s": 9.828948020935059
            },
            "pytorch_torch_compile_eager": {
                "ttft_s": 2.8303956985473633
            },
            "pytorch_torch_compile_cudagraphs": {
                "ttft_s": 16.366379499435425
            },
            "pytorch_torch_compile_onnxrt": {
                "ttft_s": 12.082936525344849
            },
            "pytorch_torch_compile_openxla": {
                "error": "torch_xla/csrc/runtime/pjrt_registry.cc:214 : Check failed: client \n*** Begin stack trace ***\n\ttsl::CurrentStackTrace()\n\ttorch_xla::runtime::InitializePjRt(std::string const&)\n\ttorch_xla::runtime::PjRtComputationClient::PjRtComputationClient()\n\t\n\ttorch_xla::runtime::GetComputationClient()\n\ttorch_xla::bridge::GetDefaultDevice()\n\ttorch_xla::bridge::GetCurrentDevice()\n\ttorch_xla::bridge::GetCurrentAtenDevice()\n\t\n\t\n\t\n\t_PyObject_MakeTpCall\n\t_PyEval_EvalFrameDefault\n\t\n\t_PyFunction_Vectorcall\n\t_PyEval_EvalFrameDefault\n\t\n\t_PyFunction_Vectorcall\n\t_PyEval_EvalFrameDefault\n\t\n\t_PyFunction_Vectorcall\n\t_PyEval_EvalFrameDefault\n\t\n\t_PyFunction_Vectorcall\n\t_PyEval_EvalFrameDefault\n\t\n\t_PyFunction_Vectorcall\n\t_PyEval_EvalFrameDefault\n\t\n\t_PyFunction_Vectorcall\n\t_PyEval_EvalFrameDefault\n\t\n\t_PyFunction_Vectorcall\n\t_PyEval_EvalFrameDefault\n\t\n\t_PyFunction_Vectorcall\n\t_PyEval_EvalFrameDefault\n\t\n\t_PyFunction_Vectorcall\n\tTHPFunction_apply(_object*, _object*)\n\t\n\tPyObject_Call\n\t_PyEval_EvalFrameDefault\n\t\n\t\n\t_PyEval_EvalFrameDefault\n\t\n\t_PyFunction_Vectorcall\n\t_PyEval_EvalFrameDefault\n\t\n\t_PyFunction_Vectorcall\n\t_PyEval_EvalFrameDefault\n\t\n\t_PyFunction_Vectorcall\n\t_PyEval_EvalFrameDefault\n\t\n\t_PyFunction_Vectorcall\n\t_PyEval_EvalFrameDefault\n\t\n\t_PyFunction_Vectorcall\n\t_PyEval_EvalFrameDefault\n\t\n\t\n\t\n\tPyObject_Call\n\t_PyEval_EvalFrameDefault\n\t\n\t\n\tPyObject_Call\n\t_PyEval_EvalFrameDefault\n\t\n\t\n\tPyObject_Call\n\t_PyEval_EvalFrameDefault\n\t\n\t_PyFunction_Vectorcall\n\tPyObject_Call\n\t_PyEval_EvalFrameDefault\n\t\n\t\n\tPyObject_Call\n\t_PyEval_EvalFrameDefault\n\t\n\t_PyFunction_Vectorcall\n\t_PyObject_FastCallDictTstate\n\t_PyObject_Call_Prepend\n\t\n\tPyObject_Call\n\t_PyEval_EvalFrameDefault\n\t\n\t_PyFunction_Vectorcall\n\t_PyEval_EvalFrameDefault\n\t_PyFunction_Vectorcall\n\t_PyEval_EvalFrameDefault\n\t\n\tPyEval_EvalCode\n\t\n\t\n\t\n\t_PyRun_SimpleFileObject\n\t_PyRun_AnyFileObject\n\tPy_RunMain\n\tPy_BytesMain\n\t__libc_start_main\n\t\n*** End stack trace ***\nUnknown PJRT_DEVICE 'CUDA'"
            },
            "pytorch_torch_compile_tvm": {
                "error": "backend='tvm' raised:\nModuleNotFoundError: No module named 'tvm'\n\nSet TORCH_LOGS=\"+dynamo\" and TORCHDYNAMO_VERBOSE=1 for more information\n\n\nYou can suppress this exception and fall back to eager by setting:\n    import torch._dynamo\n    torch._dynamo.config.suppress_errors = True\n"
            },
            "jax_flax_xla": {
                "ttft_s": 4.71285080909729
            },
            "tensorflow_eager": {
                "ttft_s": 0.22772645950317383
            },
            "tensorflow_xla": {
                "ttft_s": 3.466984987258911
            },
            "jax_flax_eager": {
                "ttft_s": 1.684626579284668
            }
        }
    },
    {
        "Speech2Text2ForCausalLM": {
            "pytorch_torch_compile_inductor": {
                "ttft_s": 4.594448089599609
            },
            "pytorch_torch_compile_eager": {
                "ttft_s": 0.9401781558990479
            },
            "pytorch_torch_compile_cudagraphs": {
                "ttft_s": 3.02421236038208
            },
            "pytorch_torch_compile_onnxrt": {
                "ttft_s": 5.403911113739014
            },
            "pytorch_torch_compile_openxla": {
                "error": "torch_xla/csrc/runtime/pjrt_registry.cc:214 : Check failed: client \n*** Begin stack trace ***\n\ttsl::CurrentStackTrace()\n\ttorch_xla::runtime::InitializePjRt(std::string const&)\n\ttorch_xla::runtime::PjRtComputationClient::PjRtComputationClient()\n\t\n\ttorch_xla::runtime::GetComputationClient()\n\ttorch_xla::bridge::GetDefaultDevice()\n\ttorch_xla::bridge::GetCurrentDevice()\n\ttorch_xla::bridge::GetCurrentAtenDevice()\n\t\n\t\n\t\n\t_PyObject_MakeTpCall\n\t_PyEval_EvalFrameDefault\n\t\n\t_PyFunction_Vectorcall\n\t_PyEval_EvalFrameDefault\n\t\n\t_PyFunction_Vectorcall\n\t_PyEval_EvalFrameDefault\n\t\n\t_PyFunction_Vectorcall\n\t_PyEval_EvalFrameDefault\n\t\n\t_PyFunction_Vectorcall\n\t_PyEval_EvalFrameDefault\n\t\n\t_PyFunction_Vectorcall\n\t_PyEval_EvalFrameDefault\n\t\n\t_PyFunction_Vectorcall\n\t_PyEval_EvalFrameDefault\n\t\n\t_PyFunction_Vectorcall\n\t_PyEval_EvalFrameDefault\n\t\n\t_PyFunction_Vectorcall\n\t_PyEval_EvalFrameDefault\n\t\n\t_PyFunction_Vectorcall\n\tTHPFunction_apply(_object*, _object*)\n\t\n\tPyObject_Call\n\t_PyEval_EvalFrameDefault\n\t\n\t\n\t_PyEval_EvalFrameDefault\n\t\n\t_PyFunction_Vectorcall\n\t_PyEval_EvalFrameDefault\n\t\n\t_PyFunction_Vectorcall\n\t_PyEval_EvalFrameDefault\n\t\n\t_PyFunction_Vectorcall\n\t_PyEval_EvalFrameDefault\n\t\n\t_PyFunction_Vectorcall\n\t_PyEval_EvalFrameDefault\n\t\n\t_PyFunction_Vectorcall\n\t_PyEval_EvalFrameDefault\n\t\n\t\n\t\n\tPyObject_Call\n\t_PyEval_EvalFrameDefault\n\t\n\t\n\tPyObject_Call\n\t_PyEval_EvalFrameDefault\n\t\n\t\n\tPyObject_Call\n\t_PyEval_EvalFrameDefault\n\t\n\t_PyFunction_Vectorcall\n\tPyObject_Call\n\t_PyEval_EvalFrameDefault\n\t\n\t\n\tPyObject_Call\n\t_PyEval_EvalFrameDefault\n\t\n\t_PyFunction_Vectorcall\n\t_PyObject_FastCallDictTstate\n\t_PyObject_Call_Prepend\n\t\n\tPyObject_Call\n\t_PyEval_EvalFrameDefault\n\t\n\t_PyFunction_Vectorcall\n\t_PyEval_EvalFrameDefault\n\t_PyFunction_Vectorcall\n\t_PyEval_EvalFrameDefault\n\t\n\tPyEval_EvalCode\n\t\n\t\n\t\n\t_PyRun_SimpleFileObject\n\t_PyRun_AnyFileObject\n\tPy_RunMain\n\tPy_BytesMain\n\t__libc_start_main\n\t\n*** End stack trace ***\nUnknown PJRT_DEVICE 'CUDA'"
            },
            "pytorch_torch_compile_tvm": {
                "error": "backend='tvm' raised:\nModuleNotFoundError: No module named 'tvm'\n\nSet TORCH_LOGS=\"+dynamo\" and TORCHDYNAMO_VERBOSE=1 for more information\n\n\nYou can suppress this exception and fall back to eager by setting:\n    import torch._dynamo\n    torch._dynamo.config.suppress_errors = True\n"
            },
            "jax_flax_xla": {
                "error": "Unrecognized configuration class <class 'transformers.models.speech_to_text.configuration_speech_to_text.Speech2TextConfig'> for this kind of AutoModel: FlaxAutoModel.\nModel type should be one of AlbertConfig, BartConfig, BeitConfig, BertConfig, BigBirdConfig, BlenderbotConfig, BlenderbotSmallConfig, BloomConfig, CLIPConfig, Dinov2Config, DistilBertConfig, ElectraConfig, GemmaConfig, GPT2Config, GPT2Config, GPTNeoConfig, GPTJConfig, LlamaConfig, LongT5Config, MarianConfig, MBartConfig, MistralConfig, MT5Config, OPTConfig, PegasusConfig, RegNetConfig, ResNetConfig, RobertaConfig, RobertaPreLayerNormConfig, RoFormerConfig, T5Config, VisionTextDualEncoderConfig, ViTConfig, Wav2Vec2Config, WhisperConfig, XGLMConfig, XLMRobertaConfig."
            },
            "tensorflow_eager": {
                "error": "The first argument to `Layer.call` must always be passed."
            },
            "tensorflow_xla": {
                "error": "in user code:\n\n    File \"/home/mmoffatt/jax_vs_pytorch/benchmark_ttft.py\", line 176, in fwd  *\n        return model(**inp)[0]\n    File \"/opt/conda/envs/xla-env/lib/python3.10/site-packages/tf_keras/src/utils/traceback_utils.py\", line 70, in error_handler  **\n        raise e.with_traceback(filtered_tb) from None\n    File \"/opt/conda/envs/xla-env/lib/python3.10/site-packages/tf_keras/src/utils/layer_utils.py\", line 971, in split_out_first_arg\n        raise ValueError(\n\n    ValueError: The first argument to `Layer.call` must always be passed.\n"
            }
        }
    },
    {
        "T5ForConditionalGeneration": {
            "pytorch_torch_compile_inductor": {
                "ttft_s": 26.679537057876587
            },
            "pytorch_torch_compile_eager": {
                "ttft_s": 8.317655086517334
            },
            "pytorch_torch_compile_cudagraphs": {
                "ttft_s": 20.88204288482666
            },
            "pytorch_torch_compile_onnxrt": {
                "ttft_s": 46.86714148521423
            },
            "pytorch_torch_compile_openxla": {
                "error": "torch_xla/csrc/runtime/pjrt_registry.cc:214 : Check failed: client \n*** Begin stack trace ***\n\ttsl::CurrentStackTrace()\n\ttorch_xla::runtime::InitializePjRt(std::string const&)\n\ttorch_xla::runtime::PjRtComputationClient::PjRtComputationClient()\n\t\n\ttorch_xla::runtime::GetComputationClient()\n\ttorch_xla::bridge::GetDefaultDevice()\n\ttorch_xla::bridge::GetCurrentDevice()\n\ttorch_xla::bridge::GetCurrentAtenDevice()\n\t\n\t\n\t\n\t_PyObject_MakeTpCall\n\t_PyEval_EvalFrameDefault\n\t\n\t_PyFunction_Vectorcall\n\t_PyEval_EvalFrameDefault\n\t\n\t_PyFunction_Vectorcall\n\t_PyEval_EvalFrameDefault\n\t\n\t_PyFunction_Vectorcall\n\t_PyEval_EvalFrameDefault\n\t\n\t_PyFunction_Vectorcall\n\t_PyEval_EvalFrameDefault\n\t\n\t_PyFunction_Vectorcall\n\t_PyEval_EvalFrameDefault\n\t\n\t_PyFunction_Vectorcall\n\t_PyEval_EvalFrameDefault\n\t\n\t_PyFunction_Vectorcall\n\t_PyEval_EvalFrameDefault\n\t\n\t_PyFunction_Vectorcall\n\t_PyEval_EvalFrameDefault\n\t\n\t_PyFunction_Vectorcall\n\tTHPFunction_apply(_object*, _object*)\n\t\n\tPyObject_Call\n\t_PyEval_EvalFrameDefault\n\t\n\t\n\t_PyEval_EvalFrameDefault\n\t\n\t_PyFunction_Vectorcall\n\t_PyEval_EvalFrameDefault\n\t\n\t_PyFunction_Vectorcall\n\t_PyEval_EvalFrameDefault\n\t\n\t_PyFunction_Vectorcall\n\t_PyEval_EvalFrameDefault\n\t\n\t_PyFunction_Vectorcall\n\t_PyEval_EvalFrameDefault\n\t\n\t_PyFunction_Vectorcall\n\t_PyEval_EvalFrameDefault\n\t\n\t\n\t\n\tPyObject_Call\n\t_PyEval_EvalFrameDefault\n\t\n\t\n\tPyObject_Call\n\t_PyEval_EvalFrameDefault\n\t\n\t\n\tPyObject_Call\n\t_PyEval_EvalFrameDefault\n\t\n\t_PyFunction_Vectorcall\n\tPyObject_Call\n\t_PyEval_EvalFrameDefault\n\t\n\t\n\tPyObject_Call\n\t_PyEval_EvalFrameDefault\n\t\n\t_PyFunction_Vectorcall\n\t_PyObject_FastCallDictTstate\n\t_PyObject_Call_Prepend\n\t\n\tPyObject_Call\n\t_PyEval_EvalFrameDefault\n\t\n\t_PyFunction_Vectorcall\n\t_PyEval_EvalFrameDefault\n\t_PyFunction_Vectorcall\n\t_PyEval_EvalFrameDefault\n\t\n\tPyEval_EvalCode\n\t\n\t\n\t\n\t_PyRun_SimpleFileObject\n\t_PyRun_AnyFileObject\n\tPy_RunMain\n\tPy_BytesMain\n\t__libc_start_main\n\t\n*** End stack trace ***\nUnknown PJRT_DEVICE 'CUDA'"
            },
            "pytorch_torch_compile_tvm": {
                "error": "backend='tvm' raised:\nModuleNotFoundError: No module named 'tvm'\n\nSet TORCH_LOGS=\"+dynamo\" and TORCHDYNAMO_VERBOSE=1 for more information\n\n\nYou can suppress this exception and fall back to eager by setting:\n    import torch._dynamo\n    torch._dynamo.config.suppress_errors = True\n"
            },
            "jax_flax_xla": {
                "ttft_s": 8.299354314804077
            },
            "tensorflow_eager": {
                "error": "Exception encountered when calling layer 'decoder' (type TFT5MainLayer).\n\nYou have to specify either decoder_input_ids or decoder_inputs_embeds\n\nCall arguments received by layer 'decoder' (type TFT5MainLayer):\n  \u2022 input_ids=None\n  \u2022 attention_mask=None\n  \u2022 encoder_hidden_states=tf.Tensor(shape=(1, 32, 768), dtype=float32)\n  \u2022 encoder_attention_mask=tf.Tensor(shape=(1, 32), dtype=int32)\n  \u2022 inputs_embeds=None\n  \u2022 head_mask=None\n  \u2022 encoder_head_mask=None\n  \u2022 past_key_values=None\n  \u2022 use_cache=True\n  \u2022 output_attentions=False\n  \u2022 output_hidden_states=False\n  \u2022 return_dict=True\n  \u2022 training=False"
            },
            "tensorflow_xla": {
                "error": "in user code:\n\n    File \"/home/mmoffatt/jax_vs_pytorch/benchmark_ttft.py\", line 176, in fwd  *\n        return model(**inp)[0]\n    File \"/opt/conda/envs/xla-env/lib/python3.10/site-packages/tf_keras/src/utils/traceback_utils.py\", line 70, in error_handler  **\n        raise e.with_traceback(filtered_tb) from None\n    File \"/var/tmp/__autograph_generated_file138ue72o.py\", line 37, in tf__run_call_with_unpacked_inputs\n        retval_ = ag__.converted_call(ag__.ld(func), (ag__.ld(self),), dict(**ag__.ld(unpacked_inputs)), fscope)\n    File \"/var/tmp/__autograph_generated_filesn5endok.py\", line 91, in tf__call\n        decoder_outputs = ag__.converted_call(ag__.ld(self).decoder, (ag__.ld(decoder_input_ids),), dict(attention_mask=ag__.ld(decoder_attention_mask), encoder_hidden_states=ag__.ld(hidden_states), encoder_attention_mask=ag__.ld(attention_mask), inputs_embeds=ag__.ld(decoder_inputs_embeds), head_mask=ag__.ld(decoder_head_mask), past_key_values=ag__.ld(past_key_values), use_cache=ag__.ld(use_cache), output_attentions=ag__.ld(output_attentions), output_hidden_states=ag__.ld(output_hidden_states), return_dict=ag__.ld(return_dict), training=ag__.ld(training)), fscope)\n    File \"/var/tmp/__autograph_generated_file138ue72o.py\", line 37, in tf__run_call_with_unpacked_inputs\n        retval_ = ag__.converted_call(ag__.ld(func), (ag__.ld(self),), dict(**ag__.ld(unpacked_inputs)), fscope)\n    File \"/var/tmp/__autograph_generated_filehcwpld1c.py\", line 65, in tf__call\n        ag__.if_stmt(ag__.and_(lambda : ag__.ld(input_ids) is not None, lambda : ag__.ld(inputs_embeds) is not None), if_body_2, else_body_2, get_state_2, set_state_2, ('input_ids', 'input_shape'), 2)\n    File \"/var/tmp/__autograph_generated_filehcwpld1c.py\", line 62, in else_body_2\n        ag__.if_stmt(ag__.ld(input_ids) is not None, if_body_1, else_body_1, get_state_1, set_state_1, ('input_ids', 'input_shape'), 2)\n    File \"/var/tmp/__autograph_generated_filehcwpld1c.py\", line 59, in else_body_1\n        ag__.if_stmt(ag__.ld(inputs_embeds) is not None, if_body, else_body, get_state, set_state, ('input_shape',), 1)\n    File \"/var/tmp/__autograph_generated_filehcwpld1c.py\", line 56, in else_body\n        raise ag__.converted_call(ag__.ld(ValueError), (f'You have to specify either {ag__.ld(err_msg_prefix)}input_ids or {ag__.ld(err_msg_prefix)}inputs_embeds',), None, fscope)\n\n    ValueError: Exception encountered when calling layer 'tft5_for_conditional_generation_1' (type TFT5ForConditionalGeneration).\n    \n    in user code:\n    \n        File \"/opt/conda/envs/xla-env/lib/python3.10/site-packages/transformers/modeling_tf_utils.py\", line 1079, in run_call_with_unpacked_inputs  *\n            return func(self, **unpacked_inputs)\n        File \"/opt/conda/envs/xla-env/lib/python3.10/site-packages/transformers/models/t5/modeling_tf_t5.py\", line 1455, in call  *\n            decoder_outputs = self.decoder(\n        File \"/opt/conda/envs/xla-env/lib/python3.10/site-packages/tf_keras/src/utils/traceback_utils.py\", line 70, in error_handler  **\n            raise e.with_traceback(filtered_tb) from None\n        File \"/var/tmp/__autograph_generated_file138ue72o.py\", line 37, in tf__run_call_with_unpacked_inputs\n            retval_ = ag__.converted_call(ag__.ld(func), (ag__.ld(self),), dict(**ag__.ld(unpacked_inputs)), fscope)\n        File \"/var/tmp/__autograph_generated_filehcwpld1c.py\", line 65, in tf__call\n            ag__.if_stmt(ag__.and_(lambda : ag__.ld(input_ids) is not None, lambda : ag__.ld(inputs_embeds) is not None), if_body_2, else_body_2, get_state_2, set_state_2, ('input_ids', 'input_shape'), 2)\n        File \"/var/tmp/__autograph_generated_filehcwpld1c.py\", line 62, in else_body_2\n            ag__.if_stmt(ag__.ld(input_ids) is not None, if_body_1, else_body_1, get_state_1, set_state_1, ('input_ids', 'input_shape'), 2)\n        File \"/var/tmp/__autograph_generated_filehcwpld1c.py\", line 59, in else_body_1\n            ag__.if_stmt(ag__.ld(inputs_embeds) is not None, if_body, else_body, get_state, set_state, ('input_shape',), 1)\n        File \"/var/tmp/__autograph_generated_filehcwpld1c.py\", line 56, in else_body\n            raise ag__.converted_call(ag__.ld(ValueError), (f'You have to specify either {ag__.ld(err_msg_prefix)}input_ids or {ag__.ld(err_msg_prefix)}inputs_embeds',), None, fscope)\n    \n        ValueError: Exception encountered when calling layer 'decoder' (type TFT5MainLayer).\n        \n        in user code:\n        \n            File \"/opt/conda/envs/xla-env/lib/python3.10/site-packages/transformers/modeling_tf_utils.py\", line 1079, in run_call_with_unpacked_inputs  *\n                return func(self, **unpacked_inputs)\n            File \"/opt/conda/envs/xla-env/lib/python3.10/site-packages/transformers/models/t5/modeling_tf_t5.py\", line 754, in call  *\n                raise ValueError(f\"You have to specify either {err_msg_prefix}input_ids or {err_msg_prefix}inputs_embeds\")\n        \n            ValueError: You have to specify either decoder_input_ids or decoder_inputs_embeds\n        \n        \n        Call arguments received by layer 'decoder' (type TFT5MainLayer):\n          \u2022 input_ids=None\n          \u2022 attention_mask=None\n          \u2022 encoder_hidden_states=tf.Tensor(shape=(1, 32, 768), dtype=float32)\n          \u2022 encoder_attention_mask=tf.Tensor(shape=(1, 32), dtype=int32)\n          \u2022 inputs_embeds=None\n          \u2022 head_mask=None\n          \u2022 encoder_head_mask=None\n          \u2022 past_key_values=None\n          \u2022 use_cache=True\n          \u2022 output_attentions=False\n          \u2022 output_hidden_states=False\n          \u2022 return_dict=True\n          \u2022 training=False\n    \n    \n    Call arguments received by layer 'tft5_for_conditional_generation_1' (type TFT5ForConditionalGeneration):\n      \u2022 input_ids=tf.Tensor(shape=(1, 32), dtype=int32)\n      \u2022 attention_mask=tf.Tensor(shape=(1, 32), dtype=int32)\n      \u2022 decoder_input_ids=None\n      \u2022 decoder_attention_mask=None\n      \u2022 head_mask=None\n      \u2022 decoder_head_mask=None\n      \u2022 encoder_outputs=None\n      \u2022 past_key_values=None\n      \u2022 inputs_embeds=None\n      \u2022 decoder_inputs_embeds=None\n      \u2022 labels=None\n      \u2022 use_cache=None\n      \u2022 output_attentions=None\n      \u2022 output_hidden_states=None\n      \u2022 return_dict=None\n      \u2022 training=False\n"
            },
            "jax_flax_eager": {
                "ttft_s": 7.460165739059448
            }
        }
    },
    {
        "T5Small": {
            "pytorch_torch_compile_inductor": {
                "ttft_s": 30.0725576877594
            },
            "pytorch_torch_compile_eager": {
                "ttft_s": 4.543751955032349
            },
            "pytorch_torch_compile_cudagraphs": {
                "ttft_s": 11.067124366760254
            },
            "pytorch_torch_compile_onnxrt": {
                "ttft_s": 0.05124068260192871
            },
            "pytorch_torch_compile_openxla": {
                "ttft_s": 0.033941030502319336
            },
            "pytorch_torch_compile_tvm": {
                "ttft_s": 0.03305459022521973
            },
            "jax_flax_xla": {
                "ttft_s": 12.073627710342407
            },
            "tensorflow_eager": {
                "error": "Exception encountered when calling layer 'decoder' (type TFT5MainLayer).\n\nYou have to specify either decoder_input_ids or decoder_inputs_embeds\n\nCall arguments received by layer 'decoder' (type TFT5MainLayer):\n  \u2022 input_ids=None\n  \u2022 attention_mask=None\n  \u2022 encoder_hidden_states=tf.Tensor(shape=(1, 32, 512), dtype=float32)\n  \u2022 encoder_attention_mask=tf.Tensor(shape=(1, 32), dtype=int32)\n  \u2022 inputs_embeds=None\n  \u2022 head_mask=None\n  \u2022 encoder_head_mask=None\n  \u2022 past_key_values=None\n  \u2022 use_cache=True\n  \u2022 output_attentions=False\n  \u2022 output_hidden_states=False\n  \u2022 return_dict=True\n  \u2022 training=False"
            },
            "tensorflow_xla": {
                "error": "in user code:\n\n    File \"/home/mmoffatt/jax_vs_pytorch/benchmark_ttft.py\", line 176, in fwd  *\n        return model(**inp)[0]\n    File \"/opt/conda/envs/xla-env/lib/python3.10/site-packages/tf_keras/src/utils/traceback_utils.py\", line 70, in error_handler  **\n        raise e.with_traceback(filtered_tb) from None\n    File \"/var/tmp/__autograph_generated_file138ue72o.py\", line 37, in tf__run_call_with_unpacked_inputs\n        retval_ = ag__.converted_call(ag__.ld(func), (ag__.ld(self),), dict(**ag__.ld(unpacked_inputs)), fscope)\n    File \"/var/tmp/__autograph_generated_filesn5endok.py\", line 91, in tf__call\n        decoder_outputs = ag__.converted_call(ag__.ld(self).decoder, (ag__.ld(decoder_input_ids),), dict(attention_mask=ag__.ld(decoder_attention_mask), encoder_hidden_states=ag__.ld(hidden_states), encoder_attention_mask=ag__.ld(attention_mask), inputs_embeds=ag__.ld(decoder_inputs_embeds), head_mask=ag__.ld(decoder_head_mask), past_key_values=ag__.ld(past_key_values), use_cache=ag__.ld(use_cache), output_attentions=ag__.ld(output_attentions), output_hidden_states=ag__.ld(output_hidden_states), return_dict=ag__.ld(return_dict), training=ag__.ld(training)), fscope)\n    File \"/var/tmp/__autograph_generated_file138ue72o.py\", line 37, in tf__run_call_with_unpacked_inputs\n        retval_ = ag__.converted_call(ag__.ld(func), (ag__.ld(self),), dict(**ag__.ld(unpacked_inputs)), fscope)\n    File \"/var/tmp/__autograph_generated_filehcwpld1c.py\", line 65, in tf__call\n        ag__.if_stmt(ag__.and_(lambda : ag__.ld(input_ids) is not None, lambda : ag__.ld(inputs_embeds) is not None), if_body_2, else_body_2, get_state_2, set_state_2, ('input_ids', 'input_shape'), 2)\n    File \"/var/tmp/__autograph_generated_filehcwpld1c.py\", line 62, in else_body_2\n        ag__.if_stmt(ag__.ld(input_ids) is not None, if_body_1, else_body_1, get_state_1, set_state_1, ('input_ids', 'input_shape'), 2)\n    File \"/var/tmp/__autograph_generated_filehcwpld1c.py\", line 59, in else_body_1\n        ag__.if_stmt(ag__.ld(inputs_embeds) is not None, if_body, else_body, get_state, set_state, ('input_shape',), 1)\n    File \"/var/tmp/__autograph_generated_filehcwpld1c.py\", line 56, in else_body\n        raise ag__.converted_call(ag__.ld(ValueError), (f'You have to specify either {ag__.ld(err_msg_prefix)}input_ids or {ag__.ld(err_msg_prefix)}inputs_embeds',), None, fscope)\n\n    ValueError: Exception encountered when calling layer 'tft5_for_conditional_generation_3' (type TFT5ForConditionalGeneration).\n    \n    in user code:\n    \n        File \"/opt/conda/envs/xla-env/lib/python3.10/site-packages/transformers/modeling_tf_utils.py\", line 1079, in run_call_with_unpacked_inputs  *\n            return func(self, **unpacked_inputs)\n        File \"/opt/conda/envs/xla-env/lib/python3.10/site-packages/transformers/models/t5/modeling_tf_t5.py\", line 1455, in call  *\n            decoder_outputs = self.decoder(\n        File \"/opt/conda/envs/xla-env/lib/python3.10/site-packages/tf_keras/src/utils/traceback_utils.py\", line 70, in error_handler  **\n            raise e.with_traceback(filtered_tb) from None\n        File \"/var/tmp/__autograph_generated_file138ue72o.py\", line 37, in tf__run_call_with_unpacked_inputs\n            retval_ = ag__.converted_call(ag__.ld(func), (ag__.ld(self),), dict(**ag__.ld(unpacked_inputs)), fscope)\n        File \"/var/tmp/__autograph_generated_filehcwpld1c.py\", line 65, in tf__call\n            ag__.if_stmt(ag__.and_(lambda : ag__.ld(input_ids) is not None, lambda : ag__.ld(inputs_embeds) is not None), if_body_2, else_body_2, get_state_2, set_state_2, ('input_ids', 'input_shape'), 2)\n        File \"/var/tmp/__autograph_generated_filehcwpld1c.py\", line 62, in else_body_2\n            ag__.if_stmt(ag__.ld(input_ids) is not None, if_body_1, else_body_1, get_state_1, set_state_1, ('input_ids', 'input_shape'), 2)\n        File \"/var/tmp/__autograph_generated_filehcwpld1c.py\", line 59, in else_body_1\n            ag__.if_stmt(ag__.ld(inputs_embeds) is not None, if_body, else_body, get_state, set_state, ('input_shape',), 1)\n        File \"/var/tmp/__autograph_generated_filehcwpld1c.py\", line 56, in else_body\n            raise ag__.converted_call(ag__.ld(ValueError), (f'You have to specify either {ag__.ld(err_msg_prefix)}input_ids or {ag__.ld(err_msg_prefix)}inputs_embeds',), None, fscope)\n    \n        ValueError: Exception encountered when calling layer 'decoder' (type TFT5MainLayer).\n        \n        in user code:\n        \n            File \"/opt/conda/envs/xla-env/lib/python3.10/site-packages/transformers/modeling_tf_utils.py\", line 1079, in run_call_with_unpacked_inputs  *\n                return func(self, **unpacked_inputs)\n            File \"/opt/conda/envs/xla-env/lib/python3.10/site-packages/transformers/models/t5/modeling_tf_t5.py\", line 754, in call  *\n                raise ValueError(f\"You have to specify either {err_msg_prefix}input_ids or {err_msg_prefix}inputs_embeds\")\n        \n            ValueError: You have to specify either decoder_input_ids or decoder_inputs_embeds\n        \n        \n        Call arguments received by layer 'decoder' (type TFT5MainLayer):\n          \u2022 input_ids=None\n          \u2022 attention_mask=None\n          \u2022 encoder_hidden_states=tf.Tensor(shape=(1, 32, 512), dtype=float32)\n          \u2022 encoder_attention_mask=tf.Tensor(shape=(1, 32), dtype=int32)\n          \u2022 inputs_embeds=None\n          \u2022 head_mask=None\n          \u2022 encoder_head_mask=None\n          \u2022 past_key_values=None\n          \u2022 use_cache=True\n          \u2022 output_attentions=False\n          \u2022 output_hidden_states=False\n          \u2022 return_dict=True\n          \u2022 training=False\n    \n    \n    Call arguments received by layer 'tft5_for_conditional_generation_3' (type TFT5ForConditionalGeneration):\n      \u2022 input_ids=tf.Tensor(shape=(1, 32), dtype=int32)\n      \u2022 attention_mask=tf.Tensor(shape=(1, 32), dtype=int32)\n      \u2022 decoder_input_ids=None\n      \u2022 decoder_attention_mask=None\n      \u2022 head_mask=None\n      \u2022 decoder_head_mask=None\n      \u2022 encoder_outputs=None\n      \u2022 past_key_values=None\n      \u2022 inputs_embeds=None\n      \u2022 decoder_inputs_embeds=None\n      \u2022 labels=None\n      \u2022 use_cache=None\n      \u2022 output_attentions=None\n      \u2022 output_hidden_states=None\n      \u2022 return_dict=None\n      \u2022 training=False\n"
            },
            "jax_flax_eager": {
                "ttft_s": 8.081534624099731
            }
        }
    },
    {
        "TrOCRForCausalLM": {
            "error": "AttributeError(\"'VisionEncoderDecoderModel' object has no attribute '_shift_right'\")"
        }
    },
    {
        "XGLMForCausalLM": {
            "pytorch_torch_compile_inductor": {
                "ttft_s": 30.55780839920044
            },
            "pytorch_torch_compile_eager": {
                "ttft_s": 3.890655994415283
            },
            "pytorch_torch_compile_cudagraphs": {
                "ttft_s": 13.639580249786377
            },
            "pytorch_torch_compile_onnxrt": {
                "error": "backend='onnxrt' raised:\nAssertionError: Please convert all Tensors to FakeTensors first or instantiate FakeTensorMode with 'allow_non_fake_inputs'. Found in aten.alias.default(tensor(-3.4028e+38, size=()))\n\nWhile executing %tensor : [num_users=1] = call_function[target=torch.tensor](args = (-3.4028234663852886e+38,), kwargs = {device: cpu})\nOriginal traceback:\n  File \"/opt/conda/envs/xla-env/lib/python3.10/site-packages/transformers/models/xglm/modeling_xglm.py\", line 763, in forward\n    outputs = self.model(\n  File \"/opt/conda/envs/xla-env/lib/python3.10/site-packages/transformers/models/xglm/modeling_xglm.py\", line 648, in forward\n    layer_outputs = decoder_layer(\n  File \"/opt/conda/envs/xla-env/lib/python3.10/site-packages/transformers/models/xglm/modeling_xglm.py\", line 413, in forward\n    hidden_states, self_attn_weights, present_key_value = self.self_attn(\n  File \"/opt/conda/envs/xla-env/lib/python3.10/site-packages/transformers/models/xglm/modeling_xglm.py\", line 295, in forward\n    attn_weights, torch.tensor(torch.finfo(attn_weights.dtype).min, device=attn_weights.device)\n\n\nSet TORCH_LOGS=\"+dynamo\" and TORCHDYNAMO_VERBOSE=1 for more information\n\n\nYou can suppress this exception and fall back to eager by setting:\n    import torch._dynamo\n    torch._dynamo.config.suppress_errors = True\n"
            },
            "pytorch_torch_compile_openxla": {
                "error": "torch_xla/csrc/runtime/pjrt_registry.cc:214 : Check failed: client \n*** Begin stack trace ***\n\ttsl::CurrentStackTrace()\n\ttorch_xla::runtime::InitializePjRt(std::string const&)\n\ttorch_xla::runtime::PjRtComputationClient::PjRtComputationClient()\n\t\n\ttorch_xla::runtime::GetComputationClient()\n\ttorch_xla::bridge::GetDefaultDevice()\n\ttorch_xla::bridge::GetCurrentDevice()\n\ttorch_xla::bridge::GetCurrentAtenDevice()\n\t\n\t\n\t\n\t_PyObject_MakeTpCall\n\t_PyEval_EvalFrameDefault\n\t\n\t_PyFunction_Vectorcall\n\t_PyEval_EvalFrameDefault\n\t\n\t_PyFunction_Vectorcall\n\t_PyEval_EvalFrameDefault\n\t\n\t_PyFunction_Vectorcall\n\t_PyEval_EvalFrameDefault\n\t\n\t_PyFunction_Vectorcall\n\t_PyEval_EvalFrameDefault\n\t\n\t_PyFunction_Vectorcall\n\t_PyEval_EvalFrameDefault\n\t\n\t_PyFunction_Vectorcall\n\t_PyEval_EvalFrameDefault\n\t\n\t_PyFunction_Vectorcall\n\t_PyEval_EvalFrameDefault\n\t\n\t_PyFunction_Vectorcall\n\t_PyEval_EvalFrameDefault\n\t\n\t_PyFunction_Vectorcall\n\tTHPFunction_apply(_object*, _object*)\n\t\n\tPyObject_Call\n\t_PyEval_EvalFrameDefault\n\t\n\t\n\t_PyEval_EvalFrameDefault\n\t\n\t_PyFunction_Vectorcall\n\t_PyEval_EvalFrameDefault\n\t\n\t_PyFunction_Vectorcall\n\t_PyEval_EvalFrameDefault\n\t\n\t_PyFunction_Vectorcall\n\t_PyEval_EvalFrameDefault\n\t\n\t_PyFunction_Vectorcall\n\t_PyEval_EvalFrameDefault\n\t\n\t_PyFunction_Vectorcall\n\t_PyEval_EvalFrameDefault\n\t\n\t\n\t\n\tPyObject_Call\n\t_PyEval_EvalFrameDefault\n\t\n\t\n\tPyObject_Call\n\t_PyEval_EvalFrameDefault\n\t\n\t\n\tPyObject_Call\n\t_PyEval_EvalFrameDefault\n\t\n\t_PyFunction_Vectorcall\n\tPyObject_Call\n\t_PyEval_EvalFrameDefault\n\t\n\t\n\tPyObject_Call\n\t_PyEval_EvalFrameDefault\n\t\n\t_PyFunction_Vectorcall\n\t_PyObject_FastCallDictTstate\n\t_PyObject_Call_Prepend\n\t\n\tPyObject_Call\n\t_PyEval_EvalFrameDefault\n\t\n\t_PyFunction_Vectorcall\n\t_PyEval_EvalFrameDefault\n\t_PyFunction_Vectorcall\n\t_PyEval_EvalFrameDefault\n\t\n\tPyEval_EvalCode\n\t\n\t\n\t\n\t_PyRun_SimpleFileObject\n\t_PyRun_AnyFileObject\n\tPy_RunMain\n\tPy_BytesMain\n\t__libc_start_main\n\t\n*** End stack trace ***\nUnknown PJRT_DEVICE 'CUDA'"
            },
            "pytorch_torch_compile_tvm": {
                "error": "backend='tvm' raised:\nModuleNotFoundError: No module named 'tvm'\n\nSet TORCH_LOGS=\"+dynamo\" and TORCHDYNAMO_VERBOSE=1 for more information\n\n\nYou can suppress this exception and fall back to eager by setting:\n    import torch._dynamo\n    torch._dynamo.config.suppress_errors = True\n"
            },
            "jax_flax_xla": {
                "ttft_s": 7.663578033447266
            },
            "tensorflow_eager": {
                "ttft_s": 0.5172061920166016
            },
            "tensorflow_xla": {
                "error": "{{function_node __wrapped__AddV2_device_/job:localhost/replica:0/task:0/device:GPU:0}} failed to allocate memory [Op:AddV2] name: "
            },
            "jax_flax_eager": {
                "ttft_s": 3.7685465812683105
            }
        }
    },
    {
        "XLNetLMHeadModel": {
            "pytorch_torch_compile_inductor": {
                "ttft_s": 18.805472135543823
            },
            "pytorch_torch_compile_eager": {
                "ttft_s": 3.069810628890991
            },
            "pytorch_torch_compile_cudagraphs": {
                "ttft_s": 11.837625741958618
            },
            "pytorch_torch_compile_onnxrt": {
                "error": "Unsupported attribute type '<class 'torch.dtype'>' for attribute 'to' in node=%908 : Tensor = onnx::Cast(%_val_907)\n, value is torch.float32"
            },
            "pytorch_torch_compile_openxla": {
                "error": "torch_xla/csrc/runtime/pjrt_registry.cc:214 : Check failed: client \n*** Begin stack trace ***\n\ttsl::CurrentStackTrace()\n\ttorch_xla::runtime::InitializePjRt(std::string const&)\n\ttorch_xla::runtime::PjRtComputationClient::PjRtComputationClient()\n\t\n\ttorch_xla::runtime::GetComputationClient()\n\ttorch_xla::bridge::GetDefaultDevice()\n\ttorch_xla::bridge::GetCurrentDevice()\n\ttorch_xla::bridge::GetCurrentAtenDevice()\n\t\n\t\n\t\n\t_PyObject_MakeTpCall\n\t_PyEval_EvalFrameDefault\n\t\n\t_PyFunction_Vectorcall\n\t_PyEval_EvalFrameDefault\n\t\n\t_PyFunction_Vectorcall\n\t_PyEval_EvalFrameDefault\n\t\n\t_PyFunction_Vectorcall\n\t_PyEval_EvalFrameDefault\n\t\n\t_PyFunction_Vectorcall\n\t_PyEval_EvalFrameDefault\n\t\n\t_PyFunction_Vectorcall\n\t_PyEval_EvalFrameDefault\n\t\n\t_PyFunction_Vectorcall\n\t_PyEval_EvalFrameDefault\n\t\n\t_PyFunction_Vectorcall\n\t_PyEval_EvalFrameDefault\n\t\n\t_PyFunction_Vectorcall\n\t_PyEval_EvalFrameDefault\n\t\n\t_PyFunction_Vectorcall\n\tTHPFunction_apply(_object*, _object*)\n\t\n\tPyObject_Call\n\t_PyEval_EvalFrameDefault\n\t\n\t\n\t_PyEval_EvalFrameDefault\n\t\n\t_PyFunction_Vectorcall\n\t_PyEval_EvalFrameDefault\n\t\n\t_PyFunction_Vectorcall\n\t_PyEval_EvalFrameDefault\n\t\n\t_PyFunction_Vectorcall\n\t_PyEval_EvalFrameDefault\n\t\n\t_PyFunction_Vectorcall\n\t_PyEval_EvalFrameDefault\n\t\n\t_PyFunction_Vectorcall\n\t_PyEval_EvalFrameDefault\n\t\n\t\n\t\n\tPyObject_Call\n\t_PyEval_EvalFrameDefault\n\t\n\t\n\tPyObject_Call\n\t_PyEval_EvalFrameDefault\n\t\n\t\n\tPyObject_Call\n\t_PyEval_EvalFrameDefault\n\t\n\t_PyFunction_Vectorcall\n\tPyObject_Call\n\t_PyEval_EvalFrameDefault\n\t\n\t\n\tPyObject_Call\n\t_PyEval_EvalFrameDefault\n\t\n\t_PyFunction_Vectorcall\n\t_PyObject_FastCallDictTstate\n\t_PyObject_Call_Prepend\n\t\n\tPyObject_Call\n\t_PyEval_EvalFrameDefault\n\t\n\t_PyFunction_Vectorcall\n\t_PyEval_EvalFrameDefault\n\t_PyFunction_Vectorcall\n\t_PyEval_EvalFrameDefault\n\t\n\tPyEval_EvalCode\n\t\n\t\n\t\n\t_PyRun_SimpleFileObject\n\t_PyRun_AnyFileObject\n\tPy_RunMain\n\tPy_BytesMain\n\t__libc_start_main\n\t\n*** End stack trace ***\nUnknown PJRT_DEVICE 'CUDA'"
            },
            "pytorch_torch_compile_tvm": {
                "error": "backend='tvm' raised:\nModuleNotFoundError: No module named 'tvm'\n\nSet TORCH_LOGS=\"+dynamo\" and TORCHDYNAMO_VERBOSE=1 for more information\n\n\nYou can suppress this exception and fall back to eager by setting:\n    import torch._dynamo\n    torch._dynamo.config.suppress_errors = True\n"
            },
            "jax_flax_xla": {
                "error": "Unrecognized configuration class <class 'transformers.models.xlnet.configuration_xlnet.XLNetConfig'> for this kind of AutoModel: FlaxAutoModel.\nModel type should be one of AlbertConfig, BartConfig, BeitConfig, BertConfig, BigBirdConfig, BlenderbotConfig, BlenderbotSmallConfig, BloomConfig, CLIPConfig, Dinov2Config, DistilBertConfig, ElectraConfig, GemmaConfig, GPT2Config, GPT2Config, GPTNeoConfig, GPTJConfig, LlamaConfig, LongT5Config, MarianConfig, MBartConfig, MistralConfig, MT5Config, OPTConfig, PegasusConfig, RegNetConfig, ResNetConfig, RobertaConfig, RobertaPreLayerNormConfig, RoFormerConfig, T5Config, VisionTextDualEncoderConfig, ViTConfig, Wav2Vec2Config, WhisperConfig, XGLMConfig, XLMRobertaConfig."
            },
            "tensorflow_eager": {
                "ttft_s": 0.22660064697265625
            },
            "tensorflow_xla": {
                "ttft_s": 10.276795148849487
            }
        }
    },
    {
        "YituTechConvBert": {
            "jax_flax_xla": {
                "error": "Unrecognized configuration class <class 'transformers.models.convbert.configuration_convbert.ConvBertConfig'> for this kind of AutoModel: FlaxAutoModel.\nModel type should be one of AlbertConfig, BartConfig, BeitConfig, BertConfig, BigBirdConfig, BlenderbotConfig, BlenderbotSmallConfig, BloomConfig, CLIPConfig, Dinov2Config, DistilBertConfig, ElectraConfig, GemmaConfig, GPT2Config, GPT2Config, GPTNeoConfig, GPTJConfig, LlamaConfig, LongT5Config, MarianConfig, MBartConfig, MistralConfig, MT5Config, OPTConfig, PegasusConfig, RegNetConfig, ResNetConfig, RobertaConfig, RobertaPreLayerNormConfig, RoFormerConfig, T5Config, VisionTextDualEncoderConfig, ViTConfig, Wav2Vec2Config, WhisperConfig, XGLMConfig, XLMRobertaConfig."
            }
        }
    }
]